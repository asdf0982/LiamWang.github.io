<!doctype html>



  


<html class="theme-next mist use-motion" lang="en">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>






<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />



  <meta name="google-site-verification" content="zioBb4Ac5xjO7-IkGVs3Qnyfbu-5lW01M2wTphSvcWU" />













  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.1" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="Hexo, NexT" />








  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.1" />






<meta name="description" content="哈~我不适合在白天工作~">
<meta property="og:type" content="website">
<meta property="og:title" content="Liam&#39;s Blog">
<meta property="og:url" content="https://asdf0982.github.io/index.html">
<meta property="og:site_name" content="Liam&#39;s Blog">
<meta property="og:description" content="哈~我不适合在白天工作~">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Liam&#39;s Blog">
<meta name="twitter:description" content="哈~我不适合在白天工作~">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    sidebar: {"position":"left","display":"hide","offset":12,"offset_float":0,"b2t":false,"scrollpercent":false},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="https://asdf0982.github.io/"/>





  <title>Liam's Blog - Share the joy of coding.</title>
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="en">

  




<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
            (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
          m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
  ga('create', 'UA-99966000-1', 'auto');
  ga('send', 'pageview');
</script>












  
  
    
  

  <div class="container sidebar-position-left 
   page-home 
 ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Liam's Blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <h1 class="site-subtitle" itemprop="description">Share the joy of coding.</h1>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            About
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archives
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            Tags
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />
            
            Search
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off"
             placeholder="Searching..." spellcheck="false"
             type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="https://asdf0982.github.io/2017/06/24/FaceNet/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Liam Wang">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/uploads/avatar2.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Liam's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2017/06/24/FaceNet/" itemprop="url">FaceNet:A Unified Embedding for Face Recognition and Clustering 阅读笔记</a></h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-06-24T11:35:03+08:00">
                2017-06-24
              </time>
            

            
              <span class="post-meta-divider">|</span>
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-check-o"></i>
              </span>
              
                <span class="post-meta-item-text">Post modified</span>
              
              <time title="Post modified" itemprop="dateModified" datetime="2017-06-24T22:55:04+08:00">
                2017-06-24
              </time>
            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>本文由GOOGLE于2015年发表在CVPR<br>参考:</p>
<ul>
<li><a href="https://zhuanlan.zhihu.com/p/24837264" target="_blank" rel="external">谷歌人脸识别系统FaceNet解析</a></li>
<li><a href="http://blog.csdn.net/stdcoutzyx/article/details/46687471" target="_blank" rel="external">FaceNet–Google的人脸识别</a></li>
<li><a href="http://www.cnblogs.com/xiaohuahua108/p/6505756.html" target="_blank" rel="external">FaceNet—深度学习与人脸识别的二次结合</a></li>
</ul>
<h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>本文提出一个系统名为FaceNet. 它可以直接将人脸图像映射至欧式空间, 其距离能直接反应出人脸间的相似度. 一旦生成该空间, 识别, 验证, 聚类等任务都可以用它来轻松完成. FaceNet在LFW上达到了99.63%的准确率, 在Youtube Faces DB达到95.12%.<br>和之前的方法(先输出高维度特征向量, 然后用PCA等降维, 再用分类器分类)不同, FaceNet直接使用基于triplets的LMNN（最大边界近邻分类）的loss函数训练神经网络, 网络直接输出为128维度的向量空间.<br>其人脸比对的结果如图所示:<br><img src="/2017/06/24/FaceNet/1.png" alt="[Illumination and Pose invariance]" title="[Illumination and Pose invariance]"><br>可以看到1.1就是阈值.</p>
<h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><p>本文探索了2个网络.第一类为Zeiler&amp;Fergus研究中使用的神经网络, 受NIN影响, 我们在网络后面加了多个1*1*d卷积层, 第二类为Inception网络.<br>关键在于网络的末端使用了triplet loss作为目标函数. </p>
<h3 id="Triplet-Loss"><a href="#Triplet-Loss" class="headerlink" title="Triplet Loss"></a>Triplet Loss</h3><p>triplet loss: 在3个样本中,最小化类内差距,最大化类间差距.如图:<br><img src="/2017/06/24/FaceNet/triplet.png" alt="[Triplet Loss]" title="[Triplet Loss]"><br>公式如下:<br><img src="/2017/06/24/FaceNet/tripletloss.png" alt="[triplet loss公式]" title="[triplet loss公式]"></p>
<h3 id="Triplet-3个样本的选择"><a href="#Triplet-3个样本的选择" class="headerlink" title="Triplet 3个样本的选择"></a>Triplet 3个样本的选择</h3><p>我们想要从类内选择出一些P样本(hard-positive), 使他们与A样本的平均距离最大(argmax). 同时从类外选择出一些N样本(hard-negative), 使他们与A样本的平均距离最小(argmin). 但计算整个训练集之间的平均最大最小距离是不现实的.这里有两个办法解决它:</p>
<ol>
<li>每隔几步产生triplets, 计算它们在子数据集中的argmax和argmin. </li>
<li>在线产生triplets, 从mini-batch中得到hard positive/negative样本.(本文使用)</li>
</ol>
<p>文章选用方法2, 为了防止选择不当, 使用如下公式来约束样本选择.<br><img src="/2017/06/24/FaceNet/select.png" alt="[help to select]" title="[help to select]"></p>
<h3 id="深度卷积网络"><a href="#深度卷积网络" class="headerlink" title="深度卷积网络"></a>深度卷积网络</h3><p>第一个是Zeiler&amp;Fergus的22层网络.<br><img src="/2017/06/24/FaceNet/NN1.png" alt="[NN1]" title="[NN1]"><br>第二个网络<br><img src="/2017/06/24/FaceNet/NN2.png" alt="[NN2]" title="[NN2]"></p>
<h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><ol>
<li>NN2(input 224*224)的效果最好.</li>
<li>即使图像只有80*80, 验证正确率下降的也不大.</li>
<li>NN1输出的特征在128维时效果最好.</li>
<li>训练的数据量增大能提高准确率.</li>
</ol>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="https://asdf0982.github.io/2017/06/23/DeepID2-1/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Liam Wang">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/uploads/avatar2.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Liam's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2017/06/23/DeepID2-1/" itemprop="url">Deeply learned face representations are sparse, selective, and robust 阅读笔记</a></h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-06-23T20:50:44+08:00">
                2017-06-23
              </time>
            

            
              <span class="post-meta-divider">|</span>
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-check-o"></i>
              </span>
              
                <span class="post-meta-item-text">Post modified</span>
              
              <time title="Post modified" itemprop="dateModified" datetime="2017-06-25T16:23:09+08:00">
                2017-06-25
              </time>
            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>本文由香港中文大学于2015年发表在CVPR会议</p>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>本文设计了一个深度卷积网络, 获取到 的特征名为DeepID2+.相比与DeepID2,2+ 的网络通过增加隐层特征的维度和对靠前的卷积层监督使DeepID2+在LFW上表现很好.<br>对于人脸识别有3个点很关键:</p>
<ol>
<li>sparse: 稀疏, 即特征向量中包含很多零, 可压缩性强.</li>
<li>selective: 精选, 指具有判别能力的一些分量, 在不同人脸上响应程度不同.</li>
<li>robust: 健壮, 指图像被遮挡后特征中某些向量依然保持不变的一致性.</li>
</ol>
<h2 id="DeepID2-网络结构"><a href="#DeepID2-网络结构" class="headerlink" title="DeepID2+ 网络结构"></a>DeepID2+ 网络结构</h2><p>如图:<br><img src="/2017/06/23/DeepID2-1/Deepid2plus.png" alt="[DeepID2+ net]" title="[DeepID2+ net]"><br>DeepID2+网络也使用了识别和验证两种信号进行监督.此部分参考我上一篇笔记.<br>与DeepID2网络的不同点:</p>
<ol>
<li>每个卷积层后(实际上是1,2,3maxpool层和conv4层后)都获得一个128维的特征, 共有4个卷积层故特征从160维增大到512维.</li>
<li>训练集增大.从8000类160000图增大至12000类290000图.</li>
<li>用全连接层连接每个卷积层, 并使用DeepID2的那种监督方式.这样监督信号就离前期的卷积层更近了,也更有效.</li>
</ol>
<p>实验发现DeepID2选取了25个patch，DeepID2+选取了同样的25个patch，然后抽取的特征分别训练联合贝叶斯模型，得到的结果是DeepID2+平均比DeepID2提高2%。<br>特别的,根据稀疏性对DeepID2+进行二值化处理后, 再使用联合贝叶斯或汉明距离实验,发现识别率下降有限.二值化后的特征节省空间并且可以使人脸检索变得速度更快，更接近实用场景。</p>
<h2 id="关于DeepID3"><a href="#关于DeepID3" class="headerlink" title="关于DeepID3"></a>关于DeepID3</h2><p>DeepID3实现了两个更深的网络,一个参考VGG, 一个参考googlenet.<br>如图所示:<br><img src="/2017/06/23/DeepID2-1/DeepID3.png" alt="[Two DeepID3 Network]" title="[Two DeepID3 Network]"><br>但结果较DeepID2+差距不大.</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="https://asdf0982.github.io/2017/06/22/DeepID2/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Liam Wang">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/uploads/avatar2.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Liam's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2017/06/22/DeepID2/" itemprop="url">Deep Learning Face Representation by Joint Identification-Verification 阅读笔记</a></h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-06-22T15:22:18+08:00">
                2017-06-22
              </time>
            

            
              <span class="post-meta-divider">|</span>
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-check-o"></i>
              </span>
              
                <span class="post-meta-item-text">Post modified</span>
              
              <time title="Post modified" itemprop="dateModified" datetime="2017-06-22T22:55:05+08:00">
                2017-06-22
              </time>
            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>本文由香港中文大学于2014年发表<br>参考:</p>
<ul>
<li><a href="http://blog.csdn.net/stdcoutzyx/article/details/41497545" target="_blank" rel="external">DeepID2——强大的人脸分类算法</a></li>
<li><a href="http://blog.csdn.net/u010318961/article/details/51967845" target="_blank" rel="external">经典计算机视觉论文笔记汇总</a></li>
<li><a href="http://blog.csdn.net/stdcoutzyx/article/details/42091205" target="_blank" rel="external">DeepID人脸识别算法之三代</a></li>
</ul>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>人脸识别的中心主题应该是减少类内变化增大类间差异.<br>所谓识别(identification)是将一张图片从大量的类别中识别出来. 而所谓验证(verification)是比较两批图像是否为同一类.我们使用识别和验证两种信号(signal)得到的特征称为DeepID2. 使用DeepID2, 在LFW数据集上得到了99.15%的正确率</p>
<h2 id="特征抽取"><a href="#特征抽取" class="headerlink" title="特征抽取"></a>特征抽取</h2><p>DeepID2使用的结构:<br><img src="/2017/06/22/DeepID2/DeepID2.png" alt="[The ConvNet for DeepID2]" title="[The ConvNet for DeepID2]"><br>其网络结构与DeepID很相似,只不过输入图像的大小改为55*47.<br>DeepID2特征从两个监督信号学习得到.<br>首先是人脸识别信号, 识别通过在DeepID2层后接一个softmax层实现.loss表示为Ident loss<br>然后是人脸验证信号, 它能有效的减小类内变化, 通常有L1, L2, Cosine相似度3种形式, 文中使用L2.表示为Verif loss, 公式如下:<br><img src="/2017/06/22/DeepID2/L2.png" alt="[Verif loss]" title="[Verif loss]"><br>当图同类时($y_{ij}=1$)减小类内差距.<br>当图异类时($y_{ij}=-1$)增大类间差距至m, m是需要手动调节的.<br>余弦距离公式也给出, 但文章没有使用,我就不列出.<br>DeepID2特征学习算法:<br><img src="/2017/06/22/DeepID2/train.png" alt="[loss]" title="[loss]"><br>可以看到最终的目标函数(loss)是由Ident loss和Verif loss加权得到的.</p>
<h2 id="人脸验证"><a href="#人脸验证" class="headerlink" title="人脸验证"></a>人脸验证</h2><p>使用SDM获得人脸的21个基准点, 然后使用这些基准点将人脸图像对齐.<br>然后在不同的位置, 大小, 色彩通道和水平翻转切割每张人脸图像,每张图像得到400个不同的patch. 将其放入200个前文提到的深度卷积网络, 抽取出400个DeepID2特征向量.选择水平翻转的图像与原始图像作为一对, 放入网络得到2个DeepID2特征向量.<br>为了减少DeepID2特征的长度, 使用前向-反向贪心算法选择出25个特征向量. 25*160=4000维依然太大, 使用PCA将人脸特征向量压缩至180维, 然后使用联合贝叶斯算法进行人脸识别. 例子:<br><img src="/2017/06/22/DeepID2/25face.png" alt="[example]" title="[example]"></p>
<h2 id="疑问和思考"><a href="#疑问和思考" class="headerlink" title="疑问和思考"></a>疑问和思考</h2><ol>
<li>联合贝叶斯算法是什么样的?</li>
<li>SDM算法是什么?</li>
</ol>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="https://asdf0982.github.io/2017/06/21/DeepID/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Liam Wang">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/uploads/avatar2.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Liam's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2017/06/21/DeepID/" itemprop="url">Deep Learning Face Representation from Predicting 10,000 Classes 阅读笔记</a></h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-06-21T14:25:17+08:00">
                2017-06-21
              </time>
            

            
              <span class="post-meta-divider">|</span>
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-check-o"></i>
              </span>
              
                <span class="post-meta-item-text">Post modified</span>
              
              <time title="Post modified" itemprop="dateModified" datetime="2017-06-22T15:07:26+08:00">
                2017-06-22
              </time>
            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>本文由香港中文大学于2014年发表在CVPR会议<br>参考:</p>
<ul>
<li><a href="http://www.cnblogs.com/zzq1989/p/4373680.html" target="_blank" rel="external">Deep Learning Face Representation from Predicting 10,000 Classes论文笔记</a></li>
<li><a href="http://blog.csdn.net/chenriwei2/article/details/31415069" target="_blank" rel="external">【深度学习论文笔记】Deep Learning Face Representation from Predicting 10,000 Classes</a></li>
<li><a href="http://blog.csdn.net/prm10/article/details/50598975" target="_blank" rel="external">Deep Learning Face Representation from Predicting 10,000 Classes</a></li>
<li><a href="http://blog.csdn.net/u010318961/article/details/51967845" target="_blank" rel="external">经典计算机视觉论文笔记——DeepFace\DeepID\DeepID2\DeepID3\FaceNet\VGGFace汇总</a></li>
</ul>
<h2 id="Introduction-and-Related-Work"><a href="#Introduction-and-Related-Work" class="headerlink" title="Introduction and Related Work"></a>Introduction and Related Work</h2><p>本文提供了一种用多个深度模型获得高水平人脸特征(DeepID)来对人脸分类的方法.所谓DeepID,假如网络有10000个分类的输出, 那么在最后一层的输入(倒数第二层的输出)就是DeepID的组成部分,文中有160维.如图<br><img src="/2017/06/21/DeepID/DeepID.png" alt="[DeepID]" title="[DeepID]"><br>DeepID有很好的泛化能力, 再对它用Joint Bayesian等方法就能进行人脸验证.</p>
<h2 id="Deep-ConvNets"><a href="#Deep-ConvNets" class="headerlink" title="Deep ConvNets"></a>Deep ConvNets</h2><p>结构如下:<br><img src="/2017/06/21/DeepID/ConvNet.png" alt="[ConvNet]" title="[ConvNet]"><br>输入图像为39*31*k, k表示通道数(RGB图像k为3, 灰度图像k为1).<br>激活函数使用RELU.<br>DeepID层为全连接层,其输入为Maxp3和Conv4两者.输出特征160维. 因Conv4包含了较多的全局信息, 需要Maxp3进行细节补充.<img src="/2017/06/21/DeepID/hiddenlayer.png" alt="[DeepID层(FC)公式]" title="[DeepID层(FC)公式]"><br>$x^1,w^1,x^2,w^2$代表Maxp3和Conv4的输出和对应的权重.<br>该模型使用SGD的最优化方法.</p>
<h2 id="Feature-extraction"><a href="#Feature-extraction" class="headerlink" title="Feature extraction"></a>Feature extraction</h2><p>找到人脸上的5个标记点.(两个眼睛中心点, 鼻尖点, 两个嘴角点)<br>将人脸根据两个眼睛中心点和嘴角连线的中点对齐.<br>从60个不同的小块抽取DeepID特征, 60这一数字源于10个区域, 3个大小, RGB和灰度2种通道.如图:<br><img src="/2017/06/21/DeepID/facepatch.png" alt="[face path]" title="[face path]"><br>上方的十张图是由中等大小尺寸切的.上方左边的5张图切自弱对齐后的全局区域,上方右边的5张图则分别以5个人脸标记点为中心切.下方代表3种大小.(每种大小又分为长方形和正方形两个形状.)<br>每张图片输出160维,一个patch有两张图片(一张是原始的,一张是<strong>水平翻转</strong>的副本),共有60个patch.则最终DeepID将所有维度连接起来,即160*2*60=19200维.这将用于最终的人脸验证.(每个网络输入一个patch,一张长方形一张正方形)</p>
<h2 id="Face-verification"><a href="#Face-verification" class="headerlink" title="Face verification"></a>Face verification</h2><p>分别使用JointBayesian和神经网络进行人脸识别.后续结果表面JointBayesian效果较好.<br>神经网络部分示意图:<br><img src="/2017/06/21/DeepID/NN.png" alt="[用于人脸识别的NN]" title="[用于人脸识别的NN]"><br>输入有640*60 是因为人脸识别需要两个张人脸图像.每张图像切好得到一个patch会产生160*2维输出.共有两张图像, 故640 = 160*2*2<br>注意到第一个隐层是locally-connect layer ,表示一个神经元只与对应的group连接, 这样神经元就能学习到紧凑的局部特征.第二的隐层为全连接层, 这表示从局部特征中学习全局特征.<br>隐层都使用RELU, 最后一层(输出)使用sigmoid.并且对所有隐层节点使用dropout.</p>
<h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p>采用CelebFace训练模型, 在LFW上验证.<br>随机选择80%的人训练DeepID模型, 剩下20%的人训练联合贝叶斯或神经网络.<br>在人脸识别阶段, 将特征通过PCA降维至150维, 再输入联合贝叶斯中.在测试时, 判断两张脸是否是同一个人是通过看联合贝叶斯的极大似然比来决定的, 该阈值通过训练集确定.<br>在LFW上的表现如下,其他结果见论文.<br><img src="/2017/06/21/DeepID/result.png" alt="[result in LFW]" title="[result in LFW]"></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="https://asdf0982.github.io/2017/06/20/A-Light-CNN-for-Deep-Face-Representation-with-Noisy-Labels/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Liam Wang">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/uploads/avatar2.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Liam's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2017/06/20/A-Light-CNN-for-Deep-Face-Representation-with-Noisy-Labels/" itemprop="url">A Light CNN for Deep Face Representation with Noisy Labels 阅读笔记</a></h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-06-20T21:14:50+08:00">
                2017-06-20
              </time>
            

            
              <span class="post-meta-divider">|</span>
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-check-o"></i>
              </span>
              
                <span class="post-meta-item-text">Post modified</span>
              
              <time title="Post modified" itemprop="dateModified" datetime="2017-06-21T14:16:01+08:00">
                2017-06-21
              </time>
            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>本文中科院自动所于2017年发表。也是我上一篇博客阅读内容的升级版。<br>论文链接：<a href="https://arxiv.org/pdf/1511.02683v3.pdf" target="_blank" rel="external">PDF</a><br>参考阅读：</p>
<ul>
<li><a href="http://blog.csdn.net/qq_14845119/article/details/56677339" target="_blank" rel="external">人脸识别之light_cnn</a></li>
<li><a href="http://blog.csdn.net/sihailongwang/article/details/72831259" target="_blank" rel="external">人脸识别方向论文笔记（1）– A Light CNN for Deep Face Representation With Noisy Labels</a></li>
</ul>
<p>第一作者提供项目地址: <a href="https://github.com/AlfredXiangWu/face_verification_experiment" target="_blank" rel="external">https://github.com/AlfredXiangWu/face_verification_experiment</a></p>
<h2 id="Introduction-and-Related-Work"><a href="#Introduction-and-Related-Work" class="headerlink" title="Introduction and Related Work"></a>Introduction and Related Work</h2><p>本文有以下贡献：</p>
<ol>
<li>介绍了激活函数MFM。相比RELU，它的临界值由训练数据得到，并采用竞争关系其在不同的数据集上拥有更好泛化和适应能力。</li>
<li>受AlexNet, VGG, ResNet启发设计了3个基于MFM的light CNN。</li>
<li>提出了一种通过预训练深度网络的语义引导方法来处理大规模数据集中的噪声标记图像。标签不一致可以通过预测的概率有效检测，然后被重新标记或删除用于训练。</li>
<li>提出的有256维特征的单一模型在各种不同人脸识别基准下表现不错。模型有较少参数故能在CPU或嵌入式系统上较快地抽取特征。</li>
</ol>
<h2 id="Architecture"><a href="#Architecture" class="headerlink" title="Architecture"></a>Architecture</h2><h3 id="Max-Feature-Map-Operation"><a href="#Max-Feature-Map-Operation" class="headerlink" title="Max-Feature-Map Operation"></a>Max-Feature-Map Operation</h3><p>我上一篇博文已经介绍了这个概念.这里只是增加了MFM 3/2的用法.<br>附上示意图与公式.<br><img src="/2017/06/20/A-Light-CNN-for-Deep-Face-Representation-with-Noisy-Labels/MFM.png" alt="[MFM与传统激活函数对比]" title="[MFM与传统激活函数对比]"><br><img src="/2017/06/20/A-Light-CNN-for-Deep-Face-Representation-with-Noisy-Labels/MFM2.png" alt="[MFM公式]" title="[MFM公式]"></p>
<h3 id="The-Light-CNN-Framework"><a href="#The-Light-CNN-Framework" class="headerlink" title="The Light CNN Framework"></a>The Light CNN Framework</h3><p>第一个结构:(缘于AlexNet)<br><img src="/2017/06/20/A-Light-CNN-for-Deep-Face-Representation-with-Noisy-Labels/CNN1.png" alt="[Light CNN-4]" title="[Light CNN-4]"><br>第二个结构:<br><img src="/2017/06/20/A-Light-CNN-for-Deep-Face-Representation-with-Noisy-Labels/CNN2.png" alt="[Light CNN-9]" title="[Light CNN-9]"><br>可以看到此结构相比于旧版论文, 在Conv2a位置更细致并在Conv4-5之间去掉了一个pool层.<br>第三个结构:<br><img src="/2017/06/20/A-Light-CNN-for-Deep-Face-Representation-with-Noisy-Labels/CNN3.png" alt="[Light CNN-29]" title="[Light CNN-29]"><br>注意:</p>
<ol>
<li>取消了批量归一化操作(batch normalization).</li>
<li>采用全连接层而不是global average pooling layer.因为训练图像都是对齐的.global average pooling layer可能损坏图像的一部分语义和空间特征.</li>
</ol>
<h3 id="Semantic-Bootstrapping-for-Noisy-Label"><a href="#Semantic-Bootstrapping-for-Noisy-Label" class="headerlink" title="Semantic Bootstrapping for Noisy Label"></a>Semantic Bootstrapping for Noisy Label</h3><p>如下步骤:</p>
<ol>
<li>用原始的含噪声标签数据集训练LightCNN</li>
<li>用训练好的模型预测训练集的标签.设置一个是否接受预测的阈值.</li>
<li>用重新标签过的数据集再次训练LightCNN</li>
</ol>
<h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><p>在LFW和YFW上的表现:<br><img src="/2017/06/20/A-Light-CNN-for-Deep-Face-Representation-with-Noisy-Labels/result1.png" alt="[Comparison]" title="[Comparison]"><br>在其他数据集的表现见论文.</p>
<h2 id="疑问和思考"><a href="#疑问和思考" class="headerlink" title="疑问和思考"></a>疑问和思考</h2><ol>
<li>LightCNN-29的模型尚未理解</li>
</ol>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="https://asdf0982.github.io/2017/06/20/A-lightened-cnn-for-deep-face-representation/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Liam Wang">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/uploads/avatar2.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Liam's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2017/06/20/A-lightened-cnn-for-deep-face-representation/" itemprop="url">A lightened cnn for deep face representation 阅读笔记</a></h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-06-20T11:01:47+08:00">
                2017-06-20
              </time>
            

            
              <span class="post-meta-divider">|</span>
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-check-o"></i>
              </span>
              
                <span class="post-meta-item-text">Post modified</span>
              
              <time title="Post modified" itemprop="dateModified" datetime="2017-06-20T21:22:24+08:00">
                2017-06-20
              </time>
            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>本文由北京科技大学与中科院于2015年发表<br>论文链接：<a href="https://pdfs.semanticscholar.org/d4e6/69d5d35fa0ca9f8d9a193c82d4153f5ffc4e.pdf" target="_blank" rel="external">PDF</a><br>参考阅读：</p>
<ul>
<li><a href="http://blog.csdn.net/cv_family_z/article/details/50401101" target="_blank" rel="external">A Lightened CNN for Deep Face Representation</a></li>
<li><a href="http://blog.sina.com.cn/s/blog_a5b8a4420102wypn.html" target="_blank" rel="external">A Lightened CNN for Deep Face Representation论文笔记</a></li>
<li><a href="http://blog.csdn.net/tinyzhao/article/details/53127870" target="_blank" rel="external">人脸验证：Lightened CNN</a></li>
</ul>
<p>第一作者提供github代码地址：<a href="https://github.com/AlfredXiangWu/face_verification_experiment" target="_blank" rel="external">https://github.com/AlfredXiangWu/face_verification_experiment</a></p>
<h2 id="Introduction-and-Related-Work"><a href="#Introduction-and-Related-Work" class="headerlink" title="Introduction and Related Work"></a>Introduction and Related Work</h2><p>概述并归类人脸识别领域相关的方法，其related work简述了最新的成果（有内容重复是什么鬼），其脚注对学习领域内经典的几篇论文有很好的借鉴意义。<br>把使用CNN对人脸验证的方法大致分成了三类：</p>
<ol>
<li>使用CNN提取人脸特征向量，再使用分类器进行判断是否为同一人。</li>
<li>直接优化验证损失（verification loss），这个方法对训练集中不存在的新类别效果不佳。</li>
<li>结合识别和验证</li>
</ol>
<p>尽管之前的方法都在LFW上达到了很高的准确率，但是模型计算量的需求很高，较难在移动设备中推广。<br>传统的深度网络模型由于很深，需要花费很多时间提取特征。并且大多数模型都采用RELU激活函数，这导致得到的特征通常维度高且稀疏。为了获得低维度且紧凑的特征，通常采用Joint Bayesian或metric learning等方法。因此直接寻求一个轻量的高速抽取特征且维度表现低的CNN相当重要。<br>所以论文主要说明了3件事:</p>
<ol>
<li>一种名为MFM的激活函数，用于卷积层。<img src="/2017/06/20/A-lightened-cnn-for-deep-face-representation/lightenedcnn.png" alt="[MFM使用示意图]" title="[MFM使用示意图]"></li>
<li>两个轻量的CNN.</li>
<li>提出的CNN模型在LFW和YFW上获得了不错表现。值得注意的是，模型大小仅为20-30MB，特征提取仅耗时67ms。该模型有希望应用到实时监测领域。</li>
</ol>
<h2 id="Architecture"><a href="#Architecture" class="headerlink" title="Architecture"></a>Architecture</h2><h3 id="MFM激活函数"><a href="#MFM激活函数" class="headerlink" title="MFM激活函数"></a>MFM激活函数</h3><p>根据上面的MFM示意图，可以看到灰度图像（单通道）输入到两个相同结构的卷积层中，MFM比较两个卷积层中对应的通道，取最大值。<br><img src="/2017/06/20/A-lightened-cnn-for-deep-face-representation/MFM.png" alt="[示意图2]" title="[示意图2]"><br>MFM的公式：<br><img src="/2017/06/20/A-lightened-cnn-for-deep-face-representation/MFM2.png" alt="[MFM公式]" title="[MFM公式]"><br>卷积层有相同的2个，每个有n个通道，每个通道的大小为h*w。MFM就是比较两个层里相应通道的结果，故MFM输出的大小为h*w*n。<br>其导数公式：<br><img src="/2017/06/20/A-lightened-cnn-for-deep-face-representation/MFM3.png" alt="[MFM导数公式]" title="[MFM导数公式]"><br>可以看到有一半激活层梯度为零。相比RELU，MFM能得到更紧凑的特征。<br>此外，MFM激活函数也可以被看作是两个卷积层之间的稀疏连接，它将信息稀疏地编码至特征空间。</p>
<h3 id="lightened-CNN结构"><a href="#lightened-CNN结构" class="headerlink" title="lightened CNN结构"></a>lightened CNN结构</h3><img src="/2017/06/20/A-lightened-cnn-for-deep-face-representation/lightenedcnn2.png" alt="[Lightened CNN结构示意图]" title="[Lightened CNN结构示意图]">
<p>NIN其实就是使用了MLP卷积层。<br>为了弄清conv2_a，找到源码中proto的代码表达。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line">layers&#123;</div><div class="line">  name: &quot;conv2a&quot;</div><div class="line">  type: CONVOLUTION</div><div class="line">  blobs_lr: 1</div><div class="line">  blobs_lr: 2</div><div class="line">  weight_decay: 1</div><div class="line">  weight_decay: 0</div><div class="line">  convolution_param &#123;</div><div class="line">	num_output: 96</div><div class="line">	kernel_size: 1</div><div class="line">	stride: 1</div><div class="line">	weight_filler &#123;</div><div class="line">	  type: &quot;xavier&quot;</div><div class="line">	&#125;</div><div class="line">	bias_filler &#123;</div><div class="line">	  type: &quot;constant&quot;</div><div class="line">	  value: 0.1</div><div class="line">	&#125;</div><div class="line">  &#125;</div><div class="line">  bottom: &quot;pool1&quot;</div><div class="line">  top: &quot;conv2a&quot;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>96个大小为1的卷积核，即一个全连接层。</p>
<h2 id="疑问和思考"><a href="#疑问和思考" class="headerlink" title="疑问和思考"></a>疑问和思考</h2><ol>
<li><a href="http://blog.csdn.net/u014114990/article/details/50144653" target="_blank" rel="external">神经网络为什么要有激活函数，为什么relu 能够防止梯度消失？</a></li>
<li>NIN层是如何实现的？</li>
</ol>
<ul>
<li><a href="http://zkread.com/article/1275089.html" target="_blank" rel="external">深度学习方法（十）：卷积神经网络结构变化——Maxout Networks，Network In Network，Global Average Pooling</a></li>
<li><a href="http://blog.leanote.com/post/lonelyminer/Detection%E4%B9%8BNIN" target="_blank" rel="external">Detection之NIN</a></li>
</ul>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="https://asdf0982.github.io/2017/06/19/DeepFace-Closing-the-Gap-to-Human-Level-Performance-in-Face-Verification/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Liam Wang">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/uploads/avatar2.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Liam's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2017/06/19/DeepFace-Closing-the-Gap-to-Human-Level-Performance-in-Face-Verification/" itemprop="url">DeepFace: Closing the Gap to Human-Level Performance in Face Verification 阅读笔记</a></h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-06-19T11:19:59+08:00">
                2017-06-19
              </time>
            

            
              <span class="post-meta-divider">|</span>
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-check-o"></i>
              </span>
              
                <span class="post-meta-item-text">Post modified</span>
              
              <time title="Post modified" itemprop="dateModified" datetime="2017-06-20T21:22:46+08:00">
                2017-06-20
              </time>
            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>本文由Facebook AI研究中心于2014年发表.<br>论文链接:<a href="http://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Taigman_DeepFace_Closing_the_2014_CVPR_paper.pdf" target="_blank" rel="external">PDF</a><br>参考:</p>
<ul>
<li><a href="http://blog.csdn.net/kunyxu/article/details/54375669" target="_blank" rel="external">FaceBook 论文：DeepFace: Closing the Gap to Human-Level Performance in Face Verification 笔记</a></li>
<li><a href="http://blog.csdn.net/shixiangyun2/article/details/50809069" target="_blank" rel="external">【翻译+原创】DeepFace: Closing the Gap to Human-Level Performance in Face Verification 论文笔记</a></li>
<li><a href="https://saicoco.github.io/Deep-face-series/" target="_blank" rel="external">论文笔记–DeepLearning face Recognition</a></li>
</ul>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>该文主要做了3件事:</p>
<ol>
<li>构建一个有效的, 利用含大量标签过的人脸数据集训练的深度神经网络(DNN)获取人脸特征. 并且该网络在其他数据集上的表现也不错.</li>
<li>一个基于3D建模的人脸对齐系统.</li>
<li>在LFW上使机器达到近乎人类的识别能力, 在YFW上降低错误率50%以上.</li>
</ol>
<h2 id="Face-Alignment-人脸对齐"><a href="#Face-Alignment-人脸对齐" class="headerlink" title="Face Alignment 人脸对齐"></a>Face Alignment 人脸对齐</h2><img src="/2017/06/19/DeepFace-Closing-the-Gap-to-Human-Level-Performance-in-Face-Verification/alignment.png" alt="[Alignment]" title="[Alignment]">
<p>分为以下几步:</p>
<ol>
<li>检测人脸, 找到6个初始基准点.</li>
<li>根据这6个点进行剪切.</li>
<li>在切割后的图像上找到67个基准点, 进行三角形分割并在轮廓上添加三角形防止图像不连续.</li>
<li>将三角化的人脸转化为3D形状.</li>
<li>将3D形状三角化,较黑的三角代表较不可见.</li>
<li>将67个基准点的模型调整至正面向前</li>
<li>投影至2D图像,最终对齐人脸.</li>
<li>一种新的3D模型生成方式. (论文中未使用)</li>
</ol>
<p>关于3D对齐的具体方法我不做深究, 留个坑吧.</p>
<h2 id="Representation-特征提取"><a href="#Representation-特征提取" class="headerlink" title="Representation 特征提取"></a>Representation 特征提取</h2><p>网络结构:<br><img src="/2017/06/19/DeepFace-Closing-the-Gap-to-Human-Level-Performance-in-Face-Verification/DeepFace.png" alt="[DeepFace architecture]" title="[DeepFace architecture]"><br>论文里说152*152的图像使用32个大小为11*11*3的卷积核用32*11*11*3@152*152表示.(但上图并不是这样表示的)<br>我补充一下网络结构.<br>C1: filter_size=11*11; filter_num=32;stride=1;pad=0<br>M2: filter_size=3*3;stride=2<br>C3: filter_size=9*9; filter_num=16;stride=1;pad=0<br>Local-conv层含义参考:<a href="https://prateekvjoshi.com/2016/04/12/understanding-locally-connected-layers-in-convolutional-neural-networks/" target="_blank" rel="external"><strong>Understanding Locally Connected Layers In Convolutional Neural Networks</strong></a><br>其实就是卷积核独立不共享,这样做的好处是能反应脸部不同位置的特征.(但也导致参数超多)<br>L4: filter_size=9*9; filter_num=16*55*55;stride=1;pad=0<br>L5: filter_size=7*7; filter_num=16*25*25;stride=2;pad=0<br>L6: filter_size=5*5; filter_num=16*21*21;stride=1;pad=0<br>F7,F8: 全连接层,这两层能对捕捉到图像中较远距离位置的相关性.<br>softmax: 得到k个类的分数p_k<br>loss使用交叉熵函数,即L=-log(p_k)<br>使用SGD最优化方法, RELU激活函数, 在第一个全连接层使用drop-out, 在最后一层对所有特征进行了L2归一化以减弱图像对光照的敏感性.</p>
<h2 id="Verification-Metric-验证"><a href="#Verification-Metric-验证" class="headerlink" title="Verification Metric 验证"></a>Verification Metric 验证</h2><h3 id="加权的-χ-2-距离"><a href="#加权的-χ-2-距离" class="headerlink" title="加权的$χ^2$距离"></a>加权的$χ^2$距离</h3><img src="/2017/06/19/DeepFace-Closing-the-Gap-to-Human-Level-Performance-in-Face-Verification/distance.png" alt="[distance]" title="[distance]">
<p>归一化后的特征向量,所有值在[0,1]之间且非常稀疏.<br>距离公式见上图.</p>
<h3 id="Siamese-network"><a href="#Siamese-network" class="headerlink" title="Siamese network"></a>Siamese network</h3><p>参考:<a href="https://vra.github.io/2016/12/13/siamese-caffe/" target="_blank" rel="external">Caffe中的Siamese网络</a><br>Siamese原意是”泰国的，泰国人”，而与之相关的一个比较常见的词是”Siamese twin”， 意思是”连体双胞胎”，所以Siamemse Network是从这个意思转变而来，指的是结构非常相似的两路网络，分别训练，但共享各个层的参数，在最后有一个连接的部分。Siamese网络对于相似性比较的场景比较有效。此外Siamese因为共享参数，所以能减少训练过程中的参数个数。</p>
<h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><p>在SFC上训练,在LFW和YFW上验证.训练和验证的过程不详细看了,我暂时没有复现的欲望.</p>
<h2 id="疑问和思考"><a href="#疑问和思考" class="headerlink" title="疑问和思考"></a>疑问和思考</h2><ol>
<li>siamese network是如何实现的?</li>
</ol>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="https://asdf0982.github.io/2017/06/18/Deep-Face-recognition/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Liam Wang">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/uploads/avatar2.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Liam's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2017/06/18/Deep-Face-recognition/" itemprop="url">Deep Face Recognition 阅读笔记</a></h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-06-18T14:46:32+08:00">
                2017-06-18
              </time>
            

            
              <span class="post-meta-divider">|</span>
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-check-o"></i>
              </span>
              
                <span class="post-meta-item-text">Post modified</span>
              
              <time title="Post modified" itemprop="dateModified" datetime="2017-06-20T21:22:55+08:00">
                2017-06-20
              </time>
            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>此文由牛津大学视觉研究所于2015年发表.<br>论文链接: <a href="https://www.robots.ox.ac.uk/~vgg/publications/2015/Parkhi15/parkhi15.pdf" target="_blank" rel="external">PDF</a><br>参考: </p>
<ul>
<li><a href="http://yongyuan.name/blog/deep-face-recognition-note.html" target="_blank" rel="external">Deep Face Recognition论文阅读</a></li>
<li><a href="http://blog.csdn.net/kunyxu/article/details/54376850" target="_blank" rel="external">VGG-Face：Deep Face Recognition 笔记</a></li>
<li><a href="http://blog.csdn.net/tangwei2014/article/details/46788025" target="_blank" rel="external">triplet loss 原理以及梯度推导</a></li>
<li><a href="http://blog.csdn.net/sallyxyl1993/article/details/51685093" target="_blank" rel="external">论文：Deep Face Recognition 概括</a></li>
</ul>
<p>本文有两个主要部分:</p>
<ol>
<li>如何用较少的人力, 搜集和建立一个相对优质的数据集.</li>
<li>用深度网络完成人脸识别和验证, 并比较不同的网络结构在不同的数据集上的表现.</li>
</ol>
<h2 id="Dataset-Collection-收集整理数据集"><a href="#Dataset-Collection-收集整理数据集" class="headerlink" title="Dataset Collection 收集整理数据集"></a>Dataset Collection 收集整理数据集</h2><ol>
<li><strong>过滤出候选的人名列表</strong><br>从IMDB资料库中获得5k个人名, 其中男女各一半.使用google图片搜索每个人名搜集200张图片. 筛去图片纯净度低的, 与LFW汗YFW重合人名的, 还剩下2622个人名.</li>
<li><strong>为每个人名收集更多图片</strong><br>在google和bing里搜图并在每个名字之后加上”actor”再搜一遍,每次搜500张,总计单个名字下搜集2000张图片.</li>
<li><strong>使用自动的过滤器提高纯净度</strong><br>基于Google的查询结果, 取前50张图片作为正训练样本. 其他人名查询下的前50张为负训练样本, 使用Fisher Vector Faces训练一个one-vs-rest的线性SVM分类器对2000张图片排序, 保留前1000张.</li>
<li><strong>删除相似的图片</strong><br>抽出不同搜索引擎找到的相同图片和在网路上不同位置的相同图片, 还有那些只是色彩平衡不同的近似图片. 通过计算每张图片的VLAD descriptor, 对每个人名下的1000张图片聚类, 使用一个相当紧缩的阈值范围保留一部分图片.(图表展示为平均623张,不是每个人名下623张.)</li>
<li><strong>最终人工筛选</strong><br>这一步的目的是通过人为注释提高数据集纯度. 但是为了减轻人员标注的负担, 使用AlexNet对所有人名下的图片进行rank. rank完的图片以200张一批的方式展示给注释人员. 如果一批内图像的纯度大约高于95%, 那么就是good. 最终得到982803张图片.</li>
</ol>
<p>搜集过程和用时表格(A代表自动, M代表手动):<br><img src="/2017/06/18/Deep-Face-recognition/Table2.png" alt="[Dataset statistics after each stage of processing]" title="[Dataset statistics after each stage of processing]"><br>与公开的数据集进行对比:<br><img src="/2017/06/18/Deep-Face-recognition/Table1.png" alt="[Dataset comparisons]" title="[Dataset comparisons]"></p>
<h2 id="Network-architecture-and-training-网络结构与训练"><a href="#Network-architecture-and-training-网络结构与训练" class="headerlink" title="Network architecture and training 网络结构与训练"></a>Network architecture and training 网络结构与训练</h2><p>训练采用VGGNet,结构如图:<br><img src="/2017/06/18/Deep-Face-recognition/VGG.png" alt="[Network configuration]" title="[Network configuration]"><br>输入的图像大小为224*224,<br>卷积层的配置为{stride=2;pad=1;filter_size=3*3;filter_num=64}<br>则卷积后输出大小1+(224+2*1-3)/1 = 224,即卷积后仍为224*224的图像,只不过包含64个通道(filter_num).<br>池化层的配置为{stride=2;pad=0;filter_size=2*2}<br>则池化后输出大小(224-2)/2+1 = 112,即池化后为112*112的图像(大小减半,通道数不变)<br>经过一系列的conv-maxpool,最后包含3个FC层(理解为filter和input一样大的卷积层).<br>第一个FC层的输入为7*7, 512通道.filter也为7*7, 512通道.filter_num为4096.故输出大小(1, 4096).<br>网络的最后一层使用<em>softmax log-loss</em>作为分类器.<br>训练结束后, 移除最后一层, 模型最终得到的score vectors可通过计算欧式距离再比较的方法进行人脸对比检测. 然而, 通过在欧式空间里使用”triplet loss”的方法可以使上面的分数向量进一步改善.公式如下:<br><img src="/2017/06/18/Deep-Face-recognition/triplet-loss.png" alt="[tripplet loss]" title="[tripplet loss]"><br>a:anchor; p:positive; n:negative, 参考<a href="http://blog.csdn.net/tangwei2014/article/details/46788025" target="_blank" rel="external"><strong>triplet loss 原理以及梯度推导</strong></a></p>
<h3 id="Architecture"><a href="#Architecture" class="headerlink" title="Architecture"></a>Architecture</h3><p>考虑使用A,B,D三个网络结构, 其中A就是上表展示的VGG网络.最后3个是全连接层, 只是将他看作卷积层(卷积核与输入图像一样大).<br>我猜测BD的结构为VGGNet论文中描述的,如图:<br><img src="/2017/06/18/Deep-Face-recognition/VGGNet.png" alt="[VGGNet]" title="[VGGNet]"><br>B,D的网络结构相比A分别多了2和5个卷积层.<br>输入人脸图像的大小是224*224.</p>
<h3 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h3><p>描述了训练的详细过程, 如使用SGD+momentum的最优化方法, 学习率的设置, 通过翻转图像增大数据集, BD模型均在A模型训练好的基础上进行微调等等.</p>
<h2 id="Experiments-and-results"><a href="#Experiments-and-results" class="headerlink" title="Experiments and results"></a>Experiments and results</h2><p><strong>各个网络结构的表现如下</strong>:(C-数据集搜集和整理中第五步的数据集; F-第三步)<br><img src="/2017/06/18/Deep-Face-recognition/Table4.png" alt="[Performance evaluation on LFW]" title="[Performance evaluation on LFW]"><br>可以发现</p>
<ol>
<li>F数据集表现的更好;</li>
<li>测试时2D对齐能轻微提高表现,但训练时没有帮助; </li>
<li>网络B的表现最好;</li>
<li>验证时的嵌入学习(triplet-loss)能显著提升表现.</li>
</ol>
<p><strong>与最先进的网络对比</strong>:<br><img src="/2017/06/18/Deep-Face-recognition/Table5.png" alt="[Performance evaluation on LFW]" title="[Performance evaluation on LFW]"><br>可以看到此文使用比其他网络少许多的数据集和较少的网络得到与其他先进网络媲美的结果.</p>
<h2 id="疑问和思考"><a href="#疑问和思考" class="headerlink" title="疑问和思考"></a>疑问和思考</h2><ol>
<li>triplet loss具体是在哪一步如何实现的?</li>
<li>为什么净化后的数据集训练效果反而降低了?<br>文章给出了两点可能的解释:1.就算标签存在噪声,数据量越大越好.2.一些微妙的正确图样在步骤5被移除了.</li>
</ol>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="https://asdf0982.github.io/2017/06/16/CS231n学习笔记-LSTM/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Liam Wang">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/uploads/avatar2.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Liam's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2017/06/16/CS231n学习笔记-LSTM/" itemprop="url">CS231n学习笔记(LSTM)</a></h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-06-16T18:04:25+08:00">
                2017-06-16
              </time>
            

            
              <span class="post-meta-divider">|</span>
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-check-o"></i>
              </span>
              
                <span class="post-meta-item-text">Post modified</span>
              
              <time title="Post modified" itemprop="dateModified" datetime="2017-06-16T21:34:15+08:00">
                2017-06-16
              </time>
            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>作业: <a href="/2017/06/16/CS231n学习笔记-LSTM/LSTM_Captioning.ipynb" title="[LSTM_Captioning.ipynb]">[LSTM_Captioning.ipynb]</a><br>参考:<a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" target="_blank" rel="external"><strong>Understanding LSTM Networks</strong></a>和<a href="http://www.jianshu.com/p/e46b1aa48886" target="_blank" rel="external"><strong>CS231n_assignment3</strong></a><br>在使用RNN时,时间(t)过大会导致前后的信息相关程度越来越小,最后出现梯度消失现象.而LSTM(Long Short-Term Memory Networks)就是为了解决这个<strong>长期依赖</strong>问题而被提出的.<br>LSTM的一般结构见下图. 和RNN一样，LSTM也是随着时间序列重复着一样的模块，只是LSTM的每个某块比RNN更加复杂，拥有四个层（3个门+1个记忆单元）。下图方框内上方的那条水平线，被称为胞元状态（cell state），LSTM通过门结构对记忆单元上的信息进行线性修改，保证了当时间序列变得很长的时候，前后信息的关联度不会衰减。<img src="/2017/06/16/CS231n学习笔记-LSTM/LSTM.png" alt="[LSTM]" title="[LSTM]"></p>
<ol>
<li>forget gate:根据上一个时刻输出的h_t-1和当前输入x_t,通过sigmoid控制得到f_t <img src="/2017/06/16/CS231n学习笔记-LSTM/forget_gate.png" alt="[forget gate]" title="[forget gate]"></li>
<li>Input gate:决定哪些值进行更新进cell state.<img src="/2017/06/16/CS231n学习笔记-LSTM/input_gate.png" alt="[input gate]" title="[input gate]"><br>对cell state进行更新<img src="/2017/06/16/CS231n学习笔记-LSTM/update.png" alt="[update cell state]" title="[update cell state]"></li>
<li>Output gate:<img src="/2017/06/16/CS231n学习笔记-LSTM/output_gate.png" alt="[output gate]" title="[output gate]"></li>
</ol>
<p>LSTM还有很多变体见参考<strong>CS231n_assignment3</strong></p>
<h2 id="LSTM-step-forward"><a href="#LSTM-step-forward" class="headerlink" title="LSTM: step forward"></a>LSTM: step forward</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">H = prev_h.shape[1]</div><div class="line">a = x.dot(Wx) + prev_h.dot(Wh) + b</div><div class="line">z_i = sigmoid(a[:,:H]) #input gate it</div><div class="line">z_f = sigmoid(a[:,H:2*H]) #forget gate ft</div><div class="line">z_o = sigmoid(a[:,2*H:3*H])#output gate ot</div><div class="line">z_g = np.tanh(a[:,3*H:]) #input gate Ct</div><div class="line">next_c = z_f * prev_c + z_i * z_g</div><div class="line">z_t = np.tanh(next_c)</div><div class="line">next_h = z_o * z_t</div><div class="line">cache = (z_i, z_f, z_o, z_g, z_t, prev_c, prev_h, Wx, Wh, x)</div></pre></td></tr></table></figure>
<h2 id="LSTM-step-backward"><a href="#LSTM-step-backward" class="headerlink" title="LSTM: step backward"></a>LSTM: step backward</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line">H = dnext_h.shape[1]</div><div class="line">z_i, z_f, z_o, z_g, z_t, prev_c, prev_h, Wx, Wh, x = cache</div><div class="line"></div><div class="line">dz_o = z_t * dnext_h</div><div class="line">dc_t = z_o * (1 - z_t * z_t) * dnext_h + dnext_c</div><div class="line">dz_f = prev_c * dc_t</div><div class="line">dz_i = z_g * dc_t</div><div class="line">dprev_c = z_f * dc_t</div><div class="line">dz_g = z_i * dc_t</div><div class="line">  </div><div class="line">da_i = (1 - z_i) * z_i * dz_i</div><div class="line">da_f = (1 - z_f) * z_f * dz_f</div><div class="line">da_o = (1 - z_o) * z_o * dz_o</div><div class="line">da_g = (1 - z_g * z_g) * dz_g</div><div class="line">da = np.hstack((da_i, da_f, da_o, da_g))</div><div class="line"></div><div class="line">dWx = x.T.dot(da)</div><div class="line">dWh = prev_h.T.dot(da)</div><div class="line">  </div><div class="line">db = np.sum(da, axis = 0)</div><div class="line">dx = da.dot(Wx.T)</div><div class="line">dprev_h = da.dot(Wh.T)</div></pre></td></tr></table></figure>
<h2 id="LSTM-forward"><a href="#LSTM-forward" class="headerlink" title="LSTM: forward"></a>LSTM: forward</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">N, T, D = x.shape</div><div class="line">H = b.shape[0]/4</div><div class="line">h = np.zeros((N, T, H))</div><div class="line">cache = &#123;&#125;</div><div class="line">prev_h = h0</div><div class="line">prev_c = np.zeros((N, H))</div><div class="line">for t in range(T):</div><div class="line">  xt = x[:, t, :]</div><div class="line">  next_h, next_c, cache[t] = lstm_step_forward(xt, prev_h, prev_c, Wx, Wh, b)</div><div class="line">  prev_h = next_h</div><div class="line">  prev_c = next_c</div><div class="line">  h[:, t, :] = prev_h</div></pre></td></tr></table></figure>
<h2 id="LSTM-backward"><a href="#LSTM-backward" class="headerlink" title="LSTM: backward"></a>LSTM: backward</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line">N, T, H = dh.shape</div><div class="line">z_i, z_f, z_o, z_g, z_t, prev_c, prev_h, Wx, Wh, x = cache[T-1]</div><div class="line">D = x.shape[1]</div><div class="line">  </div><div class="line">dprev_h = np.zeros((N, H))</div><div class="line">dprev_c = np.zeros((N, H))</div><div class="line">dx = np.zeros((N, T, D))</div><div class="line">dh0 = np.zeros((N, H))</div><div class="line">dWx= np.zeros((D, 4*H))</div><div class="line">dWh = np.zeros((H, 4*H))</div><div class="line">db = np.zeros((4*H,))</div><div class="line">  </div><div class="line">for t in range(T):</div><div class="line">  t = T-1-t</div><div class="line">  step_cache = cache[t]</div><div class="line">  dnext_h = dh[:,t,:] + dprev_h</div><div class="line">  dnext_c = dprev_c</div><div class="line">  dx[:,t,:], dprev_h, dprev_c, dWxt, dWht, dbt = lstm_step_backward(dnext_h, dnext_c, step_cache)</div><div class="line">  dWx, dWh, db = dWx+dWxt, dWh+dWht, db+dbt</div><div class="line">  </div><div class="line">dh0 = dprev_h</div></pre></td></tr></table></figure>
          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="https://asdf0982.github.io/2017/06/14/CS231n学习笔记-RNN/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Liam Wang">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/uploads/avatar2.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Liam's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2017/06/14/CS231n学习笔记-RNN/" itemprop="url">CS231n学习笔记(RNN)</a></h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-06-14T16:18:19+08:00">
                2017-06-14
              </time>
            

            
              <span class="post-meta-divider">|</span>
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-check-o"></i>
              </span>
              
                <span class="post-meta-item-text">Post modified</span>
              
              <time title="Post modified" itemprop="dateModified" datetime="2017-06-16T21:35:03+08:00">
                2017-06-16
              </time>
            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>作业: <a href="/2017/06/14/CS231n学习笔记-RNN/RNN_Captioning.ipynb" title="[RNN_Captioning.ipynb]">[RNN_Captioning.ipynb]</a><br>数据集准备:打开datesets内的shell文件,找到url拖至迅雷里下载至datasets并解压.<br>运行至第三个代码块时可能运行至<code>os.remove(fname)</code>报错.是因为文件缓存被python打开无法删除导致的,注释掉这句即可.<br><code>np.linspace</code>在指定的间隔内返回均匀间隔的数字.</p>
<h2 id="Vanilla-RNN-step-forward"><a href="#Vanilla-RNN-step-forward" class="headerlink" title="Vanilla RNN: step forward"></a>Vanilla RNN: step forward</h2><p>打开<code>cs231n/rnn_layers.py</code>找到<code>rnn_step_forward(x, prev_h, Wx, Wh, b)</code><br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">a = prev_h.dot(Wh) + x.dot(Wx) + b</div><div class="line">next_h = np.tanh(a)</div><div class="line">cache = (x, prev_h, Wh, Wx, b, next_h)</div></pre></td></tr></table></figure></p>
<p>即next_h = tanh(x*Wx+pre_h*Wh+b)</p>
<h2 id="Vanilla-RNN-step-backward"><a href="#Vanilla-RNN-step-backward" class="headerlink" title="Vanilla RNN: step backward"></a>Vanilla RNN: step backward</h2><p>找到<code>rnn_step_backward(dnext_h, cache)</code><br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">x, prev_h, Wh, Wx, b, next_h = cache</div><div class="line">da = dnext_h * (<span class="number">1</span> - next_h ** <span class="number">2</span>) <span class="comment">#(N,H)</span></div><div class="line">dx = np.dot(da, Wx.T) <span class="comment">#(N,D)</span></div><div class="line">dprev_h = np.dot(da, Wh.T) <span class="comment">#(N,H)</span></div><div class="line">dWx = np.dot(x.T, da) <span class="comment">#(D,H)</span></div><div class="line">dWh = np.dot(prev_h.T, da) <span class="comment">#(H,H)</span></div><div class="line">db = np.sum(da, axis = <span class="number">0</span>) <span class="comment">#(H,)</span></div></pre></td></tr></table></figure></p>
<p>若激活函数为sigmoid,那么$f’(z) = f(z)(1-f)$<br>若激活函数为tanh,那么$f’(z) = 1-(f(z))^2$</p>
<h2 id="Vanilla-RNN-forward"><a href="#Vanilla-RNN-forward" class="headerlink" title="Vanilla RNN: forward"></a>Vanilla RNN: forward</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">N, T, D = x.shape</div><div class="line">(H,) = b.shape</div><div class="line">h = np.zeros((N, T,H))</div><div class="line">prev_h = h0</div><div class="line"><span class="keyword">for</span> t <span class="keyword">in</span> range(T):</div><div class="line">  xt = x[:, t, :]</div><div class="line">  next_h,_ = rnn_step_forward(xt, prev_h, Wx, Wh, b)</div><div class="line">  prev_h = next_h</div><div class="line">  h[:, t, :] = prev_h</div><div class="line">cache = (x, h0, Wh, Wx, b, h)</div></pre></td></tr></table></figure>
<h2 id="Vanilla-RNN-backward"><a href="#Vanilla-RNN-backward" class="headerlink" title="Vanilla RNN: backward"></a>Vanilla RNN: backward</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line">x, h0, Wh, Wx, b, h = cache</div><div class="line">N, T, H = dh.shape</div><div class="line">D = x.shape[<span class="number">2</span>]</div><div class="line">next_h = h[:, T<span class="number">-1</span>, :]</div><div class="line">dprev_h = np.zeros((N, H))</div><div class="line">dx = np.zeros((N, T, D))</div><div class="line">dh0 = np.zeros((N, H))</div><div class="line">dWx = np.zeros((D, H))</div><div class="line">dWh = np.zeros((H, H))</div><div class="line">db = np.zeros((H,))</div><div class="line"><span class="keyword">for</span> t <span class="keyword">in</span> range(T):</div><div class="line">  t = T<span class="number">-1</span>-t </div><div class="line">  xt = x[:, t, :] <span class="comment">#(N,D)</span></div><div class="line">  <span class="keyword">if</span> t==<span class="number">0</span>:</div><div class="line">    prev_h = h0</div><div class="line">  <span class="keyword">else</span>:</div><div class="line">    prev_h = h[:,t<span class="number">-1</span>,:]</div><div class="line">  step_cache = (xt, prev_h, Wh, Wx, b, next_h)</div><div class="line">  next_h = prev_h</div><div class="line">  dnext_h = dh[:, t, :] + dprev_h</div><div class="line">  dx[:, t, :], dprev_h, dWxt, dWht, dbt = rnn_step_backward(dnext_h, step_cache)</div><div class="line">  dWx, dWh, db = dWx+dWxt, dWh+dWht, db+dbt</div><div class="line">dh0 = dprev_h</div></pre></td></tr></table></figure>
<h2 id="Word-embedding-forward"><a href="#Word-embedding-forward" class="headerlink" title="Word embedding: forward"></a>Word embedding: forward</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">N, T = x.shape</div><div class="line">V, D = W.shape</div><div class="line">out = np.zeros((N, T, D))</div><div class="line">for i in range(N):</div><div class="line">  for j in range(T):</div><div class="line">    out[i, j] = W[x[i,j]]</div><div class="line">cache = (x, W.shape)</div></pre></td></tr></table></figure>
<p>词嵌入(word embedding)参考文章:<a href="http://licstar.net/archives/328" target="_blank" rel="external">Deep Learning in NLP （一）词向量和语言模型</a><br>这里W代表词典表有V个词, 每个词都用D维数组表示. 输入的x共有N句, 每句有T个词.<br>上面的代码就是将x里的词提出来,映射到W上.</p>
<h2 id="Word-embedding-backward"><a href="#Word-embedding-backward" class="headerlink" title="Word embedding: backward"></a>Word embedding: backward</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">x, W_shape = cache</div><div class="line">dW = np.zeros(W_shape)</div><div class="line">np.add.at(dW, x, dout)</div></pre></td></tr></table></figure>
<p>np.add.at表示dW在x的位置上,分别加上加上dout.例子如下:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; a = np.array([1, 2, 3, 4])</div><div class="line">&gt;&gt;&gt; np.add.at(a, [0, 1, 2, 2], 1)</div><div class="line">&gt;&gt;&gt; print(a)</div><div class="line">array([2, 3, 5, 4])</div><div class="line"></div><div class="line">&gt;&gt;&gt; a = np.array([1, 2, 3, 4])</div><div class="line">&gt;&gt;&gt; b = np.array([1, 2])</div><div class="line">&gt;&gt;&gt; np.add.at(a, [0, 1], b)</div><div class="line">&gt;&gt;&gt; print(a)</div><div class="line">array([2, 4, 3, 4])</div></pre></td></tr></table></figure></p>
<h2 id="CaptioningRNN-loss"><a href="#CaptioningRNN-loss" class="headerlink" title="CaptioningRNN.loss"></a>CaptioningRNN.loss</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div></pre></td><td class="code"><pre><div class="line">affine_out, affine_cache = affine_forward(features ,W_proj, b_proj)</div><div class="line">#(2)</div><div class="line">word_embedding_out, word_embedding_cache = word_embedding_forward(captions_in, W_embed)</div><div class="line">#(3)</div><div class="line">if self.cell_type == &apos;rnn&apos;:</div><div class="line">    rnn_or_lstm_out, rnn_cache = rnn_forward(word_embedding_out, affine_out, Wx, Wh, b)</div><div class="line">elif self.cell_type == &apos;lstm&apos;:</div><div class="line">    rnn_or_lstm_out, lstm_cache = lstm_forward(word_embedding_out, affine_out, Wx, Wh, b)</div><div class="line">else:</div><div class="line">    raise ValueError(&apos;Invalid cell_type &quot;%s&quot;&apos; % self.cell_type)</div><div class="line">#(4)</div><div class="line">temporal_affine_out, temporal_affine_cache = temporal_affine_forward(rnn_or_lstm_out, W_vocab, b_vocab)</div><div class="line">#(5)</div><div class="line">loss, dtemporal_affine_out = temporal_softmax_loss(temporal_affine_out, captions_out, mask)</div><div class="line">#(4)</div><div class="line">drnn_or_lstm_out, grads[&apos;W_vocab&apos;], grads[&apos;b_vocab&apos;] = temporal_affine_backward(dtemporal_affine_out, temporal_affine_cache)</div><div class="line">#(3)</div><div class="line">if self.cell_type == &apos;rnn&apos;:</div><div class="line">    dword_embedding_out, daffine_out, grads[&apos;Wx&apos;], grads[&apos;Wh&apos;], grads[&apos;b&apos;] = rnn_backward(drnn_or_lstm_out, rnn_cache)</div><div class="line">elif self.cell_type == &apos;lstm&apos;:</div><div class="line">    dword_embedding_out, daffine_out, grads[&apos;Wx&apos;], grads[&apos;Wh&apos;], grads[&apos;b&apos;] = lstm_backward(drnn_or_lstm_out, lstm_cache)</div><div class="line">else:</div><div class="line">    raise ValueError(&apos;Invalid cell_type &quot;%s&quot;&apos; % self.cell_type)</div><div class="line">#(2)</div><div class="line">grads[&apos;W_embed&apos;] = word_embedding_backward(dword_embedding_out, word_embedding_cache)</div><div class="line">#(1)</div><div class="line">dfeatures, grads[&apos;W_proj&apos;], grads[&apos;b_proj&apos;] = affine_backward(daffine_out, affine_cache)</div></pre></td></tr></table></figure>
<h2 id="CaptioningRNN-sample"><a href="#CaptioningRNN-sample" class="headerlink" title="CaptioningRNN.sample"></a>CaptioningRNN.sample</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line">N, D = features.shape</div><div class="line">affine_out, affine_cache = affine_forward(features ,W_proj, b_proj)</div><div class="line"></div><div class="line">prev_word_idx = [self._start]*N</div><div class="line">prev_h = affine_out</div><div class="line">prev_c = np.zeros(prev_h.shape)</div><div class="line">captions[:,<span class="number">0</span>] = self._start</div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>,max_length):</div><div class="line">    prev_word_embed  = W_embed[prev_word_idx]</div><div class="line">    <span class="keyword">if</span> self.cell_type == <span class="string">'rnn'</span>:</div><div class="line">        next_h, rnn_step_cache = rnn_step_forward(prev_word_embed, prev_h, Wx, Wh, b)</div><div class="line">    <span class="keyword">elif</span> self.cell_type == <span class="string">'lstm'</span>:</div><div class="line">        next_h, next_c,lstm_step_cache = lstm_step_forward(prev_word_embed, prev_h, prev_c, Wx, Wh, b)</div><div class="line">        prev_c = next_c</div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        <span class="keyword">raise</span> ValueError(<span class="string">'Invalid cell_type "%s"'</span> % self.cell_type)</div><div class="line">    vocab_affine_out, vocab_affine_out_cache = affine_forward(next_h, W_vocab, b_vocab)</div><div class="line">    captions[:,i] = list(np.argmax(vocab_affine_out, axis = <span class="number">1</span>))</div><div class="line">    prev_word_idx = captions[:,i]</div><div class="line">    prev_h = next_h</div></pre></td></tr></table></figure>
          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
  </section>

  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="page-number" href="/page/3/">3</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right"></i></a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      

      <section class="site-overview sidebar-panel sidebar-panel-active">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/uploads/avatar2.jpg"
               alt="Liam Wang" />
          <p class="site-author-name" itemprop="name">Liam Wang</p>
           
              <p class="site-description motion-element" itemprop="description"></p>
           
        </div>
        <nav class="site-state motion-element">

          
            <div class="site-state-item site-state-posts">
              <a href="/archives">
                <span class="site-state-item-count">25</span>
                <span class="site-state-item-name">posts</span>
              </a>
            </div>
          

          

          
            
            
            <div class="site-state-item site-state-tags">
              <a href="/tags/index.html">
                <span class="site-state-item-count">10</span>
                <span class="site-state-item-name">tags</span>
              </a>
            </div>
          

        </nav>

        

        <div class="links-of-author motion-element">
          
            
              <span class="links-of-author-item">
                <a href="https://github.com/asdf0982" target="_blank" title="GitHub">
                  
                    <i class="fa fa-fw fa-github"></i>
                  
                  GitHub
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="https://twitter.com/ghat0982" target="_blank" title="Twitter">
                  
                    <i class="fa fa-fw fa-twitter"></i>
                  
                  Twitter
                </a>
              </span>
            
          
        </div>

        
        

        
        

        


      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy; 
  <span itemprop="copyrightYear">2017</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Liam Wang</span>
</div>


<div class="powered-by">
  Powered by <a class="theme-link" href="https://hexo.io">Hexo</a>
</div>

<div class="theme-info">
  Theme -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Mist
  </a>
</div>


        
<div class="busuanzi-count">
  <script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  

  
</div>


        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.1"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.1"></script>



  
  

  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.1"></script>



  


  




	





  





  





  






  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (search_path.endsWith("json")) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
