<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[CS231n学习笔记(全连接网络)]]></title>
    <url>%2F2017%2F06%2F01%2FCS231n%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E5%85%A8%E8%BF%9E%E6%8E%A5%E7%BD%91%E7%BB%9C%2F</url>
    <content type="text"><![CDATA[进入了assignment2学习，打开FullyConnectedNets.ipynb，有一大段引言我试着翻译一下。 在之前的作业中，你在CIFAR-10数据集上实现了一个2层的全连接神经网络。这次简单的实现并不十分模块化，因为loss和梯度都在一块函数中计算。对于一个简单的2层网络来说，这样是可管理的。但随着我们向更大的模型推进时，这样做又变得不切实际。在理想情况下，我们希望使用更模块化的设计来独立实现不同类型的层，然后根据不同模型的结构把他们拼到一起。这次作业我们将使用更模块化的方法来实现一个全连接的神经网络。每一层我们都将实现一个前向和反向传播的功能。前向传播接收输入，权重和其他参数，并返回反向传播所需的输出和缓存对象。如下：12345678910def layer_forward(x, w): """ Receive inputs x and weights w """ # Do some computations ... z = # ... some intermediate value # Do some more computations ... out = # the output cache = (x, w, z, out) # Values we need to compute gradients return out, cache 而反向传播将接收上行导数（？）和缓存对象，并返回根据梯度调整的输入和权重。如下：12345678910111213def layer_backward(dout, cache): """ Receive derivative of loss with respect to outputs and cache, and compute derivative with respect to inputs. """ # Unpack cache values x, w, z, out = cache # Use values in cache to compute derivatives dx = # Derivative of loss with respect to x dw = # Derivative of loss with respect to w return dx, dw 以这种模块化的方式实现了一些层后，我们就可以轻松地把它们组合起来去构建不同结构的分类器了。除了能实现任意深度的全连接网络，我们也能探索不同的最优化更新规则并引入Dropout方法作为规则化和批规范化的工具使优化深度网络更高效。]]></content>
      <tags>
        <tag>Deep Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Date型数据在SSM架构下的参数绑定与回显]]></title>
    <url>%2F2017%2F05%2F31%2FDate%E5%9E%8B%E6%95%B0%E6%8D%AE%E5%9C%A8SSM%E6%9E%B6%E6%9E%84%E4%B8%8B%E7%9A%84%E5%8F%82%E6%95%B0%E7%BB%91%E5%AE%9A%E4%B8%8E%E5%9B%9E%E6%98%BE%2F</url>
    <content type="text"><![CDATA[Date型数据有很多种表现形式，传递起来相对来说比较复杂。最近做的项目涉及到这方面内容，做个笔记。 Date型数据的参数绑定。想要将jsp页面收到String型参数转为Date型参数再保存到实体类中，首先需要创建一个转换器。创建Controller包,姑且命名为liam.ssm.controller.converter。在其中创建一个类(Class),命名为CustomDateConverter.java123456789101112131415public class CustomDateConverter implements Converter&lt;String, Date&gt; &#123; @Override public Date convert(String source) &#123; try &#123; SimpleDateFormat simpleDateFormat = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss"); //String型数据的格式为"yyyy-MM-dd HH:mm:ss"才能被正确转换 return simpleDateFormat.parse(source); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; return null; //转换失败返回空 &#125;&#125; 然后由于spingmvc使用注解驱动。需要在配置文件(springmvc.xml)将刚刚设置的转换器配置进去。123456789101112&lt;mvc:annotation-driven conversion-service="conversionService"&gt;&lt;/mvc:annotation-driven&gt;&lt;!-- conversionService --&gt; &lt;bean id="conversionService" class="org.springframework.format.support.FormattingConversionServiceFactoryBean"&gt; &lt;!-- 转换器 --&gt; &lt;property name="converters"&gt; &lt;list&gt; &lt;bean class="liam.ssm.controller.converter.CustomDateConverter"/&gt; &lt;/list&gt; &lt;/property&gt; &lt;/bean&gt; 这样参数绑定就完成了，下面看看jsp中的相关代码1&lt;input name="liam.time" id="time" value="2017-5-31 18:02:12"/&gt; 到后台controller，假如我们创建过liam这个实体类，那么time这个参数应该能正确传递。（本质上以上的操作均属于自定义的参数绑定，可以应用到其他类型绑定上去。）1234@RequestMapping("/test") public String test(Liam liam)throws Exception&#123; //TODO 设置有个断点，进来看看liam内包含的time是否正确 &#125; Date型数据的回显如果简单使用&lt;input name=&quot;liam.time&quot; id=&quot;time&quot; value=&quot;${liam.time}&quot;/&gt;会发现显示的时间是Wed May 31 18:11:32 CST 2017这样的形式。为了将其原模原样的回显成yyyy-MM-dd HH:mm:ss格式我使用JSTL标签。12&lt;%@taglib uri="http://java.sun.com/jsp/jstl/fmt" prefix="fmt"%&gt;&lt;fmt:formatDate value="$&#123;liam.time&#125;" pattern="yyyy-MM-dd HH:mm:ss"/&gt; 如果想要在input输入框回显，不显示具体时间，则这样1&lt;input name=&quot;liam.time&quot; value=&apos;&lt;fmt:formatDate value=&quot;$&#123;liam.time &#125;&quot; pattern=&quot;yyyy-MM-dd&quot; /&gt;&apos; /&gt; 注意：yyyy-MM-dd HH:mm:ss中的字符大小写不要随意修改，也许包含不同的含义。 字母 日期或时间元素 表示 示例 G Era 标志符 Text AD y 年 Year 1996 ; 96 M 年中的月份 Month July ; Jul ; 07 w 年中的周数 Number 27 W 月份中的周数 Number 2 D 年中的天数 Number 189 d 月份中的天数 Number 10 F 月份中的星期 Number 2 E 星期中的天数 Text Tuesday ; Tue a Am/pm 标记 Text PM H 一天中的小时数（0-23） Number 0 k 一天中的小时数（1-24） Number 24 K am/pm 中的小时数（0-11） Number 0 h am/pm 中的小时数（1-12） Number 12 m 小时中的分钟数 Number 30 s 分钟中的秒数 Number 55 S 毫秒数 Number 978 z 时区 General time zone Pacific Standard Time ; PST ; GMT-08:00 Z 时区 RFC 822 time zone -0800]]></content>
      <tags>
        <tag>spring mvc</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CS231n学习笔记-两层网络]]></title>
    <url>%2F2017%2F05%2F29%2FCS231n%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E4%B8%A4%E5%B1%82%E7%BD%91%E7%BB%9C%2F</url>
    <content type="text"><![CDATA[完成作业 [two_layer_net.ipynb] [neural_net.py] 前向传播计算图像在各类下的分数在neural_net.py下编辑loss函数123L1 = ReLU(np.dot(X,W1)+b1)L2 = np.dot(L1,W2)+b2 # N,Cscores = L2 L1代表第一层，使用ReLU作为激活函数。而ReLU函数没有定义，所以需要在TwoLayerNet外建一个。12def ReLU(x): return np.maximum(0,x) # f(z) = max(0, z) .ipynb文件中预置了正确答案，可以进行比对。我的如下：12345678910111213141516Your scores:[[-0.81233741 -1.27654624 -0.70335995] [-0.17129677 -1.18803311 -0.47310444] [-0.51590475 -1.01354314 -0.8504215 ] [-0.15419291 -0.48629638 -0.52901952] [-0.00618733 -0.12435261 -0.15226949]]correct scores:[[-0.81233741 -1.27654624 -0.70335995] [-0.17129677 -1.18803311 -0.47310444] [-0.51590475 -1.01354314 -0.8504215 ] [-0.15419291 -0.48629638 -0.52901952] [-0.00618733 -0.12435261 -0.15226949]]Difference between your scores and correct scores:3.68027204961e-08 计算loss1234567scores_max = np.max(scores, axis=1, keepdims=True)exp_scores = np.exp(scores - scores_max)probs = exp_scores/np.sum(exp_scores, axis=1, keepdims=True)correct_logprobs = -np.log(probs[range(N),y])data_loss = np.sum(correct_logprobs)/Nreg_loss = 0.5 * reg * (np.sum(W1**2)+np.sum(W2**2))loss = data_loss + reg_loss 这里使用softmax的方法来计算。第一句keepdims参数设置为True时，scores_max.shape为(5,1),方便之后shape为(5,3)的scores通过广播能与score_max相减。keepdim使用说明 计算gradient1234567891011121314151617181920descores = probs # N,Cdescores[range(N),y] += -1descores /= N# W2,b2dW2 = np.dot(L1.T, descores) # H,Cdb2 = np.sum(descores,axis=0,keepdims=True) # 1,C# L1dL1 = np.dot(descores,W2.T) # N,H# ReLudL1[L1 &lt;= 0] = 0# W1,b1dW1 = np.dot(X.T,dL1) # D,Hdb1 = np.sum(dL1,axis=0,keepdims=True)# regularizationdW1 += reg * W1dW2 += reg * W2grads['W1'] = dW1grads['b1'] = db1grads['W2'] = dW2grads['b2'] = db2 softmax作业中gradient是严格按照公式计算的，如下123y_trueClass = np.zeros_like(prob)y_trueClass[range(num_train), y] = 1.0 # N by CdW += -np.dot(X.T, y_trueClass - prob) / num_train + reg * W 而这次作业里代码将公式中第一个负号去掉，trueclass与prob位置对调。 微调参数我调整了迭代次数，学习率和学习率下降等参数。训练后用图像展示训练过程，发现最好的模型就在默认的提供的参数附近哈哈哈。12345678910111213plt.subplot(2, 1, 1)plt.plot(mytrain['loss_history'])plt.title('Loss history')plt.xlabel('Iteration')plt.ylabel('Loss')plt.subplot(2, 1, 2)plt.plot(mytrain['train_acc_history'], label='train')plt.plot(mytrain['val_acc_history'], label='val')plt.title('Classification accuracy history')plt.xlabel('Epoch')plt.ylabel('Clasification accuracy')plt.show() 我很喜欢这样的图片展示，这是我其中一个模型的。]]></content>
      <tags>
        <tag>Deep Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CS231n学习笔记(softmax)]]></title>
    <url>%2F2017%2F05%2F28%2FCS231n%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-softmax%2F</url>
    <content type="text"><![CDATA[完成作业: [softmax.ipynb] [softmax.py] softmax的计算方式与SVM类似。参考此图其loss和gradient的计算推导过程如图]]></content>
      <tags>
        <tag>Deep Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[UEditor的数据回显]]></title>
    <url>%2F2017%2F05%2F26%2FUEditor%E7%9A%84%E6%95%B0%E6%8D%AE%E5%9B%9E%E6%98%BE%2F</url>
    <content type="text"><![CDATA[最近遇到了ueditor的不能显示数据的回显问题。解决思路： 在jsp页面添加div类型的隐藏栏，作为中转站保存数据。 使用js代码配置ueditor面板，从上获得数据并展示 12&lt;div id="temp" style="display: none;"&gt;$&#123;***.**&#125;&lt;/div&gt;&lt;srcipt id="container" name="content" type="text/plain"&gt;&lt;/srcipt&gt; 12345&lt;script type="text/javascript"&gt;var ue = UE.getEditor('container');ue.ready(function()&#123;ue.setContent($('#temp').html());&#125;); Tips： 最初我用的办法是&lt;input id=&quot;temp&quot; type=&quot;hidden&quot; value=&quot;${***.**}&quot;&gt;和ue.setContent($(&#39;#temp&#39;).val());图像和信息能正确上传但不能回显。原因是数据库中图像的内容为&lt;p&gt;&lt;img src=&quot;http://localhost:8080/…/ueditor/jsp/upload/image/20170602/**.png……&quot;&gt;这里src=之后的双引号会和value=后的双引号配对，导致图像的路径信息无法传入，所以改为上面的方法。 使用eclipse+tomcat调试项目，图像保存在workspace\.metadata\.plugins\org.eclipse.wst.server.core\tmp0\wtpwebapps\myown\ueditor\jsp\upload\image下的日期目录（其中workspace代表eclipse的默认工程路径，myown是我的项目名称）。在eclipse项目内点击ueditor-&gt;jsp-&gt;config.json中调整图像的保存路径。我修改为&quot;imageUrlPrefix&quot;: &quot;/myown&quot;,]]></content>
      <tags>
        <tag>Tips</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[连接电脑时蓝牙耳机音质明显下降]]></title>
    <url>%2F2017%2F05%2F25%2F%E8%BF%9E%E6%8E%A5%E7%94%B5%E8%84%91%E6%97%B6%E8%93%9D%E7%89%99%E8%80%B3%E6%9C%BA%E9%9F%B3%E8%B4%A8%E6%98%8E%E6%98%BE%E4%B8%8B%E9%99%8D%2F</url>
    <content type="text"><![CDATA[先比较连接手机时的音质，判断是否为耳机自身故障。 确定非耳机故障时，右击【蓝牙设备】图标-【显示蓝牙设备】 在相关设置下选择【设备和打印机】 右击【蓝牙耳机】图标-声音设置 观察到一个耳机却又两个模式，点开Stereo模式的【属性】-选择【高级】 观察到“2通道，CD音质”说明现在使用的耳机模式是听音乐的。 作为对比回去再打开Hands-Free模式的【属性】【高级】 观察到“1通道，电话音质”，这就是我之前使用的，音质明显下降的版本。 选择Stereo为默认模式。]]></content>
      <tags>
        <tag>Tips</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CS231n 学习笔记(SVM)]]></title>
    <url>%2F2017%2F05%2F24%2FCS231n-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-SVM%2F</url>
    <content type="text"><![CDATA[完成作业： [svm.ipynb] [linear_svm.py] [linear_classifier.py] hstack(),vstack()12345678910111213141516171819202122import numpy as npx = np.array([[1,2],[3,4],[5,6]])a = np.ones((x.shape[0],1))b = np.ones((1,x.shape[1]))c = np.hstack([x,a])d = np.vstack([x,b])print (a)print (b)print (c)print (d)[[ 1.] [ 1.] [ 1.]][[ 1. 1.]][[ 1. 2. 1.] [ 3. 4. 1.] [ 5. 6. 1.]][[ 1. 2.] [ 3. 4.] [ 5. 6.] [ 1. 1.]] h-horizontalv-vertical难以表达自行感受 numpy.random.randn(d0, d1, …, dn)其中d0, d1, …, dn为整数型，输出标准正太分布的矩阵。若想输出$N(\mu,\sigma^2)$则公式为：$\sigma$*np.random.randn(…)+$\mu$若我们要生成满足正太分布为N(3，2.5^2)，2行4列的数组，则2.5*np.random.randn(2, 4)+3输出12array([[-4.49401501, 4.00950034, -1.81814867, 7.29718677], [ 0.39924804, 4.68456316, 4.99394529, 4.84057254]]) svm_loss_naive()123456789101112131415161718192021222324252627282930313233343536373839404142434445464748def svm_loss_naive(W, X, y, reg): """ Structured SVM loss function, naive implementation (with loops). Inputs have dimension D, there are C classes, and we operate on minibatches of N examples. Inputs: - W: A numpy array of shape (D, C) containing weights. - X: A numpy array of shape (N, D) containing a minibatch of data. - y: A numpy array of shape (N,) containing training labels; y[i] = c means that X[i] has label c, where 0 &lt;= c &lt; C. - reg: (float) regularization strength Returns a tuple of: - loss as single float - gradient with respect to weights W; an array of same shape as W """ dW = np.zeros(W.shape) # initialize the gradient as zero # compute the loss and the gradient num_classes = W.shape[1] num_train = X.shape[0] loss = 0.0 for i in xrange(num_train): scores = X[i].dot(W) #scores包含10个类别的分数 correct_class_score = scores[y[i]] #正确类别下的分数 for j in xrange(num_classes): if j == y[i]: continue margin = scores[j] - correct_class_score + 1 # note delta = 1 if margin &gt; 0: loss += margin dW[:,y[i]] += X[i,:] dW[:,j] += -X[i,:] # Right now the loss is a sum over all training examples, but we want it # to be an average instead so we divide by num_train. loss /= num_train dW /= num_train # Add regularization to the loss. loss += 0.5 * reg * np.sum(W * W) dW += reg * W return loss, dW dW部分参考：梯度推导 梯度的抽样检测1234567891011121314151617181920def grad_check_sparse(f, x, analytic_grad, num_checks=10, h=1e-5): """ sample a few random elements and only return numerical in this dimensions. """ for i in xrange(num_checks): ix = tuple([randrange(m) for m in x.shape]) oldval = x[ix] x[ix] = oldval + h # increment by h fxph = f(x) # evaluate f(x + h) x[ix] = oldval - h # increment by h fxmh = f(x) # evaluate f(x - h) x[ix] = oldval # reset grad_numerical = (fxph - fxmh) / (2 * h) grad_analytic = analytic_grad[ix] rel_error = abs(grad_numerical - grad_analytic) / (abs(grad_numerical) + abs(grad_analytic)) print 'numerical: %f analytic: %f, relative error: %e' % (grad_numerical, grad_analytic, rel_error) 即判断$ \frac{f(x+h)-f(x-h)}{2h} = \frac{\partial f(x)}{\partial x} $ svm_loss_vectorized(W, X, y, reg)123456789101112131415161718192021222324def svm_loss_vectorized(W, X, y, reg): """ Structured SVM loss function, vectorized implementation.Inputs and outputs are the same as svm_loss_naive. """ loss = 0.0 dW = np.zeros(W.shape) # initialize the gradient as zero scores = X.dot(W) # num_train by num_class num_train = X.shape[0] num_classes = W.shape[1] scores_correct = scores[np.arange(num_train), y] # 1 by num_train #print scores_correct scores_correct = np.reshape(scores_correct, (num_train, 1)) # num_train by 1 margins = scores - scores_correct + 1.0 # num_train by num_class margins[np.arange(num_train), y] = 0.0 # 将正确标签下的margins置为0 margins[margins &lt;= 0] = 0.0 # 将值小于0的margins置为0 #print margins[0] loss += np.sum(margins) / num_train loss += 0.5 * reg * np.sum(W * W) # compute the gradient margins[margins &gt; 0] = 1.0 row_sum = np.sum(margins, axis=1) # 1 by num_train margins[np.arange(num_train), y] = -row_sum dW += np.dot(X.T, margins)/num_train + reg * W # dimension by num_class 关于loss就是将大于0的margins相加关于gradient即将margins中的数值分类。再通过最后一步 dW += np.dot(X.T, margins)/num_train + reg * W 完成梯度的计算。本质上与naive函数相同1234567 if margin &gt; 0: loss += margin dW[:,y[i]] += -X[i:] dW[:,j] += X[i:] dW /= num_traindW += reg * W PS:在linear_classifier.py中X的shape是D*N所以在一些地方要将X转置才行。]]></content>
      <tags>
        <tag>Deep Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CS231n学习笔记]]></title>
    <url>%2F2017%2F05%2F22%2FCS231n%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[完成作业: [k_nearest_neighbor.py] [knn.ipynb] enumerate()12for y, cls in enumerate(classes): print y, cls enumerate能遍历一个数组或列表，并获得索引和索引内容。得到：123456789100 plane1 car2 bird3 cat4 deer5 dog6 frog7 horse8 ship9 truck flatnonzero()12idxs = np.flatnonzero(y_train == y)print idxs y_train 为训练集图片的标签集。flatnonzero() 打印非零元素的位置。整体意为以图片标签分类，输出图片的下标。12345678910[ 29 30 35 ..., 49941 49992 49994][ 4 5 32 ..., 49993 49998 49999][ 6 13 18 ..., 49987 49991 49995][ 9 17 21 ..., 49979 49982 49983][ 3 10 20 ..., 49981 49984 49990][ 27 40 51 ..., 49964 49980 49988][ 0 19 22 ..., 49962 49966 49996][ 7 11 12 ..., 49965 49978 49986][ 8 62 69 ..., 49968 49976 49985][ 1 2 14 ..., 49963 49971 49997] np.random.choice()官方介绍12idxs = np.random.choice(idxs, samples_per_class, replace=False) print idxs 从idxs中随机取出samples_per_class个数，不可重复。 subplot(m,n,p)1plt.subplot(samples_per_class, num_classes, plt_idx) subplot(m,n,p)是将多个图画到一个平面上的工具。其中，m表示是图排成m行，n表示图排成n列，也就是整个figure中有n个图是排成一行的，一共m行，如果m=2就是表示2行图。p表示图所在的位置，p=1表示从左到右从上到下的第一个位置。 reshape()1X_train = np.reshape(X_train, (X_train.shape[0], -1)) 如果newshape给的参数是（x,-1）,那么函数会自动判别newshape为(x, mn/x）,这里的x一定要能被m*n整除！stackoverflow有相关的介绍。‘-1’在此处的意义-1表示我懒得计算该填什么数字，让机器通过其他的值推测出来。 k_nearest_neighbor.py 作业12345678910111213for i in xrange(num_test): for j in xrange(num_train): ##################################################################### # TODO: # # Compute the l2 distance between the ith test point and the jth # # training point, and store the result in dists[i, j]. You should # # not use a loop over dimension. # ##################################################################### dists[i,j] = np.sum((X[i,:]-self.X_train[j,:])**2) ##################################################################### # END OF YOUR CODE # ##################################################################### return dists 这里代表X为测试集，X_train是训练集。X有500个点，每个点代表一张图片，有32*32维。每个点的维度都与训练集X_train相减后求平方的和。得到测试集与训练集的点的“距离”。 Inline Question #1: Notice the structured patterns in the distance matrix, where some rows or columns are visible brighter. (Note that with the default color scheme black indicates low distances while white indicates high distances.) What in the data is the cause behind the distinctly bright rows? What causes the columns?Your Answer: 明亮的行表示当前测试图与多数训练图相似度低，明亮的列表示当前训练图与多数测试图相似度低 argsort()返回的是数组值从小到大的索引值123x = np.array([3, 1, 2])np.argsort(x)array([1, 2, 0]) most_common()获取出现频率最高的s个字符1234567from collections import Counters = '''A Counter is a dict subclass for counting hashable objects. It is an unordered collection where elements are stored as dictionary keys and their counts are stored as dictionary values. Counts are allowed to be any integer value including zero or negative counts. The Counter class is similar to bags or multisets in other languages.'''.lower()c = Counter(s)# 获取出现频率最高的5个字符print c.most_common(5)# Result:[(' ', 54), ('e', 32), ('s', 25), ('a', 24), ('t', 24)] numpy.linalg.norm(x, ord=None, axis=None, keepdims=False)官方解释ord表示求哪种范数，默认为L2范数。axis表示按照什么维度。0就是x轴，（0就是竖着比较，1就是横着比较。）具体看下面例子。12345678&gt;&gt;&gt; c = np.array([[ 1, 2, 3],... [-1, 1, 4]])&gt;&gt;&gt; LA.norm(c, axis=0)array([ 1.41421356, 2.23606798, 5. ])&gt;&gt;&gt; LA.norm(c, axis=1)array([ 3.74165739, 4.24264069])&gt;&gt;&gt; LA.norm(c, ord=1, axis=1)array([ 6., 6.]) numpy.ndarray.T矩阵的转置1234567&gt;&gt;&gt; x = np.array([[1.,2.],[3.,4.]])&gt;&gt;&gt; xarray([[ 1., 2.], [ 3., 4.]])&gt;&gt;&gt; x.Tarray([[ 1., 3.], [ 2., 4.]]) 作业 no-loops部分1234M = np.dot(X, self.X_train.T)te = np.square(X).sum(axis = 1)tr = np.square(self.X_train).sum(axis = 1)dists = np.sqrt(-2*M+tr+np.matrix(te).T) 利用(a-b)^2=a^2+b^2-2ab和numpy的broadcasting性质。]]></content>
      <tags>
        <tag>Deep Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ssm架构下json数据传递问题和解决]]></title>
    <url>%2F2017%2F05%2F18%2Fssm%E6%9E%B6%E6%9E%84%E4%B8%8Bjson%E6%95%B0%E6%8D%AE%E4%BC%A0%E9%80%92%E9%97%AE%E9%A2%98%E5%92%8C%E8%A7%A3%E5%86%B3%2F</url>
    <content type="text"><![CDATA[环境说明 springmvc-4.3.6 jar包 jackson-2.7.4 [jar]包 代码说明在springmvc.xml文件中采用&lt;mvc:annotation-driven/&gt;注解驱动。在jsp文件下,配置一个按钮调用下面的js代码12345678910function requestJson()&#123; $ajax(&#123; type:"post", url:"$&#123;pageContext.request.contextPath &#125;/requestJson.action", data:'&#123;"articleid":10,"title":"test"&#125;', success:function(data)&#123; alert(data); &#125; &#125;);&#125; 在后台创建一个controller。传入json数据，@RequestBody转为实体类。传出实体类，使用@ResponseBody转为json格式。1234567@Controllerpublic class JsonTest&#123; @RequestMapping("/requestJson") public @ResponseBody items requestJson(@RequestBody Items items)&#123; return items; &#125;&#125; 在浏览器中打开jsp页面，F12控制台观察requestJson.action的状态码 问题1.状态码200/415，不进入后端controller断点，浏览器弹出网页html代码解决办法：缺少class，更换jar包。 2.状态码400/500，不进入后端controller断点，浏览器无反应解决办法：jsp页面中data数据需采用正确的json编写格式，以逗号分隔。]]></content>
      <tags>
        <tag>spring mvc</tag>
        <tag>json</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[windows下使用hexo+github部署个人博客]]></title>
    <url>%2F2017%2F05%2F17%2F20170517%2F</url>
    <content type="text"><![CDATA[需要装好git, node.js 并全程科学上网(全局代理)。 使用gitbash npm install hexo 新建文件夹用于个人博客，并进入目录下hexo init 继续使用hexo g 和hexo s 生成并本地测试网站, 打开浏览器输入localhost:4000 ，此时需临时关闭全局代理方可看到本地博客。 配置config.yml文件(关键为deploy区域，其余部分自由发挥) 1234deploy: type: git repo: https://github.com/asdf0982/asdf0982.github.io.git branch: master 使用hexo d 将本地文档上传到github(需全局代理保证网络链接)如一切正常会弹出一个github登录框。关于ssh的配置，其实只是省略了这个登录的步骤。]]></content>
      <tags>
        <tag>git</tag>
        <tag>hexo</tag>
        <tag>windows</tag>
      </tags>
  </entry>
</search>