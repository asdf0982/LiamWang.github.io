<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[pycaffeExample]]></title>
    <url>%2F2017%2F08%2F29%2FpycaffeExample%2F</url>
    <content type="text"><![CDATA[一直在用命令行运行caffe, 现在来试试它的python接口.最近在看compact bilinear pooling相关的内容, github上有完整的网络结构prototxt, 适合拿来练习.文件:[compact_bilinear.py]首先你得确定已经make pycaffe接下来创建一个python文件.1234567891011121314151617# -*- coding: utf-8 -*-import caffe from caffe import layers as L, params as P, proto, to_proto# 设定文件的保存路径root = '/home/liangw/work/caffe/' # 根目录#train_list = root + 'mnist/train/train.txt' # 训练图片列表#test_list = root + 'mnist/test/test.txt' # 测试图片列表train_lmdb = root + 'examples/compact_bilinear/train_lmdb' # 训练集lmdbtest_lmdb = root + 'examples/compact_bilinear/val_lmdb' # 看不顺眼"+"号? 你也可以采用os.path.join()这样的写法 #train_meanfile = root + 'data/cifar100_train_mean.binaryproto' #训练集均值文件 #test_meanfile = root + 'data/cifar100_test_mean.binaryproto'train_proto = root + 'examples/attentive_cb/attentive_cb_train.prototxt' # 生成的训练配置文件test_proto = root + 'examples/attentive_cb/attentive_cb_test.prototxt' # 生成的测试配置文件solver_proto = root + 'examples/attentive_cb/attentive_cb_solver.prototxt' # 生成的参数文件weight = root + 'examples/attentive_cb/VGG_ILSVRC_16_layers.caffemodel' # finetune所需的caffemodel 上面的内容作用相当于一个config.123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687# 定义网络结构def Network(lmdb, batch_size, include_acc=False, finetune_last=False): n = caffe.NetSpec() if include_acc: # 是否需要测试 n.data,n.label = L.Data(source=lmdb, backend=P.Data.LMDB, batch_size=batch_size, ntop=2, transform_param=dict(mirror=False, crop_size=448, mean_value=[104,117,123])) else: #假如测试, 数据就不需要扩增了,即mirror=False n.data,n.label = L.Data(source=lmdb, backend=P.Data.LMDB, batch_size=batch_size, ntop=2, transform_param=dict(mirror=True, crop_size=448, mean_value=[104,117,124])) #使用VGG作为基础网络 if (finetune_last == True): # 由于是finetune我们需要先训练新加上去的层,故原有的层权重不变 w_lr_mult = 0.0 w_decay_mult = 0.0 b_lr_mult = 0.0 b_decay_mult = 0.0 else: w_lr_mult = 1.0 w_decay_mult = 1.0 b_lr_mult = 2.0 b_decay_mult = 0.0# 关于层的使用,建议去看caffe.proto文件, 其中有详细的定义.当我不知道层中的参数该如何写的时候, 我会到github去搜别人的代码. n.conv1_1 = L.Convolution(n.data, kernel_size=3, pad=1, num_output=64, param=[dict(lr_mult=w_lr_mult, decay_mult=w_decay_mult),\ dict(lr_mult=b_lr_mult, decay_mult=b_decay_mult)]) n.relu1_1 = L.ReLU(n.conv1_1) n.conv1_2 = L.Convolution(n.relu1_1, kernel_size=3, pad=1, num_output=64, param=[dict(lr_mult=w_lr_mult, decay_mult=w_decay_mult),\ dict(lr_mult=b_lr_mult, decay_mult=b_decay_mult)]) n.relu1_2 = L.ReLU(n.conv1_2) n.pool1 = L.Pooling(n.relu1_2, pool=P.Pooling.MAX, kernel_size=2, stride=2) n.conv2_1 = L.Convolution(n.pool1, kernel_size=3, pad=1, num_output=128, param=[dict(lr_mult=w_lr_mult, decay_mult=w_decay_mult),\ dict(lr_mult=b_lr_mult, decay_mult=b_decay_mult)]) n.relu2_1 = L.ReLU(n.conv2_1) n.conv2_2 = L.Convolution(n.relu2_1, kernel_size=3, pad=1, num_output=128, param=[dict(lr_mult=w_lr_mult, decay_mult=w_decay_mult),\ dict(lr_mult=b_lr_mult, decay_mult=b_decay_mult)]) n.relu2_2 = L.ReLU(n.conv2_2) n.pool2 = L.Pooling(n.relu2_2, pool=P.Pooling.MAX, kernel_size=2, stride=2) n.conv3_1 = L.Convolution(n.pool2, kernel_size=3, pad=1, num_output=256, param=[dict(lr_mult=w_lr_mult, decay_mult=w_decay_mult),\ dict(lr_mult=b_lr_mult, decay_mult=b_decay_mult)]) n.relu3_1 = L.ReLU(n.conv3_1) n.conv3_2 = L.Convolution(n.relu3_1, kernel_size=3, pad=1, num_output=256, param=[dict(lr_mult=w_lr_mult, decay_mult=w_decay_mult),\ dict(lr_mult=b_lr_mult, decay_mult=b_decay_mult)]) n.relu3_2 = L.ReLU(n.conv3_2) n.conv3_3 = L.Convolution(n.relu3_2, kernel_size=3, pad=1, num_output=256, param=[dict(lr_mult=w_lr_mult, decay_mult=w_decay_mult),\ dict(lr_mult=b_lr_mult, decay_mult=b_decay_mult)]) n.relu3_3 = L.ReLU(n.conv3_3) n.pool3 = L.Pooling(n.relu3_3, pool=P.Pooling.MAX, kernel_size=2, stride=2) n.conv4_1 = L.Convolution(n.pool3, kernel_size=3, pad=1, num_output=512, param=[dict(lr_mult=w_lr_mult, decay_mult=w_decay_mult),\ dict(lr_mult=b_lr_mult, decay_mult=b_decay_mult)]) n.relu4_1 = L.ReLU(n.conv4_1) n.conv4_2 = L.Convolution(n.relu4_1, kernel_size=3, pad=1, num_output=512, param=[dict(lr_mult=w_lr_mult, decay_mult=w_decay_mult),\ dict(lr_mult=b_lr_mult, decay_mult=b_decay_mult)]) n.relu4_2 = L.ReLU(n.conv4_2) n.conv4_3 = L.Convolution(n.relu4_2, kernel_size=3, pad=1, num_output=512, param=[dict(lr_mult=w_lr_mult, decay_mult=w_decay_mult),\ dict(lr_mult=b_lr_mult, decay_mult=b_decay_mult)]) n.relu4_3 = L.ReLU(n.conv4_3) n.pool4 = L.Pooling(n.relu4_3, pool=P.Pooling.MAX, kernel_size=2, stride=2) n.conv5_1 = L.Convolution(n.pool4, kernel_size=3, pad=1, num_output=512, param=[dict(lr_mult=w_lr_mult, decay_mult=w_decay_mult),\ dict(lr_mult=b_lr_mult, decay_mult=b_decay_mult)]) n.relu5_1 = L.ReLU(n.conv5_1) n.conv5_2 = L.Convolution(n.relu5_1, kernel_size=3, pad=1, num_output=512, param=[dict(lr_mult=w_lr_mult, decay_mult=w_decay_mult),\ dict(lr_mult=b_lr_mult, decay_mult=b_decay_mult)]) n.relu5_2 = L.ReLU(n.conv5_2) n.conv5_3 = L.Convolution(n.relu5_2, kernel_size=3, pad=1, num_output=512, param=[dict(lr_mult=w_lr_mult, decay_mult=w_decay_mult),\ dict(lr_mult=b_lr_mult, decay_mult=b_decay_mult)]) n.relu5_3 = L.ReLU(n.conv5_3) #bilinear n.bilinear_layer = L.CompactBilinear(n.relu5_3, n.relu5_3, compact_bilinear_param=dict(num_output=8192,sum_pool=False)) #sqrt+l2 n.signed_sqrt = L.SignedSqrt(n.bilinear_layer) n.l2_normalization= L.L2Normalize(n.signed_sqrt) #FC n.fc = L.InnerProduct(n.l2_normalization, num_output=200, param=[dict(lr_mult=1, decay_mult=1),\ dict(lr_mult=2, decay_mult=0)], weight_filler=dict(type='gaussian',std=0.0001), bias_filler=dict(type='constant',value=0.0)) n.loss = L.SoftmaxWithLoss(n.fc, n.label) if include_acc: # test阶段需要有accuracy层 n.accuracy = L.Accuracy(n.fc, n.label) return n.to_proto() else: return n.to_proto() 以上就是网络结构的定义, 直观上来说将会生成train.prototxt/test.prototxt这样的文件. 1234567891011121314151617181920212223242526272829# 编写一个函数，生成参数文件def gen_solver(solver_file, train_net, test_net): s = proto.caffe_pb2.SolverParameter() s.train_net = train_net s.test_net.append(test_net) s.test_interval = 4000 # 每训练多少次进行一次测试 s.test_iter.append(1450) # 测试样本数/test_batch_size,如10000/100=100 s.test_initialization=False s.average_loss=100 s.max_iter = 60000 # 最大训练次数 s.base_lr = 1 # 基础学习率 s.momentum = 0.9 # 动量 s.weight_decay = 0.000005 # 权值衰减项 s.lr_policy = 'multistep' # 学习率变化规则 s.stepvalue.append(20000) s.stepvalue.append(30000) s.stepvalue.append(40000) s.stepvalue.append(50000) #s.stepsize = 3000 # 学习率变化频率 s.gamma = 0.25 # 学习率变化指数 s.display = 100 # 屏幕显示间隔 s.snapshot = 10000 # 保存caffemodel的间隔 s.snapshot_prefix = root + 'examples/attentive_cb/snapshot/att_cb' # caffemodel前缀 s.type = 'SGD' # 优化算法 s.solver_mode = proto.caffe_pb2.SolverParameter.GPU # 加速 # 写入solver.prototxt with open(solver_file, 'w') as f: f.write(str(s)) 以上将写入solver.prototxt12345678910111213# 开始训练def training(solver_proto): caffe.set_device(2) #选择哪块GPU caffe.set_mode_gpu() solver = caffe.SGDSolver(solver_proto) solver.net.copy_from(weight) # 载入需finetune的预训练模型 solver.solve()#主函数if __name__ == '__main__': write_net(finetune_last=True) gen_solver(solver_proto, train_proto, test_proto) training(solver_proto) 于是在caffe的根目录下python ./需运行的python文件.py即可]]></content>
      <tags>
        <tag>Caffe</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[jupyternotebook服务器端]]></title>
    <url>%2F2017%2F08%2F14%2Fjupyternotebook-server%2F</url>
    <content type="text"><![CDATA[首先要生成密码，打开python终端。 1234567In [1]: from IPython.lib import passwdIn [2]: passwd()Enter password: Verify password: Out[2]: &apos;sha1:0e422dfccef2:84cfbcb b3ef95872fb8e23be3999c123f862d856&apos; 接下来生成秘钥：openssl req -x509 -nodes -days 365 -newkey rsa:1024 -keyout mycert.pem -out mycert.pem 创建一个服务器配置：ipython profile create nbserver 产生一个配置文件jupyter notebook --generate-config 编辑文件jupyter_notebook_config.py，在~/.jupyter/文件夹里找. 12345c.NotebookApp.password = u&apos;sha1:拷贝上面生成的密码&apos;c.NotebookApp.certfile = u&apos;/用户的根目录/.jupyter/mycert.pem&apos;c.NotebookApp.ip = &apos;*&apos;c.NotebookApp.port = 9998(关联的端口号,只要是空闲的就行)c.NotebookApp.notebook_dir (用于配置启动时的根目录) 最后启动服务器：ipython notebook --profile=nbserver 在浏览器中输入https://服务器IP:端口号 配置 anaconda 环境 1. 在 kirino 环境下安装 ipykernel 2. python -m ipykernel install --name kirino --p /home/chenchao/.conda]]></content>
      <tags>
        <tag>Tips</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深度学习调参技巧]]></title>
    <url>%2F2017%2F08%2F14%2FTipsInDL%2F</url>
    <content type="text"><![CDATA[持续更新… 扩大数据集常用的方法有将图像水平翻转, 随机剪裁,fancy PCA等.data augmentation 数据增强方法总结如在caffe的prototxt中加上mirror crop_size123456789101112131415161718192021layer &#123; name: &quot;data&quot; type: &quot;Data&quot; top: &quot;data&quot; top: &quot;label&quot; include &#123; phase: TRAIN &#125; transform_param &#123; mirror: true crop_size: 224 mean_value: 123.0 mean_value: 123.0 mean_value: 123.0 &#125; data_param &#123; source: &quot;examples/****/train_lmdb&quot; batch_size: 8 backend: LMDB &#125; &#125; 图像预处理 零均值+归一化 PCA白化白化（Whitening） PCA白化 ZCA白化 训练过程学习率 推荐从0.1开始向下调整学习率.当感觉loss不再下降时,可以将当前学习率除以2/5/10继续训练,看看loss是否继续降. 如果遇到loss一直上涨的情况,可能遇到了梯度过大的问题. 在caffe的solver中可以设置clip_gradients来抑制梯度. 使用Batch Normalization,即把数据处理成均值为零,方差为1. BN操作能解决梯度弥散或梯度爆炸问题,使w更新更为稳健. 在公开模型的基础上继续训练可以从Caffe Model Zoo下载模型,新增层继续训练,节约时间.不过不要只关心模型结构, 想在公开模型上finetune的话,对应层的名字也要一样. 从loss曲线中得到信息 激活函数现常用ReLU. 可以考虑换成PReLU 正则化与随机失活正则化方法：L1和L2 regularization、数据集扩增、dropout]]></content>
      <tags>
        <tag>Deep Learning</tag>
        <tag>Tips</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Factorized Bilinear Models for Image Recognition 阅读笔记]]></title>
    <url>%2F2017%2F07%2F06%2FFactorized-Bilinear%2F</url>
    <content type="text"><![CDATA[关于细粒度(fine-grained), 还记得CIFAR-10数据集吗?里面有卡车, 飞机等10个截然不同的类别.这是用于传统的区分, 而细粒度就是如CUB鸟类数据集, 里面都是鸟,不过包含了不同的种类. 对这些看似相同实际不同种类的图像进行区分,就是细粒度区分.主要做了这些事: 提出了Factorized Bilinear layer来增强CNN层的能力,并使它更好的并入卷积和全链接层. 为了解决过拟合问题, 提出了DropFactor的正则化(regularization)方法. 回顾Bilinear Pooling也就是回顾[Bilinear CNN Models for Fine-grained Visual Recognition]这篇论文.输入特征的大小是(h,w,n), 一般情况下h,w比较小,n比较大, 表现为一个长方体. 那么这里的$x_i$, 就是长方体中的一条(1,1,n).当然以上都是我的假设,因为最终z的大小为(n,n)所以$x_i$的大小实际上是(n,1).在之后的全链接层, y=b+Wz, z原本是一个(n,n)的矩阵被拉成了$(n^2,1)$的向量.合并上面的两个式子看到xWx的格式这两个x就代表了(双线性)bilinear的意义, 实验也证明双线性的方法在细粒度领域是有效的.同时我们也观察到Bilinear Pooling有大量的参数和很高的计算负担.并且参数过多容易导致过拟合,需要适当调整. Factorized Bilinear model基于上述的模型的优缺点,提出了FB模型.其实就是将原本xWx(W的大小为NN)的部分改为了xFFx(F的大小为KN).疑惑:1. 这里的$x_i$有n个,不知道这个n和上述的bilibearpooling的n维是不是同一个意思.(应该不是.)2. $(f_i,f_j)$是什么? 反向传播时算法的梯度公式:计算复杂度:O(kn) DropFactor (a)图不使用DropFactor,所有路径都要跑(注意共1+k条)所以为了防止过拟合,引入公式:疑惑: $m_j$ ~ Bernoulli(p)中间的符号 为什么测试时都乘以p? 实验结果]]></content>
      <tags>
        <tag>Deep Learning</tag>
        <tag>fine-grained classification</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kernel Pooling for Convolutional Neural Networks 阅读笔记]]></title>
    <url>%2F2017%2F07%2F04%2FKernel-Pooling%2F</url>
    <content type="text"><![CDATA[参考: Global average Pooling 基础-径向基(Radial basis function)神经网络、核函数的一些理解 进阶-径向基函数（RBF）神经网络 DFT-傅里叶分析之掐死教程 kernel SVM-支持向量机通俗导论（理解SVM的三层境界） Introduction 提出了一种对特征映射的一般内核池化(kernel pooling)方法。 提出的内核池化是可以区分的，可以与CNN进行联合优化。 kernel全连接层的参数太多,平均池化层又一刀切损失关键特征.怎么办啊!使用我们的Tayler系列核函数吧!它可以近似高斯GBF. 从张量中获得特征特征映射X一般有3个维度,h,w,c.我们根据c将X表示为$\mathbf{x} = [x_1,x_2,…,x_c]^T$论文提出Taylor series kernel来抽取特征.首先定义2级张量:也就是$xx^T$.注: 当$x=(x_1,x_2),y=(y_1,y_2)$时,传统的内积$&lt;x,y&gt;=x^Ty=x_1y_1+x_2y_2$如图为可视化的2级3级张量:引申出p级张量(p&gt;2):Taylor series kernel的公式为:将其表示为:$\kappa(\mathbf{x},\mathbf{y})=\phi(\mathbf{x})^T\phi(\mathbf{y})$则其特征向量维度为:$\bigcirc(c^p)$ 所以即使c=512,p=3也有$10^8$.因此需要一个近似方法. 近似方法基于Tensor Sketching. Taylor series kernel 算法的流程如下:这样我们就把特征减少至$d=1+c+\sum_{i=2}^pd_i$在实验中发现更高维度的特征有更好的近似,p越大错误越大,错误也和两个特征向量的夹角有关. Gaussian RBF kernel使用Taylor series kernel 来近似Gaussian RBF.p=4时最合适. Learning kernel composition end-to-end]]></content>
      <tags>
        <tag>Deep Learning</tag>
        <tag>fine-grained classification</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CS231n学习笔记(NetworkVisualization-TensorFlow)]]></title>
    <url>%2F2017%2F07%2F02%2FCS231n%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-NetworkVisualization%2F</url>
    <content type="text"><![CDATA[完成的作业:[NetworkVisualization-TensorFlow.ipynb]这次作业探究了3种图像生成方法: Saliency Maps: 探究图像的哪些部分影响了网络的验证. Fooling Images: 改变一张图像, 让它在你眼中看起来没什么变化, 但会被网络识别成目标的分类. Class Visualization: 合成一张目标类别的图像. 预训练的模型采用SqueezeNet, 它可以在显著减少参数的情况下达到AlexNet的识别率. 在cs231n/classifiers/squeezenet.py看到网络模型代码.Saliency Maps少量改变图像中的每一个像素, 网络分类器分数的变化情况就能反应该图像的重要性.(本文的代码块相应的填入NetworkVisualization-TensorFlow.ipynb中)12345passgradlossimg = tf.gradients(correct_scores, model.image) tgradlossimg = gradlossimg[0]tsaliency = tf.reduce_max(tf.abs(tgradlossimg), 3)saliency = sess.run(tsaliency, feed_dict=&#123;model.image: X, model.labels: y&#125;) Fooling Images123456789passtxf = tf.Variable(X_fooling)sess.run(tf.variables_initializer([txf]))targetclass_score = tf.gather_nd(model.classifier, tf.stack((tf.range(X.shape[0]), model.labels), axis=1))g = tf.gradients(targetclass_score, model.image)[0]dX = learning_rate * g / tf.norm(g)txf = tf.assign_add(txf, dX)for _ in range(100): X_fooling = sess.run(txf, feed_dict=&#123;model.image: X_fooling, model.labels:[target_y]&#125;) Class visualization$I$ 是图像, $y$ 目标类别, $s_y(I)$ 卷据网络关于 $I$ 在 $y$ 类上的分数.$$I^* = \arg\max_I s_y(I) - R(I)$$注意 $R(I)$ 在 argmax 中.$R(I) = \lambda |I|_2^2$123456789loss = None # scalar lossgrad = None # gradient of loss with respect to model.image, same size as model.imagepasstloss = tf.gather_nd(model.classifier, tf.stack((tf.range(X.shape[0]), model.labels), axis=1))treg = l2_reg * tf.square(tf.norm(model.image))loss = tloss - treggrad = tf.gradients(loss, model.image)[0]grad_normalized = grad / tf.norm(grad) 123passdX_normalized = sess.run(grad_normalized, feed_dict=&#123;model.image: X, model.labels:[target_y]&#125;) X += learning_rate * dX_normalized]]></content>
      <tags>
        <tag>Deep Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SphereFace:Deep Hypersphere Embedding for Face Recognition 阅读笔记]]></title>
    <url>%2F2017%2F06%2F30%2FSphereFace%2F</url>
    <content type="text"><![CDATA[Introduction先前的特征学习大多基于欧几里得边缘, 但是欧式边缘就适合那些学习到的具有判别力的特征吗? 本文就提出了一个组合的角度边缘来替代它.所设计的loss名为A-Softmax loss, 提取到的特征名为SphereFace本文主要有以下贡献: 提出了A-Softmax loss来学习特征, 其原理在几何上能够解释. 导出参数m的下限, 使得A-Softmax loss能够让最小类间距离大于最大类内距离. Deep Hypersphere Embedding回顾softmax loss在一个二分类问题中, softmax loss的决策边界(即正确类别的分数要比错误类别的分数要高, 其下限就是相等,也就是决策边界)是$(W_1-W_2)x+b_1-b_2=0$. 把$W^T_ix+b_i$写成$||W^T_i||||x||cos(\theta_i)+b_i$. 其中$\theta_i$是$W_i$与$x$的夹角. 如果把W进行归一化($||W_i||=1$)把b置零($b_i=0$). 这样决策边界是$cos(\theta_1)-cos(\theta_2)=0$. 最后修改后的loss为: $y_i$是样本的标签, K是类的数量, j是[1,K]中的一个元素.向softmax引入角度边界我们都知道二分类问题的modified softmax loss需要$cos(\theta_1)&gt;cos(\theta_2)$,论文将其改为$cos(m\theta_1)&gt;cos(\theta_2)$.因为cos()在$(0,\pi)$中是递减的,所以当$m\geq2, \theta \in [0, \frac{\pi}{m} ]$时,$cos(\theta_1)&gt;cos(m\theta_1)$.所以这将让决策更严格,因为让$cos(\theta_1)$更小了但依然比$cos(\theta_2)$大.决策边界如图所示: 然后把$cos(\theta_{y_i,i})$转化为$\psi(\theta_{y_i,i})$, 如图: A-softmax的超球面解释如图: A-softmax的特性特性1. m越大越不好. 存在一个最小的m能让最大类内距离小于最小类间距离.特性2. 在二分类问题中, $m_{min}\geq2+\sqrt3$. 文中给出证明.特性3. 在多分类问题中, $m_{min}\geq3$. 文中给出证明.基于以上,实验中使用$m=4$]]></content>
      <tags>
        <tag>Deep Learning</tag>
        <tag>Face Recognition</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Coupled Deep Learning for Heterogeneous Face Recognition 阅读笔记]]></title>
    <url>%2F2017%2F06%2F28%2FCDL%2F</url>
    <content type="text"><![CDATA[Introduction不同模态(例如近红外NIR和可见光VIS)采集的人脸照片的匹配问题被称为异构人脸匹配(Heterogeneous face matching, HFM)问题. 不同模态的数据之间差异较大, 并且缺失足够的训练样本对.本文提出一个深度学习网络, 名为CDL(coupled deep learning).致力于解决HFM问题.主要想法如下: 提出一个组合迹范数(trace norm).它不但能加强不同模态之间(如VIS-NIR之间)的相关性, 而且可以约束参数空间, 特别是减轻在少量不成对的异质样本上的过拟合. 由于难以直接优化高阶组合迹范数(trace norm), 我们引入了其近似公式, 并提出了一种交互算法，以便在end-to-end CNN中有效优化。 使用一个基于跨模态三元组的跨模态排名采样方法(A cross-modal ranking sampling method defined on a set of cross-modal triplets)来最大化不同类别之间的差距. 此外, 它能有效的扩大训练数据并利用有限数量异质样本之间的信息. 在CASIA NIR-VIS 2.0面部数据库和三个素描照片数据库进行了广泛的实验评估, 表明所提出的方法提高了异构面部识别的最新性能。 方法相关性约束对CNN中的softmax loss引入相关性约束.一般情况下,loss函数这么写. $X_i$表示抽出的特征.$W_i$表示网络中的参数.N和V表示NIR和VIS.引入相关性约束后: $|| M ||_*$就表示迹范数(trace norm).是这样定义的: tr():表示矩阵的迹inf():表示最大下界$\Gamma$是一个半定矩阵.并由M得到.将M引入公式,得到 则反向传播中$W_N$和$W_V$的梯度: 事实上这一步我也不懂.跨模态排名采样方法和triplet的思想类似,loss定义如下: 在三元组中, a和n选自相同模态(如VIS)不同的类别, a和p选自相同的类别不同的模态.其约束如图: 网络结构使用light CNN作为基础网络.因为NIR-VIS的训练集很小, 先把网络在有大量VIS图片的数据集中训练, 然后用NIR-VIS微调.在基础网络上修改,将上述的相关性约束和排名采样方法结合起来作为监督信号,如图: CDL的算法流程图: 疑问和思考线性代数部分的知识遗忘的比较厉害…]]></content>
      <tags>
        <tag>Deep Learning</tag>
        <tag>Face Recognition</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Joint Face Detection and Alignment using Multi-task Cascaded Convolutional Networks 阅读笔记]]></title>
    <url>%2F2017%2F06%2F27%2FMTCNN%2F</url>
    <content type="text"><![CDATA[参考: MTCNN（Multi-task convolutional neural networks）人脸对齐 项目地址: 原版matlab, python, C++Approach整体流程整个方法如图所示: 首先将一张图像调整至不同的大小来构建图像金字塔,作为下面操作的输入. 通过一个名为P-Net的全由卷积层构建的网络获得候选的面部窗口和边界框回归向量.然后根据边界框回归向量对候选窗口进行校准, 并使用NMS合并高度重叠的候选窗口. 将候选窗口输入到另一个名为R-Net的CNN.它能拒绝掉大量的非人脸窗口.也需要用边界框回归向量校准和NMS. 与第二步类似,输入到一个名为O-Net的CNN中.但这一步用更多的监督来分辨人脸区域.最终输出5个面部基准点. CNN结构 这一部分可以参考原版代码.在卷积层后使用PReLU激活. 训练训练MTCNN包含3个部分, 人脸/非人脸分类器, 边界框回归和面部基准点定位。 人脸分类:这是一个2分类问题.对于每一个样本$x_i$使用交叉熵计算loss. 边界框回归:这是一个回归问题,使用欧式距离计算loss.(y有四个维度:左上角点的x,y,高度,宽度.下方公式前者是预测值,后者是实际值) 面部基准点定位:和边界框回归类似, 可看作回归问题, 也使用欧式距离计算loss.公式如上.(y表示基准点的坐标,共有5个点,故y有10维) 多资源训练:N:训练样本数; $\alpha_j$:任务的重要程度. 在P-Net和R-Net使用(1, 0.5, 0.5)分配, 在O-Net使用(1, 0.5, 1)分配.$\beta$代表样本的标签. 线上的困难样本挖掘:在每个mini-batch中选出loss位于前70%的样本作为困难样本, 并且在BP阶段只利用这部分计算梯度. ExperimentsIoU表示与任何真实人脸的交并集 IoU&lt;0.3:非人脸(Negatives) 0.3&lt;IoU&lt;0.4:脸部基准点(landmarks) 0.4&lt;IoU&lt;0.65:部分人脸 IoU&gt;0.65:人脸 训练集数据按以上划分方式,由3:1:1:2(非人脸:人脸:部分人脸:基准点)数据组成.]]></content>
      <tags>
        <tag>Deep Learning</tag>
        <tag>Face Detection</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[A Discriminative Feature Learning Approach for Deep Face Recognition 阅读笔记]]></title>
    <url>%2F2017%2F06%2F27%2Fcenter-loss%2F</url>
    <content type="text"><![CDATA[发表于ECCV2016项目地址: caffe, mxnet参考: 【Technical Review】ECCV16 Center Loss及其在人脸识别中的应用 Center Loss - A Discriminative Feature Learning Approach for Deep Face Recognition 论文理解 Introduction传统模型抽取出的深度特征比较分散, 是因为softmax loss仅仅区分数据.论文提出了一种新的损失函数center loss. 它可以让每个类学习一个特征中心, 减小类内的距离. 结合softmax loss就可以达到增大类间差异, 减小类内差异的效果.The Proposed Approach先用如下网络LeNet++训练MNIST数据集 固定输出特征为2维,这样我们就可以在平面上看到结果.如图: 结论: 虽然类间的差距已经明显, 但类内的差距依然较大.The center loss $c_{y_i}$代表$y_i$所在类的中心.一次性将训练集的特征全部提取求出中心不现实, 论文中提出两个解决办法. 在mini-batch中更新. 为了避免类别标错的图片使中心抖动, 使用$\alpha$来控制中心的学习率.见下下图Algorithm1-output-6 center loss的梯度如下计算:训练模型时将softmax loss与center loss结合, 设置$\lambda$来平衡两个loss函数.不同的$\lambda$会带来不同的效果:不同的λ对于特征的分布有很大的影响，合适的λ会提高网络的精确度。center loss与contrastive loss,triplet loss相比, 不需要复杂的重建训练集,也不会造成数据戏剧性的大增. 实验部分见论文.注意激活函数使用了PReLU 疑问和思考 为什么总loss不用(1-$\lambda$)a+$\lambda$b这样的形式?]]></content>
      <tags>
        <tag>Deep Learning</tag>
        <tag>Face Recognition</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Going Deeper with Convolutions 阅读笔记]]></title>
    <url>%2F2017%2F06%2F26%2FGoogleNet%2F</url>
    <content type="text"><![CDATA[本文由google发表于CVPR2015 Introduction &amp; Related Work本文提出一个网络结构名为Inception. 它的特点是能提升对计算资源的使用效率.使得 在计算负担不变的情况下提升了网络的宽度和深度.它的前身GoogleNet(22层)在ILSVRC14中创下了验证和检测的记录.GoogleNet借鉴了许多NIN的思想.其1x1的卷积核主要用于降维来解决计算瓶颈问题, 同时也能提高网络的深度和宽度.检测(detection)方法来源于R-CNN.分两步: 利用颜色, 文本等低层次的线索生成未知对象的位置提案. 在这些位置使用CNN分类器对物体进行检测. Motivation目前提升CNN性能的方法就是使网络变大变深. 但这也会增加计算的消耗, 并且容易导致过拟合. 增大的网络还会导致大量的计算资源被浪费.解决这些问题的方法之一就是将全连接层改为稀疏连接层. 哪个结构用这个方法用的最好呢?对啦,就是我们的Inception结构. Architechture如图:图片被多个大小的卷积核卷积, 提取出来的特征到下一层汇总. GoogleNet如图:可以参考阅读:Caffe中GoogleNet的写法#3x3 reduce代表3x3卷积前1x1滤波器的个数.所有激活函数使用RELU.第一个卷积层输出的计算为$\frac{224+2*3-7}{2}+1=112$(注意是向下取整)第一个pool层输出的计算为$\frac{112-3}{2}+1$发现并不能整除.故一定自动补了padding.结果为56inception(3a) 256 = 64(#11)+128(#33)+32(#5*5)+32(pool), 就是把各种方法得到的相同大小的结果拼接到一块输出.使用avg pool层代替全连接层在top-1的准确率上获得了0.6%的提升. 训练采用Large scale distributed deep networks提到的分布置信网络(DistBelief). SGD+momentum的最优化方法, momentum=0.9, lr每8个epoch下降4%.详见论文.]]></content>
      <tags>
        <tag>Deep Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[FaceNet:A Unified Embedding for Face Recognition and Clustering 阅读笔记]]></title>
    <url>%2F2017%2F06%2F24%2FFaceNet%2F</url>
    <content type="text"><![CDATA[本文由GOOGLE于2015年发表在CVPR参考: 谷歌人脸识别系统FaceNet解析 FaceNet–Google的人脸识别 FaceNet—深度学习与人脸识别的二次结合 介绍本文提出一个系统名为FaceNet. 它可以直接将人脸图像映射至欧式空间, 其距离能直接反应出人脸间的相似度. 一旦生成该空间, 识别, 验证, 聚类等任务都可以用它来轻松完成. FaceNet在LFW上达到了99.63%的准确率, 在Youtube Faces DB达到95.12%.和之前的方法(先输出高维度特征向量, 然后用PCA等降维, 再用分类器分类)不同, FaceNet直接使用基于triplets的LMNN（最大边界近邻分类）的loss函数训练神经网络, 网络直接输出为128维度的向量空间.其人脸比对的结果如图所示:可以看到1.1就是阈值. 方法本文探索了2个网络.第一类为Zeiler&amp;Fergus研究中使用的神经网络, 受NIN影响, 我们在网络后面加了多个1*1*d卷积层, 第二类为Inception网络.关键在于网络的末端使用了triplet loss作为目标函数. Triplet Losstriplet loss: 在3个样本中,最小化类内差距,最大化类间差距.如图:公式如下: Triplet 3个样本的选择我们想要从类内选择出一些P样本(hard-positive), 使他们与A样本的平均距离最大(argmax). 同时从类外选择出一些N样本(hard-negative), 使他们与A样本的平均距离最小(argmin). 但计算整个训练集之间的平均最大最小距离是不现实的.这里有两个办法解决它: 每隔几步产生triplets, 计算它们在子数据集中的argmax和argmin. 在线产生triplets, 从mini-batch中得到hard positive/negative样本.(本文使用) 文章选用方法2, 为了防止选择不当, 使用如下公式来约束样本选择. 深度卷积网络第一个是Zeiler&amp;Fergus的22层网络.第二个网络 实验 NN2(input 224*224)的效果最好. 即使图像只有80*80, 验证正确率下降的也不大. NN1输出的特征在128维时效果最好. 训练的数据量增大能提高准确率.]]></content>
      <tags>
        <tag>Deep Learning</tag>
        <tag>Face Recognition</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Deeply learned face representations are sparse, selective, and robust 阅读笔记]]></title>
    <url>%2F2017%2F06%2F23%2FDeepID2-1%2F</url>
    <content type="text"><![CDATA[本文由香港中文大学于2015年发表在CVPR会议 Introduction本文设计了一个深度卷积网络, 获取到 的特征名为DeepID2+.相比与DeepID2,2+ 的网络通过增加隐层特征的维度和对靠前的卷积层监督使DeepID2+在LFW上表现很好.对于人脸识别有3个点很关键: sparse: 稀疏, 即特征向量中包含很多零, 可压缩性强. selective: 精选, 指具有判别能力的一些分量, 在不同人脸上响应程度不同. robust: 健壮, 指图像被遮挡后特征中某些向量依然保持不变的一致性. DeepID2+ 网络结构如图:DeepID2+网络也使用了识别和验证两种信号进行监督.此部分参考我上一篇笔记.与DeepID2网络的不同点: 每个卷积层后(实际上是1,2,3maxpool层和conv4层后)都获得一个128维的特征, 共有4个卷积层故特征从160维增大到512维. 训练集增大.从8000类160000图增大至12000类290000图. 用全连接层连接每个卷积层, 并使用DeepID2的那种监督方式.这样监督信号就离前期的卷积层更近了,也更有效. 实验发现DeepID2选取了25个patch，DeepID2+选取了同样的25个patch，然后抽取的特征分别训练联合贝叶斯模型，得到的结果是DeepID2+平均比DeepID2提高2%。特别的,根据稀疏性对DeepID2+进行二值化处理后, 再使用联合贝叶斯或汉明距离实验,发现识别率下降有限.二值化后的特征节省空间并且可以使人脸检索变得速度更快，更接近实用场景。 关于DeepID3DeepID3实现了两个更深的网络,一个参考VGG, 一个参考googlenet.如图所示:但结果较DeepID2+差距不大.]]></content>
      <tags>
        <tag>Deep Learning</tag>
        <tag>Face Recognition</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Deep Learning Face Representation by Joint Identification-Verification 阅读笔记]]></title>
    <url>%2F2017%2F06%2F22%2FDeepID2%2F</url>
    <content type="text"><![CDATA[本文由香港中文大学于2014年发表参考: DeepID2——强大的人脸分类算法 经典计算机视觉论文笔记汇总 DeepID人脸识别算法之三代 Introduction人脸识别的中心主题应该是减少类内变化增大类间差异.所谓识别(identification)是将一张图片从大量的类别中识别出来. 而所谓验证(verification)是比较两批图像是否为同一类.我们使用识别和验证两种信号(signal)得到的特征称为DeepID2. 使用DeepID2, 在LFW数据集上得到了99.15%的正确率 特征抽取DeepID2使用的结构:其网络结构与DeepID很相似,只不过输入图像的大小改为55*47.DeepID2特征从两个监督信号学习得到.首先是人脸识别信号, 识别通过在DeepID2层后接一个softmax层实现.loss表示为Ident loss然后是人脸验证信号, 它能有效的减小类内变化, 通常有L1, L2, Cosine相似度3种形式, 文中使用L2.表示为Verif loss, 公式如下:当图同类时($y_{ij}=1$)减小类内差距.当图异类时($y_{ij}=-1$)增大类间差距至m, m是需要手动调节的.余弦距离公式也给出, 但文章没有使用,我就不列出.DeepID2特征学习算法:可以看到最终的目标函数(loss)是由Ident loss和Verif loss加权得到的. 人脸验证使用SDM获得人脸的21个基准点, 然后使用这些基准点将人脸图像对齐.然后在不同的位置, 大小, 色彩通道和水平翻转切割每张人脸图像,每张图像得到400个不同的patch. 将其放入200个前文提到的深度卷积网络, 抽取出400个DeepID2特征向量.选择水平翻转的图像与原始图像作为一对, 放入网络得到2个DeepID2特征向量.为了减少DeepID2特征的长度, 使用前向-反向贪心算法选择出25个特征向量. 25*160=4000维依然太大, 使用PCA将人脸特征向量压缩至180维, 然后使用联合贝叶斯算法进行人脸识别. 例子: 疑问和思考 联合贝叶斯算法是什么样的? SDM算法是什么?]]></content>
      <tags>
        <tag>Deep Learning</tag>
        <tag>Face Recognition</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Deep Learning Face Representation from Predicting 10,000 Classes 阅读笔记]]></title>
    <url>%2F2017%2F06%2F21%2FDeepID%2F</url>
    <content type="text"><![CDATA[本文由香港中文大学于2014年发表在CVPR会议参考: Deep Learning Face Representation from Predicting 10,000 Classes论文笔记 【深度学习论文笔记】Deep Learning Face Representation from Predicting 10,000 Classes Deep Learning Face Representation from Predicting 10,000 Classes 经典计算机视觉论文笔记——DeepFace\DeepID\DeepID2\DeepID3\FaceNet\VGGFace汇总 Introduction and Related Work本文提供了一种用多个深度模型获得高水平人脸特征(DeepID)来对人脸分类的方法.所谓DeepID,假如网络有10000个分类的输出, 那么在最后一层的输入(倒数第二层的输出)就是DeepID的组成部分,文中有160维.如图DeepID有很好的泛化能力, 再对它用Joint Bayesian等方法就能进行人脸验证. Deep ConvNets结构如下:输入图像为39*31*k, k表示通道数(RGB图像k为3, 灰度图像k为1).激活函数使用RELU.DeepID层为全连接层,其输入为Maxp3和Conv4两者.输出特征160维. 因Conv4包含了较多的全局信息, 需要Maxp3进行细节补充.$x^1,w^1,x^2,w^2$代表Maxp3和Conv4的输出和对应的权重.该模型使用SGD的最优化方法. Feature extraction找到人脸上的5个标记点.(两个眼睛中心点, 鼻尖点, 两个嘴角点)将人脸根据两个眼睛中心点和嘴角连线的中点对齐.从60个不同的小块抽取DeepID特征, 60这一数字源于10个区域, 3个大小, RGB和灰度2种通道.如图:上方的十张图是由中等大小尺寸切的.上方左边的5张图切自弱对齐后的全局区域,上方右边的5张图则分别以5个人脸标记点为中心切.下方代表3种大小.(每种大小又分为长方形和正方形两个形状.)每张图片输出160维,一个patch有两张图片(一张是原始的,一张是水平翻转的副本),共有60个patch.则最终DeepID将所有维度连接起来,即160*2*60=19200维.这将用于最终的人脸验证.(每个网络输入一个patch,一张长方形一张正方形) Face verification分别使用JointBayesian和神经网络进行人脸识别.后续结果表面JointBayesian效果较好.神经网络部分示意图:输入有640*60 是因为人脸识别需要两个张人脸图像.每张图像切好得到一个patch会产生160*2维输出.共有两张图像, 故640 = 160*2*2注意到第一个隐层是locally-connect layer ,表示一个神经元只与对应的group连接, 这样神经元就能学习到紧凑的局部特征.第二的隐层为全连接层, 这表示从局部特征中学习全局特征.隐层都使用RELU, 最后一层(输出)使用sigmoid.并且对所有隐层节点使用dropout. 实验采用CelebFace训练模型, 在LFW上验证.随机选择80%的人训练DeepID模型, 剩下20%的人训练联合贝叶斯或神经网络.在人脸识别阶段, 将特征通过PCA降维至150维, 再输入联合贝叶斯中.在测试时, 判断两张脸是否是同一个人是通过看联合贝叶斯的极大似然比来决定的, 该阈值通过训练集确定.在LFW上的表现如下,其他结果见论文.]]></content>
      <tags>
        <tag>Deep Learning</tag>
        <tag>Face Recognition</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[A Light CNN for Deep Face Representation with Noisy Labels 阅读笔记]]></title>
    <url>%2F2017%2F06%2F20%2FA-Light-CNN-for-Deep-Face-Representation-with-Noisy-Labels%2F</url>
    <content type="text"><![CDATA[本文中科院自动所于2017年发表。也是我上一篇博客阅读内容的升级版。论文链接：PDF参考阅读： 人脸识别之light_cnn 人脸识别方向论文笔记（1）– A Light CNN for Deep Face Representation With Noisy Labels 第一作者提供项目地址: https://github.com/AlfredXiangWu/face_verification_experiment Introduction and Related Work本文有以下贡献： 介绍了激活函数MFM。相比RELU，它的临界值由训练数据得到，并采用竞争关系其在不同的数据集上拥有更好泛化和适应能力。 受AlexNet, VGG, ResNet启发设计了3个基于MFM的light CNN。 提出了一种通过预训练深度网络的语义引导方法来处理大规模数据集中的噪声标记图像。标签不一致可以通过预测的概率有效检测，然后被重新标记或删除用于训练。 提出的有256维特征的单一模型在各种不同人脸识别基准下表现不错。模型有较少参数故能在CPU或嵌入式系统上较快地抽取特征。 ArchitectureMax-Feature-Map Operation我上一篇博文已经介绍了这个概念.这里只是增加了MFM 3/2的用法.附上示意图与公式. The Light CNN Framework第一个结构:(缘于AlexNet)第二个结构:可以看到此结构相比于旧版论文, 在Conv2a位置更细致并在Conv4-5之间去掉了一个pool层.第三个结构:注意: 取消了批量归一化操作(batch normalization). 采用全连接层而不是global average pooling layer.因为训练图像都是对齐的.global average pooling layer可能损坏图像的一部分语义和空间特征. Semantic Bootstrapping for Noisy Label如下步骤: 用原始的含噪声标签数据集训练LightCNN 用训练好的模型预测训练集的标签.设置一个是否接受预测的阈值. 用重新标签过的数据集再次训练LightCNN Experiments在LFW和YFW上的表现:在其他数据集的表现见论文. 疑问和思考 LightCNN-29的模型尚未理解]]></content>
      <tags>
        <tag>Deep Learning</tag>
        <tag>Face Recognition</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[A lightened cnn for deep face representation 阅读笔记]]></title>
    <url>%2F2017%2F06%2F20%2FA-lightened-cnn-for-deep-face-representation%2F</url>
    <content type="text"><![CDATA[本文由北京科技大学与中科院于2015年发表论文链接：PDF参考阅读： A Lightened CNN for Deep Face Representation A Lightened CNN for Deep Face Representation论文笔记 人脸验证：Lightened CNN 第一作者提供github代码地址：https://github.com/AlfredXiangWu/face_verification_experiment Introduction and Related Work概述并归类人脸识别领域相关的方法，其related work简述了最新的成果（有内容重复是什么鬼），其脚注对学习领域内经典的几篇论文有很好的借鉴意义。把使用CNN对人脸验证的方法大致分成了三类： 使用CNN提取人脸特征向量，再使用分类器进行判断是否为同一人。 直接优化验证损失（verification loss），这个方法对训练集中不存在的新类别效果不佳。 结合识别和验证 尽管之前的方法都在LFW上达到了很高的准确率，但是模型计算量的需求很高，较难在移动设备中推广。传统的深度网络模型由于很深，需要花费很多时间提取特征。并且大多数模型都采用RELU激活函数，这导致得到的特征通常维度高且稀疏。为了获得低维度且紧凑的特征，通常采用Joint Bayesian或metric learning等方法。因此直接寻求一个轻量的高速抽取特征且维度表现低的CNN相当重要。所以论文主要说明了3件事: 一种名为MFM的激活函数，用于卷积层。 两个轻量的CNN. 提出的CNN模型在LFW和YFW上获得了不错表现。值得注意的是，模型大小仅为20-30MB，特征提取仅耗时67ms。该模型有希望应用到实时监测领域。 ArchitectureMFM激活函数根据上面的MFM示意图，可以看到灰度图像（单通道）输入到两个相同结构的卷积层中，MFM比较两个卷积层中对应的通道，取最大值。MFM的公式：卷积层有相同的2个，每个有n个通道，每个通道的大小为h*w。MFM就是比较两个层里相应通道的结果，故MFM输出的大小为h*w*n。其导数公式：可以看到有一半激活层梯度为零。相比RELU，MFM能得到更紧凑的特征。此外，MFM激活函数也可以被看作是两个卷积层之间的稀疏连接，它将信息稀疏地编码至特征空间。 lightened CNN结构 NIN其实就是使用了MLP卷积层。为了弄清conv2_a，找到源码中proto的代码表达。12345678910111213141516171819202122layers&#123; name: &quot;conv2a&quot; type: CONVOLUTION blobs_lr: 1 blobs_lr: 2 weight_decay: 1 weight_decay: 0 convolution_param &#123; num_output: 96 kernel_size: 1 stride: 1 weight_filler &#123; type: &quot;xavier&quot; &#125; bias_filler &#123; type: &quot;constant&quot; value: 0.1 &#125; &#125; bottom: &quot;pool1&quot; top: &quot;conv2a&quot;&#125; 96个大小为1的卷积核，即一个全连接层。 疑问和思考 神经网络为什么要有激活函数，为什么relu 能够防止梯度消失？ NIN层是如何实现的？ 深度学习方法（十）：卷积神经网络结构变化——Maxout Networks，Network In Network，Global Average Pooling Detection之NIN]]></content>
      <tags>
        <tag>Deep Learning</tag>
        <tag>Face Recognition</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[DeepFace: Closing the Gap to Human-Level Performance in Face Verification 阅读笔记]]></title>
    <url>%2F2017%2F06%2F19%2FDeepFace-Closing-the-Gap-to-Human-Level-Performance-in-Face-Verification%2F</url>
    <content type="text"><![CDATA[本文由Facebook AI研究中心于2014年发表.论文链接:PDF参考: FaceBook 论文：DeepFace: Closing the Gap to Human-Level Performance in Face Verification 笔记 【翻译+原创】DeepFace: Closing the Gap to Human-Level Performance in Face Verification 论文笔记 论文笔记–DeepLearning face Recognition Introduction该文主要做了3件事: 构建一个有效的, 利用含大量标签过的人脸数据集训练的深度神经网络(DNN)获取人脸特征. 并且该网络在其他数据集上的表现也不错. 一个基于3D建模的人脸对齐系统. 在LFW上使机器达到近乎人类的识别能力, 在YFW上降低错误率50%以上. Face Alignment 人脸对齐 分为以下几步: 检测人脸, 找到6个初始基准点. 根据这6个点进行剪切. 在切割后的图像上找到67个基准点, 进行三角形分割并在轮廓上添加三角形防止图像不连续. 将三角化的人脸转化为3D形状. 将3D形状三角化,较黑的三角代表较不可见. 将67个基准点的模型调整至正面向前 投影至2D图像,最终对齐人脸. 一种新的3D模型生成方式. (论文中未使用) 关于3D对齐的具体方法我不做深究, 留个坑吧. Representation 特征提取网络结构:论文里说152*152的图像使用32个大小为11*11*3的卷积核用32*11*11*3@152*152表示.(但上图并不是这样表示的)我补充一下网络结构.C1: filter_size=11*11; filter_num=32;stride=1;pad=0M2: filter_size=3*3;stride=2C3: filter_size=9*9; filter_num=16;stride=1;pad=0Local-conv层含义参考:Understanding Locally Connected Layers In Convolutional Neural Networks其实就是卷积核独立不共享,这样做的好处是能反应脸部不同位置的特征.(但也导致参数超多)L4: filter_size=9*9; filter_num=16*55*55;stride=1;pad=0L5: filter_size=7*7; filter_num=16*25*25;stride=2;pad=0L6: filter_size=5*5; filter_num=16*21*21;stride=1;pad=0F7,F8: 全连接层,这两层能对捕捉到图像中较远距离位置的相关性.softmax: 得到k个类的分数p_kloss使用交叉熵函数,即L=-log(p_k)使用SGD最优化方法, RELU激活函数, 在第一个全连接层使用drop-out, 在最后一层对所有特征进行了L2归一化以减弱图像对光照的敏感性. Verification Metric 验证加权的$χ^2$距离 归一化后的特征向量,所有值在[0,1]之间且非常稀疏.距离公式见上图. Siamese network参考:Caffe中的Siamese网络Siamese原意是”泰国的，泰国人”，而与之相关的一个比较常见的词是”Siamese twin”， 意思是”连体双胞胎”，所以Siamemse Network是从这个意思转变而来，指的是结构非常相似的两路网络，分别训练，但共享各个层的参数，在最后有一个连接的部分。Siamese网络对于相似性比较的场景比较有效。此外Siamese因为共享参数，所以能减少训练过程中的参数个数。 Experiments在SFC上训练,在LFW和YFW上验证.训练和验证的过程不详细看了,我暂时没有复现的欲望. 疑问和思考 siamese network是如何实现的?]]></content>
      <tags>
        <tag>Deep Learning</tag>
        <tag>Face Recognition</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Deep Face Recognition 阅读笔记]]></title>
    <url>%2F2017%2F06%2F18%2FDeep-Face-recognition%2F</url>
    <content type="text"><![CDATA[此文由牛津大学视觉研究所于2015年发表.论文链接: PDF参考: Deep Face Recognition论文阅读 VGG-Face：Deep Face Recognition 笔记 triplet loss 原理以及梯度推导 论文：Deep Face Recognition 概括 本文有两个主要部分: 如何用较少的人力, 搜集和建立一个相对优质的数据集. 用深度网络完成人脸识别和验证, 并比较不同的网络结构在不同的数据集上的表现. Dataset Collection 收集整理数据集 过滤出候选的人名列表从IMDB资料库中获得5k个人名, 其中男女各一半.使用google图片搜索每个人名搜集200张图片. 筛去图片纯净度低的, 与LFW汗YFW重合人名的, 还剩下2622个人名. 为每个人名收集更多图片在google和bing里搜图并在每个名字之后加上”actor”再搜一遍,每次搜500张,总计单个名字下搜集2000张图片. 使用自动的过滤器提高纯净度基于Google的查询结果, 取前50张图片作为正训练样本. 其他人名查询下的前50张为负训练样本, 使用Fisher Vector Faces训练一个one-vs-rest的线性SVM分类器对2000张图片排序, 保留前1000张. 删除相似的图片抽出不同搜索引擎找到的相同图片和在网路上不同位置的相同图片, 还有那些只是色彩平衡不同的近似图片. 通过计算每张图片的VLAD descriptor, 对每个人名下的1000张图片聚类, 使用一个相当紧缩的阈值范围保留一部分图片.(图表展示为平均623张,不是每个人名下623张.) 最终人工筛选这一步的目的是通过人为注释提高数据集纯度. 但是为了减轻人员标注的负担, 使用AlexNet对所有人名下的图片进行rank. rank完的图片以200张一批的方式展示给注释人员. 如果一批内图像的纯度大约高于95%, 那么就是good. 最终得到982803张图片. 搜集过程和用时表格(A代表自动, M代表手动):与公开的数据集进行对比: Network architecture and training 网络结构与训练训练采用VGGNet,结构如图:输入的图像大小为224*224,卷积层的配置为{stride=2;pad=1;filter_size=3*3;filter_num=64}则卷积后输出大小1+(224+2*1-3)/1 = 224,即卷积后仍为224*224的图像,只不过包含64个通道(filter_num).池化层的配置为{stride=2;pad=0;filter_size=2*2}则池化后输出大小(224-2)/2+1 = 112,即池化后为112*112的图像(大小减半,通道数不变)经过一系列的conv-maxpool,最后包含3个FC层(理解为filter和input一样大的卷积层).第一个FC层的输入为7*7, 512通道.filter也为7*7, 512通道.filter_num为4096.故输出大小(1, 4096).网络的最后一层使用softmax log-loss作为分类器.训练结束后, 移除最后一层, 模型最终得到的score vectors可通过计算欧式距离再比较的方法进行人脸对比检测. 然而, 通过在欧式空间里使用”triplet loss”的方法可以使上面的分数向量进一步改善.公式如下:a:anchor; p:positive; n:negative, 参考triplet loss 原理以及梯度推导 Architecture考虑使用A,B,D三个网络结构, 其中A就是上表展示的VGG网络.最后3个是全连接层, 只是将他看作卷积层(卷积核与输入图像一样大).我猜测BD的结构为VGGNet论文中描述的,如图:B,D的网络结构相比A分别多了2和5个卷积层.输入人脸图像的大小是224*224. Training描述了训练的详细过程, 如使用SGD+momentum的最优化方法, 学习率的设置, 通过翻转图像增大数据集, BD模型均在A模型训练好的基础上进行微调等等. Experiments and results各个网络结构的表现如下:(C-数据集搜集和整理中第五步的数据集; F-第三步)可以发现 F数据集表现的更好; 测试时2D对齐能轻微提高表现,但训练时没有帮助; 网络B的表现最好; 验证时的嵌入学习(triplet-loss)能显著提升表现. 与最先进的网络对比:可以看到此文使用比其他网络少许多的数据集和较少的网络得到与其他先进网络媲美的结果. 疑问和思考 triplet loss具体是在哪一步如何实现的? 为什么净化后的数据集训练效果反而降低了?文章给出了两点可能的解释:1.就算标签存在噪声,数据量越大越好.2.一些微妙的正确图样在步骤5被移除了.]]></content>
      <tags>
        <tag>Deep Learning</tag>
        <tag>Face Recognition</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CS231n学习笔记(LSTM)]]></title>
    <url>%2F2017%2F06%2F16%2FCS231n%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-LSTM%2F</url>
    <content type="text"><![CDATA[作业: [LSTM_Captioning.ipynb]参考:Understanding LSTM Networks和CS231n_assignment3在使用RNN时,时间(t)过大会导致前后的信息相关程度越来越小,最后出现梯度消失现象.而LSTM(Long Short-Term Memory Networks)就是为了解决这个长期依赖问题而被提出的.LSTM的一般结构见下图. 和RNN一样，LSTM也是随着时间序列重复着一样的模块，只是LSTM的每个某块比RNN更加复杂，拥有四个层（3个门+1个记忆单元）。下图方框内上方的那条水平线，被称为胞元状态（cell state），LSTM通过门结构对记忆单元上的信息进行线性修改，保证了当时间序列变得很长的时候，前后信息的关联度不会衰减。 forget gate:根据上一个时刻输出的h_t-1和当前输入x_t,通过sigmoid控制得到f_t Input gate:决定哪些值进行更新进cell state.对cell state进行更新 Output gate: LSTM还有很多变体见参考CS231n_assignment3 LSTM: step forward12345678910H = prev_h.shape[1]a = x.dot(Wx) + prev_h.dot(Wh) + bz_i = sigmoid(a[:,:H]) #input gate itz_f = sigmoid(a[:,H:2*H]) #forget gate ftz_o = sigmoid(a[:,2*H:3*H])#output gate otz_g = np.tanh(a[:,3*H:]) #input gate Ctnext_c = z_f * prev_c + z_i * z_gz_t = np.tanh(next_c)next_h = z_o * z_tcache = (z_i, z_f, z_o, z_g, z_t, prev_c, prev_h, Wx, Wh, x) LSTM: step backward12345678910111213141516171819202122H = dnext_h.shape[1]z_i, z_f, z_o, z_g, z_t, prev_c, prev_h, Wx, Wh, x = cachedz_o = z_t * dnext_hdc_t = z_o * (1 - z_t * z_t) * dnext_h + dnext_cdz_f = prev_c * dc_tdz_i = z_g * dc_tdprev_c = z_f * dc_tdz_g = z_i * dc_t da_i = (1 - z_i) * z_i * dz_ida_f = (1 - z_f) * z_f * dz_fda_o = (1 - z_o) * z_o * dz_oda_g = (1 - z_g * z_g) * dz_gda = np.hstack((da_i, da_f, da_o, da_g))dWx = x.T.dot(da)dWh = prev_h.T.dot(da) db = np.sum(da, axis = 0)dx = da.dot(Wx.T)dprev_h = da.dot(Wh.T) LSTM: forward123456789101112N, T, D = x.shapeH = b.shape[0]/4h = np.zeros((N, T, H))cache = &#123;&#125;prev_h = h0prev_c = np.zeros((N, H))for t in range(T): xt = x[:, t, :] next_h, next_c, cache[t] = lstm_step_forward(xt, prev_h, prev_c, Wx, Wh, b) prev_h = next_h prev_c = next_c h[:, t, :] = prev_h LSTM: backward123456789101112131415161718192021N, T, H = dh.shapez_i, z_f, z_o, z_g, z_t, prev_c, prev_h, Wx, Wh, x = cache[T-1]D = x.shape[1] dprev_h = np.zeros((N, H))dprev_c = np.zeros((N, H))dx = np.zeros((N, T, D))dh0 = np.zeros((N, H))dWx= np.zeros((D, 4*H))dWh = np.zeros((H, 4*H))db = np.zeros((4*H,)) for t in range(T): t = T-1-t step_cache = cache[t] dnext_h = dh[:,t,:] + dprev_h dnext_c = dprev_c dx[:,t,:], dprev_h, dprev_c, dWxt, dWht, dbt = lstm_step_backward(dnext_h, dnext_c, step_cache) dWx, dWh, db = dWx+dWxt, dWh+dWht, db+dbt dh0 = dprev_h]]></content>
      <tags>
        <tag>Deep Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CS231n学习笔记(RNN)]]></title>
    <url>%2F2017%2F06%2F14%2FCS231n%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-RNN%2F</url>
    <content type="text"><![CDATA[作业: [RNN_Captioning.ipynb]数据集准备:打开datesets内的shell文件,找到url拖至迅雷里下载至datasets并解压.运行至第三个代码块时可能运行至os.remove(fname)报错.是因为文件缓存被python打开无法删除导致的,注释掉这句即可.np.linspace在指定的间隔内返回均匀间隔的数字. Vanilla RNN: step forward打开cs231n/rnn_layers.py找到rnn_step_forward(x, prev_h, Wx, Wh, b)123a = prev_h.dot(Wh) + x.dot(Wx) + bnext_h = np.tanh(a)cache = (x, prev_h, Wh, Wx, b, next_h) 即next_h = tanh(x*Wx+pre_h*Wh+b) Vanilla RNN: step backward找到rnn_step_backward(dnext_h, cache)1234567x, prev_h, Wh, Wx, b, next_h = cacheda = dnext_h * (1 - next_h ** 2) #(N,H)dx = np.dot(da, Wx.T) #(N,D)dprev_h = np.dot(da, Wh.T) #(N,H)dWx = np.dot(x.T, da) #(D,H)dWh = np.dot(prev_h.T, da) #(H,H)db = np.sum(da, axis = 0) #(H,) 若激活函数为sigmoid,那么$f’(z) = f(z)(1-f)$若激活函数为tanh,那么$f’(z) = 1-(f(z))^2$ Vanilla RNN: forward12345678910N, T, D = x.shape(H,) = b.shapeh = np.zeros((N, T,H))prev_h = h0for t in range(T): xt = x[:, t, :] next_h,_ = rnn_step_forward(xt, prev_h, Wx, Wh, b) prev_h = next_h h[:, t, :] = prev_hcache = (x, h0, Wh, Wx, b, h) Vanilla RNN: backward1234567891011121314151617181920212223x, h0, Wh, Wx, b, h = cacheN, T, H = dh.shapeD = x.shape[2]next_h = h[:, T-1, :]dprev_h = np.zeros((N, H))dx = np.zeros((N, T, D))dh0 = np.zeros((N, H))dWx = np.zeros((D, H))dWh = np.zeros((H, H))db = np.zeros((H,))for t in range(T): t = T-1-t xt = x[:, t, :] #(N,D) if t==0: prev_h = h0 else: prev_h = h[:,t-1,:] step_cache = (xt, prev_h, Wh, Wx, b, next_h) next_h = prev_h dnext_h = dh[:, t, :] + dprev_h dx[:, t, :], dprev_h, dWxt, dWht, dbt = rnn_step_backward(dnext_h, step_cache) dWx, dWh, db = dWx+dWxt, dWh+dWht, db+dbtdh0 = dprev_h Word embedding: forward1234567N, T = x.shapeV, D = W.shapeout = np.zeros((N, T, D))for i in range(N): for j in range(T): out[i, j] = W[x[i,j]]cache = (x, W.shape) 词嵌入(word embedding)参考文章:Deep Learning in NLP （一）词向量和语言模型这里W代表词典表有V个词, 每个词都用D维数组表示. 输入的x共有N句, 每句有T个词.上面的代码就是将x里的词提出来,映射到W上. Word embedding: backward123x, W_shape = cachedW = np.zeros(W_shape)np.add.at(dW, x, dout) np.add.at表示dW在x的位置上,分别加上加上dout.例子如下:12345678910&gt;&gt;&gt; a = np.array([1, 2, 3, 4])&gt;&gt;&gt; np.add.at(a, [0, 1, 2, 2], 1)&gt;&gt;&gt; print(a)array([2, 3, 5, 4])&gt;&gt;&gt; a = np.array([1, 2, 3, 4])&gt;&gt;&gt; b = np.array([1, 2])&gt;&gt;&gt; np.add.at(a, [0, 1], b)&gt;&gt;&gt; print(a)array([2, 4, 3, 4]) CaptioningRNN.loss123456789101112131415161718192021222324252627affine_out, affine_cache = affine_forward(features ,W_proj, b_proj)#(2)word_embedding_out, word_embedding_cache = word_embedding_forward(captions_in, W_embed)#(3)if self.cell_type == &apos;rnn&apos;: rnn_or_lstm_out, rnn_cache = rnn_forward(word_embedding_out, affine_out, Wx, Wh, b)elif self.cell_type == &apos;lstm&apos;: rnn_or_lstm_out, lstm_cache = lstm_forward(word_embedding_out, affine_out, Wx, Wh, b)else: raise ValueError(&apos;Invalid cell_type &quot;%s&quot;&apos; % self.cell_type)#(4)temporal_affine_out, temporal_affine_cache = temporal_affine_forward(rnn_or_lstm_out, W_vocab, b_vocab)#(5)loss, dtemporal_affine_out = temporal_softmax_loss(temporal_affine_out, captions_out, mask)#(4)drnn_or_lstm_out, grads[&apos;W_vocab&apos;], grads[&apos;b_vocab&apos;] = temporal_affine_backward(dtemporal_affine_out, temporal_affine_cache)#(3)if self.cell_type == &apos;rnn&apos;: dword_embedding_out, daffine_out, grads[&apos;Wx&apos;], grads[&apos;Wh&apos;], grads[&apos;b&apos;] = rnn_backward(drnn_or_lstm_out, rnn_cache)elif self.cell_type == &apos;lstm&apos;: dword_embedding_out, daffine_out, grads[&apos;Wx&apos;], grads[&apos;Wh&apos;], grads[&apos;b&apos;] = lstm_backward(drnn_or_lstm_out, lstm_cache)else: raise ValueError(&apos;Invalid cell_type &quot;%s&quot;&apos; % self.cell_type)#(2)grads[&apos;W_embed&apos;] = word_embedding_backward(dword_embedding_out, word_embedding_cache)#(1)dfeatures, grads[&apos;W_proj&apos;], grads[&apos;b_proj&apos;] = affine_backward(daffine_out, affine_cache) CaptioningRNN.sample1234567891011121314151617181920N, D = features.shapeaffine_out, affine_cache = affine_forward(features ,W_proj, b_proj)prev_word_idx = [self._start]*Nprev_h = affine_outprev_c = np.zeros(prev_h.shape)captions[:,0] = self._startfor i in range(1,max_length): prev_word_embed = W_embed[prev_word_idx] if self.cell_type == 'rnn': next_h, rnn_step_cache = rnn_step_forward(prev_word_embed, prev_h, Wx, Wh, b) elif self.cell_type == 'lstm': next_h, next_c,lstm_step_cache = lstm_step_forward(prev_word_embed, prev_h, prev_c, Wx, Wh, b) prev_c = next_c else: raise ValueError('Invalid cell_type "%s"' % self.cell_type) vocab_affine_out, vocab_affine_out_cache = affine_forward(next_h, W_vocab, b_vocab) captions[:,i] = list(np.argmax(vocab_affine_out, axis = 1)) prev_word_idx = captions[:,i] prev_h = next_h]]></content>
      <tags>
        <tag>Deep Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PHPstorm10+wamp3.0环境搭建]]></title>
    <url>%2F2017%2F06%2F13%2FPHPstorm10wamp3%2F</url>
    <content type="text"><![CDATA[phpstorm10的破解过程和wamp的安装过程不展开讲了。下面说说如何配置phpstorm。第1步：wamp自带xDebug，找到php.ini，在末尾加入12345678910111213141516; XDEBUG Extensionzend_extension_ts = &quot;D:/wamp64/bin/php/php5.6.25/zend_ext/php_xdebug-2.4.1-5.6-vc11-x86_64.dll&quot;;[xdebug]xdebug.remote_enable = offxdebug.profiler_enable = offxdebug.profiler_enable_trigger = offxdebug.profiler_output_name = cachegrind.out.%t.%pxdebug.profiler_output_dir = &quot;d:/wamp64/tmp&quot;xdebug.show_local_vars=0xdebug.idekey=PhpStormxdebug.remote_enable = Onxdebug.remote_host=localhostxdebug.remote_port=9000xdebug.remote_handler=dbgp 注意：其中.dll和tmp的路径根据实际情况修改。 第2步： File-&gt;Settings-&gt;Languages&amp;Frame Works-&gt;Php-&gt;Interpreter 选择web服务器套件中php.exe的路径第3步： File-&gt;Settings-&gt;Languages&amp;Frame Works-&gt;Php-&gt;Servers 配置服务器相关设置:第4步： File-&gt;Settings-&gt;Languages&amp;Frame Works-&gt;Php-&gt;Debug-&gt;DBGp Proxy 配置相关设置：第5步： File-&gt;Settings-&gt;Languages&amp;Frame Works-&gt;Php-Debug 找到右边窗口对应的debug设置，把端口改成9000第6步：在主界面右上角运行调试配置中创建phpwebapplication（我的php文件在www文件下的test下 所以设置为test）第7步：安装chrome 扩展程序 —— xdebug helper 。点击phpstorm右上角的监听按钮。即可进行调试]]></content>
      <tags>
        <tag>PHP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CS231n学习笔记(TensorFlow)]]></title>
    <url>%2F2017%2F06%2F11%2FCS231n%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-TensorFlow%2F</url>
    <content type="text"><![CDATA[完成作业: [TensorFlow.ipynb]跳过数据导入部分,直接进入model部分.tensorflow的api有很多,建议边写边查.api文档 simple_model12345678910111213141516171819202122232425262728293031323334# clear old variablestf.reset_default_graph()# setup input (e.g. the data that changes every batch)# The first dim is None, and gets sets automatically based on batch size fed inX = tf.placeholder(tf.float32, [None, 32, 32, 3])y = tf.placeholder(tf.int64, [None])is_training = tf.placeholder(tf.bool)def simple_model(X,y): # define our weights (e.g. init_two_layer_convnet) # setup variables Wconv1 = tf.get_variable("Wconv1", shape=[7, 7, 3, 32]) bconv1 = tf.get_variable("bconv1", shape=[32]) W1 = tf.get_variable("W1", shape=[5408, 10]) b1 = tf.get_variable("b1", shape=[10]) # define our graph (e.g. two_layer_convnet) a1 = tf.nn.conv2d(X, Wconv1, strides=[1,2,2,1], padding='VALID') + bconv1 h1 = tf.nn.relu(a1) h1_flat = tf.reshape(h1,[-1,5408]) y_out = tf.matmul(h1_flat,W1) + b1 return y_outy_out = simple_model(X,y)# define our losstotal_loss = tf.losses.hinge_loss(tf.one_hot(y,10),logits=y_out)mean_loss = tf.reduce_mean(total_loss)# define our optimizeroptimizer = tf.train.AdamOptimizer(5e-4) # select optimizer and set learning ratetrain_step = optimizer.minimize(mean_loss) Wconv1 = tf.get_variable(&quot;Wconv1&quot;, shape=[7, 7, 3, 32])表示卷积核的大小为7*7,每个卷积核包含3个通道,共有32个卷积核. bconv1 = tf.get_variable(&quot;bconv1&quot;, shape=[32])32个卷积核,每个都包含1个偏置项. W1 = tf.get_variable(&quot;W1&quot;, shape=[5408, 10])已知stride,图像与卷积核的宽度高度,根据公式’H_n = 1 + (H + 2 * pad - HH)/stride’计算卷积后输出矩阵的大小,1+(32-7)/2 = 13, 故每个卷积核会输出一个13*13的矩阵(不再以通道区分),共有32个卷积核,故输出的参数总数为13*13*32 = 5408. 而CASIA-10是最终分10类的,所以取10. a1 = tf.nn.conv2d(X, Wconv1, strides=[1,2,2,1], padding=&#39;VALID&#39;) + bconv1 What does tf.nn.conv2d do in tensorflow? tensorflow conv2d的padding解释以及参数解释strides[0]和strides[3]必须为1.因为tensor的shape是[batch, height, width, channels]我们的stride只在中间两个中进行, 所以一般strides = [1, X, X, 1] total_loss = tf.losses.hinge_loss(tf.one_hot(y,10),logits=y_out)tf.one_hot()使用,即创建一个全零矩阵,将对应位置置1. Training the model on one epoch1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071def run_model(session, predict, loss_val, Xd, yd, epochs=1, batch_size=64, print_every=100, training=None, plot_losses=False): # have tensorflow compute accuracy correct_prediction = tf.equal(tf.argmax(predict,1), y) accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32)) # shuffle indicies train_indicies = np.arange(Xd.shape[0]) np.random.shuffle(train_indicies) training_now = training is not None #True # setting up variables we want to compute (and optimizing) # if we have a training function, add that to things we compute variables = [mean_loss,correct_prediction,accuracy] if training_now: variables[-1] = training # counter iter_cnt = 0 for e in range(epochs): # keep track of losses and accuracy correct = 0 losses = [] # make sure we iterate over the dataset once for i in range(int(math.ceil(Xd.shape[0]/batch_size))):#也许最后还有一些训练集没有被纳入,也就不管它们了,宁缺毋滥 # generate indicies for the batch start_idx = (i*batch_size)%Xd.shape[0] #batch索引起始位置 idx = train_indicies[start_idx:start_idx+batch_size] #一个batch所有索引(已打乱) # create a feed dictionary for this batch feed_dict = &#123;X: Xd[idx,:], y: yd[idx], is_training: training_now &#125; # get batch size actual_batch_size = yd[idx].shape[0] # have tensorflow compute loss and correct predictions # and (if given) perform a training step loss, corr, _ = session.run(variables,feed_dict=feed_dict) # aggregate performance stats losses.append(loss*actual_batch_size) correct += np.sum(corr) # print every now and then if training_now and (iter_cnt % print_every) == 0: print("Iteration &#123;0&#125;: with minibatch training loss = &#123;1:.3g&#125; and accuracy of &#123;2:.2g&#125;"\ .format(iter_cnt,loss,np.sum(corr)/actual_batch_size)) #.ng表示n位有效数字 iter_cnt += 1 total_correct = correct/Xd.shape[0] total_loss = np.sum(losses)/Xd.shape[0] print("Epoch &#123;2&#125;, Overall loss = &#123;0:.3g&#125; and accuracy of &#123;1:.3g&#125;"\ .format(total_loss,total_correct,e+1)) if plot_losses: plt.plot(losses) plt.grid(True) plt.title('Epoch &#123;&#125; Loss'.format(e+1)) plt.xlabel('minibatch number') plt.ylabel('minibatch loss') plt.show() return total_loss,total_correctwith tf.Session() as sess: with tf.device("/cpu:0"): #"/cpu:0" or "/gpu:0" sess.run(tf.global_variables_initializer()) print('Training') run_model(sess,y_out,mean_loss,X_train,y_train,1,64,100,train_step,True) print('Validation') run_model(sess,y_out,mean_loss,X_val,y_val,1,64) correct_prediction = tf.equal(tf.argmax(predict,1), y)tf.argmax是一个非常有用的函数，它能给出某个tensor对象在某一维上的其数据最大值所在的索引值。由于标签向量是由0,1组成，因此最大值1所在的索引位置就是类别标签，比如tf.argmax(predict,1)返回的是模型对于任一输入x预测到的标签值.tf.equal返回一个布尔数组,如[True, False, True, True] accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))可以使用tf.cast()函数将其转换为布尔值转换为[1, 0, 1, 1]，以方便准确率的计算.tf.reduce_mean可跨越维度的计算张量各元素的平均值. np.random.shuffle(train_indicies)以随机次序重组序列 Training a specific model1234567891011121314151617181920212223def complex_model(X,y,is_training): Wconv1 = tf.get_variable(&quot;Wconv1&quot;, shape=[7, 7, 3, 32]) bconv1 = tf.get_variable(&quot;bconv1&quot;, shape=[32]) W1 = tf.get_variable(&quot;W1&quot;, shape=[5408, 1024]) b1 = tf.get_variable(&quot;b1&quot;, shape=[1024]) W2 = tf.get_variable(&quot;W2&quot;, shape=[1024, 10]) b2 = tf.get_variable(&quot;b2&quot;, shape=[10]) # define our graph (e.g. two_layer_convnet) a1 = tf.nn.conv2d(X, Wconv1, strides=[1,1,1,1], padding=&apos;VALID&apos;) + bconv1 #print (a1.shape) h1 = tf.nn.relu(a1) h2 = tf.contrib.layers.batch_norm(h1, center=True, scale=True, is_training=True, scope=&apos;bn&apos;) pool1 = tf.layers.max_pooling2d(inputs=h2, pool_size=[2, 2], strides=2) pool1_flat = tf.reshape(pool1,[-1,5408]) y_1 = tf.matmul(pool1_flat,W1) + b1 y_1_new = tf.nn.relu(y_1) y_out = tf.matmul(y_1_new,W2) + b2 return y_out 经过conv层,a1.shape = [?,26,26,32],再经过maxpooling层,pool1_flat的参数个数为(26*26*32)/(2*2) = 5408 Train a great model on CIFAR-10!部分的模型在最开始的文件里.]]></content>
      <tags>
        <tag>Deep Learning</tag>
        <tag>tensorflow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Install tensorflow on Win10]]></title>
    <url>%2F2017%2F06%2F10%2Finstall-tensorflow%2F</url>
    <content type="text"><![CDATA[install anaconda3 I use Anaconda2 and my python version is 2.7, but tensorflow needs python3.5. So the first step is to build python2.7 and 3.5 coexistence environment. If you have already installed Anaconda2, you need download Anaconda3. (choose 4.2.0) Install it and change install path to “D:\Anaconda2\envs\Anaconda3”(D:\Anaconda2 is your Anaconda2’s path) Cancel the default check when installing the software. 12345# To activate this environment, use:# &gt; activate Anaconda3## To deactivate this environment, use:# &gt; deactivate Anaconda3 you can activate Anaconda3 and then python to check your python version. tensorflowlook at install_windows activate Anaconda3 update pip (Anaconda3) D:\&gt;python -m pip install --upgrade pip if your PC don’t hava a GPU 1(Anaconda3) D:\&gt;pip install --ignore-installed --upgrade https://storage.googleapis.com/tensorflow/windows/cpu/tensorflow-1.1.0-cp35-cp35m-win_amd64.whl if you succeed, you can see 12Installing collected packages: numpy, six, werkzeug, setuptools, protobuf, wheel, tensorflowSuccessfully installed numpy-1.13.0 protobuf-3.3.0 setuptools-36.0.1 six-1.10.0 tensorflow-1.1.0 werkzeug-0.12.2 wheel-0.29.0 Invoke python from your shell as follows:$ pythonEnter the following short program inside the python interactive shell: 1234&gt;&gt;&gt; import tensorflow as tf&gt;&gt;&gt; hello = tf.constant(&apos;Hello, TensorFlow!&apos;)&gt;&gt;&gt; sess = tf.Session()&gt;&gt;&gt; print(sess.run(hello)) If the system outputs the following, then you are ready to begin writing TensorFlow programs:1Hello, TensorFlow!]]></content>
      <tags>
        <tag>Deep Learning</tag>
        <tag>tensorflow</tag>
        <tag>Tips</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CS231n学习笔记(卷积网络)]]></title>
    <url>%2F2017%2F06%2F08%2FCS231n%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E5%8D%B7%E7%A7%AF%E7%BD%91%E7%BB%9C%2F</url>
    <content type="text"><![CDATA[完成作业: [ConvolutionalNetworks.ipynb] [cnn.py] [layers.py] Convolution: Naive forward pass pad=x就是在图像边缘加x圈零,stride=2就是卷积核(fliter)在横向和纵向移动的时候都要空一格.12345678910111213stride, pad = conv_param['stride'], conv_param['pad']N, C, H, W = x.shapeF, C, HH, WW = w.shapex_padded = np.pad(x, ((0, 0), (0, 0), (pad, pad), (pad, pad)), mode='constant')H_n = 1 + (H + 2 * pad - HH) / strideW_n = 1 + (W + 2 * pad - WW) / strideout = np.zeros((N, F, H_n, W_n))for i in xrange(N): # ith image for f in xrange(F): # fth filter for j in xrange(H_n): for k in xrange(W_n): out[i, f, j, k] = np.sum(x_padded[i, :, j*stride:HH+j*stride, k*stride:WW+k*stride] * w[f]) + b[f] 图像的切割1234kitten, puppy = imread('kitten.jpg'), imread('puppy.jpg')# kitten is wide, and puppy is already squared = kitten.shape[1] - kitten.shape[0]kitten_cropped = kitten[:, d//2:-d//2, :] #kitten[Channel,Wide,height] 切割图像成为一个正方形.图像的3个通道分别是红,绿,蓝.也就是RGB 图像的放缩123img_size = 200 # Make this smaller if it runs too slowx = np.zeros((2, 3, img_size, img_size))x[0, :, :, :] = imresize(puppy, (img_size, img_size)).transpose((2, 0, 1)) imresize(puppy, (img_size, img_size))的形状是(200L, 200L, 3L)transpose((2,0,1))是以原来的shape按0,1,2这样排,然后去2,0,1重排得到结果.故x[0, :, :, :]的形状是(3L, 200L, 200L) Convolution: Naive backward pass123456789101112131415161718192021222324252627x, w, b, conv_param = cachepad = conv_param[&apos;pad&apos;]stride = conv_param[&apos;stride&apos;]F, C, HH, WW = w.shapeN, C, H, W = x.shapeH_new = 1 + (H + 2 * pad - HH) / strideW_new = 1 + (W + 2 * pad - WW) / stridedx = np.zeros_like(x)dw = np.zeros_like(w)db = np.zeros_like(b)s = stridex_padded = np.pad(x, ((0, 0), (0, 0), (pad, pad), (pad, pad)), &apos;constant&apos;)dx_padded = np.pad(dx, ((0, 0), (0, 0), (pad, pad), (pad, pad)), &apos;constant&apos;)for i in xrange(N): # ith image for f in xrange(F): # fth filter for j in xrange(H_new): for k in xrange(W_new): window = x_padded[i, :, j*s:HH+j*s, k*s:WW+k*s] db[f] += dout[i, f, j, k] dw[f] += window * dout[i, f, j, k] dx_padded[i, :, j*s:HH+j*s, k*s:WW+k*s] += w[f] * dout[i, f, j, k]# Unpaddx = dx_padded[:, :, pad:pad+H, pad:pad+W] Max pooling: Naive forward 123456789101112HH, WW = pool_param[&apos;pool_height&apos;], pool_param[&apos;pool_width&apos;]s = pool_param[&apos;stride&apos;]N, C, H, W = x.shapeH_new = 1 + (H - HH) / sW_new = 1 + (W - WW) / sout = np.zeros((N, C, H_new, W_new))for i in xrange(N): for j in xrange(C): for k in xrange(H_new): for l in xrange(W_new): window = x[i, j, k*s:HH+k*s, l*s:WW+l*s] out[i, j, k, l] = np.max(window) Max pooling: Naive backward1234567891011121314x, pool_param = cacheHH, WW = pool_param[&apos;pool_height&apos;], pool_param[&apos;pool_width&apos;]s = pool_param[&apos;stride&apos;]N, C, H, W = x.shapeH_new = 1 + (H - HH) / sW_new = 1 + (W - WW) / sdx = np.zeros_like(x)for i in xrange(N): for j in xrange(C): for k in xrange(H_new): for l in xrange(W_new): window = x[i, j, k*s:HH+k*s, l*s:WW+l*s] m = np.max(window) dx[i, j, k*s:HH+k*s, l*s:WW+l*s] = (window == m) * dout[i, j, k, l] 在编译python setup.py build_ext --inplace时遇到没有C语言编译环境的问题解决:windows下安装python的C扩展编译环境(解决“Unable to find vcvarsall.bat”) Spatial batch normalization: forward1234N, C, H, W = x.shapex_new = x.transpose(0, 2, 3, 1).reshape(N*H*W, C)out, cache = batchnorm_forward(x_new, gamma, beta, bn_param)out = out.reshape(N, H, W, C).transpose(0, 3, 1, 2) Spatial batch normalization: backward1234N, C, H, W = dout.shapedout_new = dout.transpose(0, 2, 3, 1).reshape(N*H*W, C)dx, dgamma, dbeta = batchnorm_backward(dout_new, cache)dx = dx.reshape(N, H, W, C).transpose(0, 3, 1, 2)]]></content>
      <tags>
        <tag>Deep Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CS231n学习笔记(批量归一化和dropout)]]></title>
    <url>%2F2017%2F06%2F06%2FCS231n%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E6%89%B9%E9%87%8F%E5%BD%92%E4%B8%80%E5%8C%96%EF%BC%89%2F</url>
    <content type="text"><![CDATA[引言大意： 使深层网络更容易训练的一种方法是使用更复杂的优化程序，如SGD +momentum，RMSProp或Adam。 另一个策略是改变网络架构，使其更容易训练。这些方面的一个想法是最近提出的批量归一化(Batch normalization)这个想法比较简单。 当机器学习方法的输入数据由零均值和单位方差的不相关特征组成时，往往会更好地工作。 在训练神经网络时，我们可以在将数据馈送到网络之前对数据进行预处理，以明确地解耦其特征; 这将确保网络的第一层看到遵循良好分发的数据。 然而，即使我们对输入数据进行预处理，网络较深层的激活可能不会再去相关，并且不再具有零均值或单位方差，因为它们是从网络中较早的层输出的。 更糟糕的是，在训练过程中，网络每层的特征分布将随着每个层的权重更新而移动。[3]的作者假设深层神经网络中特征的偏移分布可能使训练深层网络更加困难。 为了克服这个问题，[3]提出将批量归一化层插入到网络中。 在训练时间内，批量归一化层使用数据的小批量来估计每个特征的平均值和标准偏差。 然后使用这些估计的平均值和标准偏差来中心和标准化小型化的特征。 在训练期间，这些平均值和标准偏差的平均值保持不变，在测试时，这些运行平均值用于对特征进行中心和归一化。这种归一化策略有可能降低网络的代表性能力，因为某些层有时可能具有非零均值或单位方差特征的优化。 为此，批量归一化层包括每个特征维度的可学习的移位和缩放参数。[3] Sergey Ioffe and Christian Szegedy, “Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift”, ICML 2015. Batch normalization: Forward先上公式 再上关键代码块1234567sample_mean = np.mean(x, axis=0, keepdims=True) # [1,D] sample_var = np.var(x, axis=0, keepdims=True) # [1,D] x_normalized = (x - sample_mean) / np.sqrt(sample_var + eps) # [N,D] out = gamma * x_normalized + beta cache = (x_normalized, gamma, beta, sample_mean, sample_var, x, eps) running_mean = momentum * running_mean + (1 - momentum) * sample_mean running_var = momentum * running_var + (1 - momentum) * sample_var gamma和std对应，beta和mean对应。 Batch Normalization: backward 12345678910111213x_normalized, gamma, beta, sample_mean, sample_var, x, eps = cacheN, D = x.shapedx_normalized = dout * gamma # [N,D]x_mu = x - sample_mean # [N,D]sample_std_inv = 1.0 / np.sqrt(sample_var + eps) # [1,D]dsample_var = -0.5 * np.sum(dx_normalized * x_mu, axis=0, keepdims=True) * sample_std_inv**3dsample_mean = -1.0 * np.sum(dx_normalized * sample_std_inv, axis=0, keepdims=True) - \ 2.0 * dsample_var * np.mean(x_mu, axis=0, keepdims=True)dx1 = dx_normalized * sample_std_invdx2 = 2.0/N * dsample_var * x_mudx = dx1 + dx2 + 1.0/N * dsample_meandgamma = np.sum(dout * x_normalized, axis=0, keepdims=True)dbeta = np.sum(dout, axis=0, keepdims=True) dout就是loss关于yi的偏导数。 dropout_forward 12mask = (np.random.rand(*x.shape) &lt; p) / pout = x * mask p代表节点生效的概率。最后除以p是为了让整体的均值不变。 dropout_backward1dx = dout * mask 参考: L2范数归一化:L2范数归一化就是向量中每个元素除以向量的L2范数.]]></content>
      <tags>
        <tag>Deep Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CS231n学习笔记(全连接网络)]]></title>
    <url>%2F2017%2F06%2F01%2FCS231n%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E5%85%A8%E8%BF%9E%E6%8E%A5%E7%BD%91%E7%BB%9C%2F</url>
    <content type="text"><![CDATA[完成作业： [FullyConnectedNets.ipynb] [fc_net.py] [layers.py] [layer_utils.py] [optim.py] 这次作业非常重要，是assignment1的一次总结和大提升，我感觉应该反复咀嚼，琢磨透。进入了assignment2学习，打开FullyConnectedNets.ipynb，有一大段引言我试着翻译一下。 在之前的作业中，你在CIFAR-10数据集上实现了一个2层的全连接神经网络。这次简单的实现并不十分模块化，因为loss和梯度都在一块函数中计算。对于一个简单的2层网络来说，这样是可管理的。但随着我们向更大的模型推进时，这样做又变得不切实际。在理想情况下，我们希望使用更模块化的设计来独立实现不同类型的层，然后根据不同模型的结构把他们拼到一起。这次作业我们将使用更模块化的方法来实现一个全连接的神经网络。每一层我们都将实现一个前向和反向传播的功能。前向传播接收输入，权重和其他参数，并返回反向传播所需的输出和缓存对象。如下： 12345678910def layer_forward(x, w): """ Receive inputs x and weights w """ # Do some computations ... z = # ... some intermediate value # Do some more computations ... out = # the output cache = (x, w, z, out) # Values we need to compute gradients return out, cache 而反向传播将接收上行导数（？）和缓存对象，并返回根据梯度调整的输入和权重。如下：12345678910111213def layer_backward(dout, cache): """ Receive derivative of loss with respect to outputs and cache, and compute derivative with respect to inputs. """ # Unpack cache values x, w, z, out = cache # Use values in cache to compute derivatives dx = # Derivative of loss with respect to x dw = # Derivative of loss with respect to w return dx, dw 以这种模块化的方式实现了一些层后，我们就可以轻松地把它们组合起来去构建不同结构的分类器了。除了能实现任意深度的全连接网络，我们也能探索不同的最优化更新规则并引入Dropout方法作为规则化和批规范化的工具使优化深度网络更高效。 fc_net.init12345678910111213141516############################################################################# TODO: Initialize the weights and biases of the two-layer net. Weights ## should be initialized from a Gaussian with standard deviation equal to ## weight_scale, and biases should be initialized to zero. All weights and ## biases should be stored in the dictionary self.params, with first layer ## weights and biases using the keys 'W1' and 'b1' and second layer weights ## and biases using the keys 'W2' and 'b2'. ##############################################################################根据上面的提示，权重W需要初始化在一个标准差为weight_scale的高斯（正态）分布矩阵#故在W1生成input_dim行，hidden_dim列的高斯矩阵self.params['W1'] = weight_scale * np.random.randn(input_dim,hidden_dim) #D,H#权重b应该被初始化为零self.params['b1'] = np.zeros((1,hidden_dim)) #1,H#同理W2，b2self.params['W2'] = weight_scale * np.random.randn(hidden_dim,num_classes) #H,Cself.params['b2'] = np.zeros((1,num_classes)) #1,C fc_net.loss12345678910111213141516171819202122232425262728293031323334353637383940414243scores = None############################################################################# TODO: Implement the forward pass for the two-layer net, computing the ## class scores for X and storing them in the scores variable. #############################################################################W1, b1, W2, b2 = self.params['W1'], self.params['b1'], self.params['W2'],self.params['b2']# Forward into first layerhidden_layer, cache_hidden_layer = affine_relu_forward(X, W1, b1)# Forward into second layerscores, cache_scores = affine_forward(hidden_layer, W2, b2)############################################################################# END OF YOUR CODE ############################################################################## If y is None then we are in test mode so just return scoresif y is None: return scoresloss, grads = 0, &#123;&#125;############################################################################# TODO: Implement the backward pass for the two-layer net. Store the loss ## in the loss variable and gradients in the grads dictionary. Compute data ## loss using softmax, and make sure that grads[k] holds the gradients for ## self.params[k]. Don't forget to add L2 regularization! ## ## NOTE: To ensure that your implementation matches ours and you pass the ## automated tests, make sure that your L2 regularization includes a factor ## of 0.5 to simplify the expression for the gradient. #############################################################################data_loss, dscores = softmax_loss(scores, y)reg_loss = 0.5 * self.reg * (np.sum(W1**2) + np.sum(W2**2))loss = data_loss + reg_loss# Backprop into second layerdx1, dW2, db2 = affine_backward(dscores,cache_scores)dW2 += self.reg * W2# Backprop into first layerdx, dW1, db1 = affine_relu_backward(dx1, cache_hidden_layer)dW1 += self.reg * W1grads.update(&#123;'W1': dW1, 'b1': db1, 'W2': dW2, 'b2': db2&#125;) 正向的计算不多说了，反向传播说一下。先理理思路softmax层最终计算出了的是loss，我们希望loss下降，于是也给出了loss关于score的偏导数dscore。即$dscores = \frac{\partial loss}{\partial scores}$,于是想要求dx1,只要用链式法则$dx1 = \frac{\partial loss}{\partial scores}*\frac{\partial scores}{\partial x1}$可得到即hidden_layer的梯度。 最优化方法推荐文章： An overview of gradient descent optimization algorithms 中文版 卷积神经网络中的优化算法比较（这篇文章包含代码，便于理解） 深度学习最全优化方法总结比较 注意：在RMSprop方法的eps是在sqr内的，二号文章的代码将其放在外。]]></content>
      <tags>
        <tag>Deep Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Date型数据在SSM架构下的参数绑定与回显]]></title>
    <url>%2F2017%2F05%2F31%2FDate%E5%9E%8B%E6%95%B0%E6%8D%AE%E5%9C%A8SSM%E6%9E%B6%E6%9E%84%E4%B8%8B%E7%9A%84%E5%8F%82%E6%95%B0%E7%BB%91%E5%AE%9A%E4%B8%8E%E5%9B%9E%E6%98%BE%2F</url>
    <content type="text"><![CDATA[Date型数据有很多种表现形式，传递起来相对来说比较复杂。最近做的项目涉及到这方面内容，做个笔记。 Date型数据的参数绑定。想要将jsp页面收到String型参数转为Date型参数再保存到实体类中，首先需要创建一个转换器。创建Controller包,姑且命名为liam.ssm.controller.converter。在其中创建一个类(Class),命名为CustomDateConverter.java123456789101112131415public class CustomDateConverter implements Converter&lt;String, Date&gt; &#123; @Override public Date convert(String source) &#123; try &#123; SimpleDateFormat simpleDateFormat = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss"); //String型数据的格式为"yyyy-MM-dd HH:mm:ss"才能被正确转换 return simpleDateFormat.parse(source); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; return null; //转换失败返回空 &#125;&#125; 然后由于spingmvc使用注解驱动。需要在配置文件(springmvc.xml)将刚刚设置的转换器配置进去。123456789101112&lt;mvc:annotation-driven conversion-service="conversionService"&gt;&lt;/mvc:annotation-driven&gt;&lt;!-- conversionService --&gt; &lt;bean id="conversionService" class="org.springframework.format.support.FormattingConversionServiceFactoryBean"&gt; &lt;!-- 转换器 --&gt; &lt;property name="converters"&gt; &lt;list&gt; &lt;bean class="liam.ssm.controller.converter.CustomDateConverter"/&gt; &lt;/list&gt; &lt;/property&gt; &lt;/bean&gt; 这样参数绑定就完成了，下面看看jsp中的相关代码1&lt;input name="liam.time" id="time" value="2017-5-31 18:02:12"/&gt; 到后台controller，假如我们创建过liam这个实体类，那么time这个参数应该能正确传递。（本质上以上的操作均属于自定义的参数绑定，可以应用到其他类型绑定上去。）1234@RequestMapping("/test") public String test(Liam liam)throws Exception&#123; //TODO 设置有个断点，进来看看liam内包含的time是否正确 &#125; Date型数据的回显如果简单使用&lt;input name=&quot;liam.time&quot; id=&quot;time&quot; value=&quot;${liam.time}&quot;/&gt;会发现显示的时间是Wed May 31 18:11:32 CST 2017这样的形式。为了将其原模原样的回显成yyyy-MM-dd HH:mm:ss格式我使用JSTL标签。12&lt;%@taglib uri="http://java.sun.com/jsp/jstl/fmt" prefix="fmt"%&gt;&lt;fmt:formatDate value="$&#123;liam.time&#125;" pattern="yyyy-MM-dd HH:mm:ss"/&gt; 如果想要在input输入框回显，不显示具体时间，则这样1&lt;input name=&quot;liam.time&quot; value=&apos;&lt;fmt:formatDate value=&quot;$&#123;liam.time &#125;&quot; pattern=&quot;yyyy-MM-dd&quot; /&gt;&apos; /&gt; 注意：yyyy-MM-dd HH:mm:ss中的字符大小写不要随意修改，也许包含不同的含义。 字母 日期或时间元素 表示 示例 G Era 标志符 Text AD y 年 Year 1996 ; 96 M 年中的月份 Month July ; Jul ; 07 w 年中的周数 Number 27 W 月份中的周数 Number 2 D 年中的天数 Number 189 d 月份中的天数 Number 10 F 月份中的星期 Number 2 E 星期中的天数 Text Tuesday ; Tue a Am/pm 标记 Text PM H 一天中的小时数（0-23） Number 0 k 一天中的小时数（1-24） Number 24 K am/pm 中的小时数（0-11） Number 0 h am/pm 中的小时数（1-12） Number 12 m 小时中的分钟数 Number 30 s 分钟中的秒数 Number 55 S 毫秒数 Number 978 z 时区 General time zone Pacific Standard Time ; PST ; GMT-08:00 Z 时区 RFC 822 time zone -0800]]></content>
      <tags>
        <tag>spring mvc</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CS231n学习笔记-两层网络]]></title>
    <url>%2F2017%2F05%2F29%2FCS231n%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E4%B8%A4%E5%B1%82%E7%BD%91%E7%BB%9C%2F</url>
    <content type="text"><![CDATA[完成作业 [two_layer_net.ipynb] [neural_net.py] 前向传播计算图像在各类下的分数在neural_net.py下编辑loss函数123L1 = ReLU(np.dot(X,W1)+b1)L2 = np.dot(L1,W2)+b2 # N,Cscores = L2 L1代表第一层，使用ReLU作为激活函数。而ReLU函数没有定义，所以需要在TwoLayerNet外建一个。12def ReLU(x): return np.maximum(0,x) # f(z) = max(0, z) .ipynb文件中预置了正确答案，可以进行比对。我的如下：12345678910111213141516Your scores:[[-0.81233741 -1.27654624 -0.70335995] [-0.17129677 -1.18803311 -0.47310444] [-0.51590475 -1.01354314 -0.8504215 ] [-0.15419291 -0.48629638 -0.52901952] [-0.00618733 -0.12435261 -0.15226949]]correct scores:[[-0.81233741 -1.27654624 -0.70335995] [-0.17129677 -1.18803311 -0.47310444] [-0.51590475 -1.01354314 -0.8504215 ] [-0.15419291 -0.48629638 -0.52901952] [-0.00618733 -0.12435261 -0.15226949]]Difference between your scores and correct scores:3.68027204961e-08 计算loss1234567scores_max = np.max(scores, axis=1, keepdims=True)exp_scores = np.exp(scores - scores_max)probs = exp_scores/np.sum(exp_scores, axis=1, keepdims=True)correct_logprobs = -np.log(probs[range(N),y])data_loss = np.sum(correct_logprobs)/Nreg_loss = 0.5 * reg * (np.sum(W1**2)+np.sum(W2**2))loss = data_loss + reg_loss 这里使用softmax的方法来计算。第一句keepdims参数设置为True时，scores_max.shape为(5,1),方便之后shape为(5,3)的scores通过广播能与score_max相减。keepdim使用说明 计算gradient1234567891011121314151617181920descores = probs # N,Cdescores[range(N),y] += -1descores /= N# W2,b2dW2 = np.dot(L1.T, descores) # H,Cdb2 = np.sum(descores,axis=0,keepdims=True) # 1,C# L1dL1 = np.dot(descores,W2.T) # N,H# ReLudL1[L1 &lt;= 0] = 0# W1,b1dW1 = np.dot(X.T,dL1) # D,Hdb1 = np.sum(dL1,axis=0,keepdims=True)# regularizationdW1 += reg * W1dW2 += reg * W2grads['W1'] = dW1grads['b1'] = db1grads['W2'] = dW2grads['b2'] = db2 softmax作业中gradient是严格按照公式计算的，如下123y_trueClass = np.zeros_like(prob)y_trueClass[range(num_train), y] = 1.0 # N by CdW += -np.dot(X.T, y_trueClass - prob) / num_train + reg * W 而这次作业里代码将公式中第一个负号去掉，trueclass与prob位置对调。 微调参数我调整了迭代次数，学习率和学习率下降等参数。训练后用图像展示训练过程，发现最好的模型就在默认的提供的参数附近哈哈哈。12345678910111213plt.subplot(2, 1, 1)plt.plot(mytrain['loss_history'])plt.title('Loss history')plt.xlabel('Iteration')plt.ylabel('Loss')plt.subplot(2, 1, 2)plt.plot(mytrain['train_acc_history'], label='train')plt.plot(mytrain['val_acc_history'], label='val')plt.title('Classification accuracy history')plt.xlabel('Epoch')plt.ylabel('Clasification accuracy')plt.show() 我很喜欢这样的图片展示，这是我其中一个模型的。]]></content>
      <tags>
        <tag>Deep Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CS231n学习笔记(softmax)]]></title>
    <url>%2F2017%2F05%2F28%2FCS231n%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-softmax%2F</url>
    <content type="text"><![CDATA[完成作业: [softmax.ipynb] [softmax.py] softmax的计算方式与SVM类似。参考此图其loss和gradient的计算推导过程如图]]></content>
      <tags>
        <tag>Deep Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[UEditor的数据回显]]></title>
    <url>%2F2017%2F05%2F26%2FUEditor%E7%9A%84%E6%95%B0%E6%8D%AE%E5%9B%9E%E6%98%BE%2F</url>
    <content type="text"><![CDATA[最近遇到了ueditor的不能显示数据的回显问题。解决思路： 在jsp页面添加div类型的隐藏栏，作为中转站保存数据。 使用js代码配置ueditor面板，从上获得数据并展示 12&lt;div id="temp" style="display: none;"&gt;$&#123;***.**&#125;&lt;/div&gt;&lt;srcipt id="container" name="content" type="text/plain"&gt;&lt;/srcipt&gt; 12345&lt;script type="text/javascript"&gt;var ue = UE.getEditor('container');ue.ready(function()&#123;ue.setContent($('#temp').html());&#125;); Tips： 最初我用的办法是&lt;input id=&quot;temp&quot; type=&quot;hidden&quot; value=&quot;${***.**}&quot;&gt;和ue.setContent($(&#39;#temp&#39;).val());图像和信息能正确上传但不能回显。原因是数据库中图像的内容为&lt;p&gt;&lt;img src=&quot;http://localhost:8080/…/ueditor/jsp/upload/image/20170602/**.png……&quot;&gt;这里src=之后的双引号会和value=后的双引号配对，导致图像的路径信息无法传入，所以改为上面的方法。 使用eclipse+tomcat调试项目，图像保存在workspace\.metadata\.plugins\org.eclipse.wst.server.core\tmp0\wtpwebapps\myown\ueditor\jsp\upload\image下的日期目录（其中workspace代表eclipse的默认工程路径，myown是我的项目名称）。在eclipse项目内点击ueditor-&gt;jsp-&gt;config.json中调整图像的保存路径。我修改为&quot;imageUrlPrefix&quot;: &quot;/myown&quot;,]]></content>
      <tags>
        <tag>Tips</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[连接电脑时蓝牙耳机音质明显下降]]></title>
    <url>%2F2017%2F05%2F25%2F%E8%BF%9E%E6%8E%A5%E7%94%B5%E8%84%91%E6%97%B6%E8%93%9D%E7%89%99%E8%80%B3%E6%9C%BA%E9%9F%B3%E8%B4%A8%E6%98%8E%E6%98%BE%E4%B8%8B%E9%99%8D%2F</url>
    <content type="text"><![CDATA[先比较连接手机时的音质，判断是否为耳机自身故障。 确定非耳机故障时，右击【蓝牙设备】图标-【显示蓝牙设备】 在相关设置下选择【设备和打印机】 右击【蓝牙耳机】图标-声音设置 观察到一个耳机却又两个模式，点开Stereo模式的【属性】-选择【高级】 观察到“2通道，CD音质”说明现在使用的耳机模式是听音乐的。 作为对比回去再打开Hands-Free模式的【属性】【高级】 观察到“1通道，电话音质”，这就是我之前使用的，音质明显下降的版本。 选择Stereo为默认模式。]]></content>
      <tags>
        <tag>Tips</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CS231n 学习笔记(SVM)]]></title>
    <url>%2F2017%2F05%2F24%2FCS231n-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-SVM%2F</url>
    <content type="text"><![CDATA[完成作业： [svm.ipynb] [linear_svm.py] [linear_classifier.py] hstack(),vstack()12345678910111213141516171819202122import numpy as npx = np.array([[1,2],[3,4],[5,6]])a = np.ones((x.shape[0],1))b = np.ones((1,x.shape[1]))c = np.hstack([x,a])d = np.vstack([x,b])print (a)print (b)print (c)print (d)[[ 1.] [ 1.] [ 1.]][[ 1. 1.]][[ 1. 2. 1.] [ 3. 4. 1.] [ 5. 6. 1.]][[ 1. 2.] [ 3. 4.] [ 5. 6.] [ 1. 1.]] h-horizontalv-vertical难以表达自行感受 numpy.random.randn(d0, d1, …, dn)其中d0, d1, …, dn为整数型，输出标准正太分布的矩阵。若想输出$N(\mu,\sigma^2)$则公式为：$\sigma$*np.random.randn(…)+$\mu$若我们要生成满足正太分布为N(3，2.5^2)，2行4列的数组，则2.5*np.random.randn(2, 4)+3输出12array([[-4.49401501, 4.00950034, -1.81814867, 7.29718677], [ 0.39924804, 4.68456316, 4.99394529, 4.84057254]]) svm_loss_naive()123456789101112131415161718192021222324252627282930313233343536373839404142434445464748def svm_loss_naive(W, X, y, reg): """ Structured SVM loss function, naive implementation (with loops). Inputs have dimension D, there are C classes, and we operate on minibatches of N examples. Inputs: - W: A numpy array of shape (D, C) containing weights. - X: A numpy array of shape (N, D) containing a minibatch of data. - y: A numpy array of shape (N,) containing training labels; y[i] = c means that X[i] has label c, where 0 &lt;= c &lt; C. - reg: (float) regularization strength Returns a tuple of: - loss as single float - gradient with respect to weights W; an array of same shape as W """ dW = np.zeros(W.shape) # initialize the gradient as zero # compute the loss and the gradient num_classes = W.shape[1] num_train = X.shape[0] loss = 0.0 for i in xrange(num_train): scores = X[i].dot(W) #scores包含10个类别的分数 correct_class_score = scores[y[i]] #正确类别下的分数 for j in xrange(num_classes): if j == y[i]: continue margin = scores[j] - correct_class_score + 1 # note delta = 1 if margin &gt; 0: loss += margin dW[:,y[i]] += X[i,:] dW[:,j] += -X[i,:] # Right now the loss is a sum over all training examples, but we want it # to be an average instead so we divide by num_train. loss /= num_train dW /= num_train # Add regularization to the loss. loss += 0.5 * reg * np.sum(W * W) dW += reg * W return loss, dW dW部分参考：梯度推导 梯度的抽样检测1234567891011121314151617181920def grad_check_sparse(f, x, analytic_grad, num_checks=10, h=1e-5): """ sample a few random elements and only return numerical in this dimensions. """ for i in xrange(num_checks): ix = tuple([randrange(m) for m in x.shape]) oldval = x[ix] x[ix] = oldval + h # increment by h fxph = f(x) # evaluate f(x + h) x[ix] = oldval - h # increment by h fxmh = f(x) # evaluate f(x - h) x[ix] = oldval # reset grad_numerical = (fxph - fxmh) / (2 * h) grad_analytic = analytic_grad[ix] rel_error = abs(grad_numerical - grad_analytic) / (abs(grad_numerical) + abs(grad_analytic)) print 'numerical: %f analytic: %f, relative error: %e' % (grad_numerical, grad_analytic, rel_error) 即判断$ \frac{f(x+h)-f(x-h)}{2h} = \frac{\partial f(x)}{\partial x} $ svm_loss_vectorized(W, X, y, reg)123456789101112131415161718192021222324def svm_loss_vectorized(W, X, y, reg): """ Structured SVM loss function, vectorized implementation.Inputs and outputs are the same as svm_loss_naive. """ loss = 0.0 dW = np.zeros(W.shape) # initialize the gradient as zero scores = X.dot(W) # num_train by num_class num_train = X.shape[0] num_classes = W.shape[1] scores_correct = scores[np.arange(num_train), y] # 1 by num_train #print scores_correct scores_correct = np.reshape(scores_correct, (num_train, 1)) # num_train by 1 margins = scores - scores_correct + 1.0 # num_train by num_class margins[np.arange(num_train), y] = 0.0 # 将正确标签下的margins置为0 margins[margins &lt;= 0] = 0.0 # 将值小于0的margins置为0 #print margins[0] loss += np.sum(margins) / num_train loss += 0.5 * reg * np.sum(W * W) # compute the gradient margins[margins &gt; 0] = 1.0 row_sum = np.sum(margins, axis=1) # 1 by num_train margins[np.arange(num_train), y] = -row_sum dW += np.dot(X.T, margins)/num_train + reg * W # dimension by num_class 关于loss就是将大于0的margins相加关于gradient即将margins中的数值分类。再通过最后一步 dW += np.dot(X.T, margins)/num_train + reg * W 完成梯度的计算。本质上与naive函数相同1234567 if margin &gt; 0: loss += margin dW[:,y[i]] += -X[i:] dW[:,j] += X[i:] dW /= num_traindW += reg * W PS:在linear_classifier.py中X的shape是D*N所以在一些地方要将X转置才行。]]></content>
      <tags>
        <tag>Deep Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CS231n学习笔记(KNN)]]></title>
    <url>%2F2017%2F05%2F22%2FCS231n%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[完成作业: [k_nearest_neighbor.py] [knn.ipynb] enumerate()12for y, cls in enumerate(classes): print y, cls enumerate能遍历一个数组或列表，并获得索引和索引内容。得到：123456789100 plane1 car2 bird3 cat4 deer5 dog6 frog7 horse8 ship9 truck flatnonzero()12idxs = np.flatnonzero(y_train == y)print idxs y_train 为训练集图片的标签集。flatnonzero() 打印非零元素的位置。整体意为以图片标签分类，输出图片的下标。12345678910[ 29 30 35 ..., 49941 49992 49994][ 4 5 32 ..., 49993 49998 49999][ 6 13 18 ..., 49987 49991 49995][ 9 17 21 ..., 49979 49982 49983][ 3 10 20 ..., 49981 49984 49990][ 27 40 51 ..., 49964 49980 49988][ 0 19 22 ..., 49962 49966 49996][ 7 11 12 ..., 49965 49978 49986][ 8 62 69 ..., 49968 49976 49985][ 1 2 14 ..., 49963 49971 49997] np.random.choice()官方介绍12idxs = np.random.choice(idxs, samples_per_class, replace=False) print idxs 从idxs中随机取出samples_per_class个数，不可重复。 subplot(m,n,p)1plt.subplot(samples_per_class, num_classes, plt_idx) subplot(m,n,p)是将多个图画到一个平面上的工具。其中，m表示是图排成m行，n表示图排成n列，也就是整个figure中有n个图是排成一行的，一共m行，如果m=2就是表示2行图。p表示图所在的位置，p=1表示从左到右从上到下的第一个位置。 reshape()1X_train = np.reshape(X_train, (X_train.shape[0], -1)) 如果newshape给的参数是（x,-1）,那么函数会自动判别newshape为(x, mn/x）,这里的x一定要能被m*n整除！stackoverflow有相关的介绍。‘-1’在此处的意义-1表示我懒得计算该填什么数字，让机器通过其他的值推测出来。 k_nearest_neighbor.py 作业12345678910111213for i in xrange(num_test): for j in xrange(num_train): ##################################################################### # TODO: # # Compute the l2 distance between the ith test point and the jth # # training point, and store the result in dists[i, j]. You should # # not use a loop over dimension. # ##################################################################### dists[i,j] = np.sum((X[i,:]-self.X_train[j,:])**2) ##################################################################### # END OF YOUR CODE # ##################################################################### return dists 这里代表X为测试集，X_train是训练集。X有500个点，每个点代表一张图片，有32*32维。每个点的维度都与训练集X_train相减后求平方的和。得到测试集与训练集的点的“距离”。 Inline Question #1: Notice the structured patterns in the distance matrix, where some rows or columns are visible brighter. (Note that with the default color scheme black indicates low distances while white indicates high distances.) What in the data is the cause behind the distinctly bright rows? What causes the columns?Your Answer: 明亮的行表示当前测试图与多数训练图相似度低，明亮的列表示当前训练图与多数测试图相似度低 argsort()返回的是数组值从小到大的索引值123x = np.array([3, 1, 2])np.argsort(x)array([1, 2, 0]) most_common()获取出现频率最高的s个字符1234567from collections import Counters = '''A Counter is a dict subclass for counting hashable objects. It is an unordered collection where elements are stored as dictionary keys and their counts are stored as dictionary values. Counts are allowed to be any integer value including zero or negative counts. The Counter class is similar to bags or multisets in other languages.'''.lower()c = Counter(s)# 获取出现频率最高的5个字符print c.most_common(5)# Result:[(' ', 54), ('e', 32), ('s', 25), ('a', 24), ('t', 24)] numpy.linalg.norm(x, ord=None, axis=None, keepdims=False)官方解释ord表示求哪种范数，默认为L2范数。axis表示按照什么维度。0就是x轴，（0就是竖着比较，1就是横着比较。）具体看下面例子。12345678&gt;&gt;&gt; c = np.array([[ 1, 2, 3],... [-1, 1, 4]])&gt;&gt;&gt; LA.norm(c, axis=0)array([ 1.41421356, 2.23606798, 5. ])&gt;&gt;&gt; LA.norm(c, axis=1)array([ 3.74165739, 4.24264069])&gt;&gt;&gt; LA.norm(c, ord=1, axis=1)array([ 6., 6.]) numpy.ndarray.T矩阵的转置1234567&gt;&gt;&gt; x = np.array([[1.,2.],[3.,4.]])&gt;&gt;&gt; xarray([[ 1., 2.], [ 3., 4.]])&gt;&gt;&gt; x.Tarray([[ 1., 3.], [ 2., 4.]]) 作业 no-loops部分1234M = np.dot(X, self.X_train.T)te = np.square(X).sum(axis = 1)tr = np.square(self.X_train).sum(axis = 1)dists = np.sqrt(-2*M+tr+np.matrix(te).T) 利用(a-b)^2=a^2+b^2-2ab和numpy的broadcasting性质。]]></content>
      <tags>
        <tag>Deep Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ssm架构下json数据传递问题和解决]]></title>
    <url>%2F2017%2F05%2F18%2Fssm%E6%9E%B6%E6%9E%84%E4%B8%8Bjson%E6%95%B0%E6%8D%AE%E4%BC%A0%E9%80%92%E9%97%AE%E9%A2%98%E5%92%8C%E8%A7%A3%E5%86%B3%2F</url>
    <content type="text"><![CDATA[环境说明 springmvc-4.3.6 jar包 jackson-2.7.4 [jar]包 代码说明在springmvc.xml文件中采用&lt;mvc:annotation-driven/&gt;注解驱动。在jsp文件下,配置一个按钮调用下面的js代码12345678910function requestJson()&#123; $ajax(&#123; type:"post", url:"$&#123;pageContext.request.contextPath &#125;/requestJson.action", data:'&#123;"articleid":10,"title":"test"&#125;', success:function(data)&#123; alert(data); &#125; &#125;);&#125; 在后台创建一个controller。传入json数据，@RequestBody转为实体类。传出实体类，使用@ResponseBody转为json格式。1234567@Controllerpublic class JsonTest&#123; @RequestMapping("/requestJson") public @ResponseBody items requestJson(@RequestBody Items items)&#123; return items; &#125;&#125; 在浏览器中打开jsp页面，F12控制台观察requestJson.action的状态码 问题1.状态码200/415，不进入后端controller断点，浏览器弹出网页html代码解决办法：缺少class，更换jar包。 2.状态码400/500，不进入后端controller断点，浏览器无反应解决办法：jsp页面中data数据需采用正确的json编写格式，以逗号分隔。]]></content>
      <tags>
        <tag>spring mvc</tag>
        <tag>json</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[windows下使用hexo+github部署个人博客]]></title>
    <url>%2F2017%2F05%2F17%2F20170517%2F</url>
    <content type="text"><![CDATA[需要装好git, node.js 并全程科学上网(全局代理)。 使用gitbash npm install hexo 新建文件夹用于个人博客，并进入目录下hexo init 继续使用hexo g 和hexo s 生成并本地测试网站, 打开浏览器输入localhost:4000 ，此时需临时关闭全局代理方可看到本地博客。 配置config.yml文件(关键为deploy区域，其余部分自由发挥) 1234deploy: type: git repo: https://github.com/asdf0982/asdf0982.github.io.git branch: master 使用hexo d 将本地文档上传到github(需全局代理保证网络链接)如一切正常会弹出一个github登录框。关于ssh的配置，其实只是省略了这个登录的步骤。]]></content>
      <tags>
        <tag>git</tag>
        <tag>hexo</tag>
        <tag>windows</tag>
      </tags>
  </entry>
</search>